[
  {
    "objectID": "english.html",
    "href": "english.html",
    "title": "English",
    "section": "",
    "text": "Farm animals and Vegetables\n\n\n\n\n\n\nenglish\n\n\nadvantage disadvantage\n\n\n\nYour post description\n\n\n\n\n\nApr 8, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIelts task 1 - Household work\n\n\n\n\n\n\nenglish\n\n\nadvantage disadvantage\n\n\n\nYour post description\n\n\n\n\n\nApr 6, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Essay Writing by Hand\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nIELTS Essay Writing by Hand\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Essay Having Children at an Older Age\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nIELTS Essay Having Children at an Older Age\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Structure a ‘Two Question’ Essay\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nHow to Structure a ‘Two Question’ Essay\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Essay Financial Matters\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nIELTS Essay Financial Matters\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIelts task 1 - Household work\n\n\n\n\n\n\nenglish\n\n\ntime\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance-Enhancing Drugs in Sports\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Services\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\nadvantages disadvantage\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nGrammar Present time\n\n\n\n\n\n\nenglish\n\n\ngrammar\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nZoos\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\nadvantages disadvantage\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Lecture 8.html",
    "href": "Lecture 8.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "Anh -&gt; cut vat the ( vd xe) -&gt; trich xuat vector dac trung -&gt; save db\nCó nhận xét kết quả của tập dữ liệu sinh ra ở mỗi kết quả dòng lênh.\nNhận xédt trong markdown"
  },
  {
    "objectID": "paid attention to.html",
    "href": "paid attention to.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": ": to concentrate on\nPay (close/careful) attention to what she says."
  },
  {
    "objectID": "posts/nextcloud-install/index.html",
    "href": "posts/nextcloud-install/index.html",
    "title": "Install Nextcloud in Ubuntu Server",
    "section": "",
    "text": "https://ubuntu.com/download/server"
  },
  {
    "objectID": "posts/nextcloud-install/index.html#linuxserver.io-nextcloud-doesdoes-not-useuse-postgresql-environmentenvironment-variablesvariables-to-connect-to-the-databasedatabase.",
    "href": "posts/nextcloud-install/index.html#linuxserver.io-nextcloud-doesdoes-not-useuse-postgresql-environmentenvironment-variablesvariables-to-connect-to-the-databasedatabase.",
    "title": "Install Nextcloud in Ubuntu Server",
    "section": "❗ LinuxServer.io Nextcloud doesdoes not useuse PostgreSQL environmentenvironment variablesvariables to connect to the databasedatabase.",
    "text": "❗ LinuxServer.io Nextcloud doesdoes not useuse PostgreSQL environmentenvironment variablesvariables to connect to the databasedatabase.\nThat’s the issueissue rightright therethere.\n\nUnlikeUnlike the officialofficial nextcloud imageimage, linuxserver/nextcloud doesdoes not read POSTGRES_USER, POSTGRES_PASSWORD, etc. from the environmentenvironment.\n\nInsteadInstead, it expects youyou to configureconfigure the databasedatabase entirelyentirely throughthrough the web UI.\n\nservices:\n  db:\n    image: postgres:16\n    container_name: nextcloud-postgres\n    environment:\n      POSTGRES_DB: nextclouddb\n      POSTGRES_USER: nextcloud\n      POSTGRES_PASSWORD: MyStrongPassword123!\n    volumes:\n      - /opt/nextcloud/postgres:/var/lib/postgresql/data\n    restart: unless-stopped\n\n  nextcloud:\n    image: lscr.io/linuxserver/nextcloud:latest\n    container_name: nextcloud\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Etc/UTC\n    volumes:\n      - /opt/nextcloud/config:/config\n      - /opt/nextcloud/data:/data\n    ports:\n      - 443:443\n    depends_on:\n      - db\n    restart: unless-stopped"
  },
  {
    "objectID": "mathematic.html",
    "href": "mathematic.html",
    "title": "Mathematic",
    "section": "",
    "text": "Statistical Inference and Learning\n\n\n\n\n\n\nStatistical\n\n\n\nYour post description\n\n\n\n\n\nFeb 26, 2025\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Randome Variable\n\n\n\n\n\n\nmath\n\n\nstatistic\n\n\n\nBernoulli Randome Variable\n\n\n\n\n\nSep 11, 2024\n\n\nKim Hung Bui\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we have normal distribution formular\n\n\n\n\n\n\nmath\n\n\nstatistic\n\n\n\nYour post description\n\n\n\n\n\nSep 11, 2024\n\n\nYour name\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "english/financial-matters/index.html",
    "href": "english/financial-matters/index.html",
    "title": "IELTS Essay Financial Matters",
    "section": "",
    "text": "Question:\nIn numerous nations, there is a growing trend for individuals to frequently discuss financial matters, such as their earnings or expenditure, in everyday conversation. What are the reasons behind this phenomenon? Is it advantageous or detrimental?\n\nThere is an increasing tendency for individuals in the modern world to openly discuss their finances in daily conversation. In my opinion, this is a result of a general lack of self-awareness and is decidedly negative given the opportunity cost it engenders.\nThe primary reason many people are prone to talk their financial dealings originates in society generally. In the past, a sense of modesty or concern for decorum might have compelled reticence. Today, some of the most widely discussed topics in the media, online, in film, and in literature concern cryptocurrencies, the real estate market, stock equities, the price of gold, and other investments. A person discussing these matters is likely only interested in subtle boasting if venting frustrations without awareness of the emotions or engagement of their interlocutor. The result over time has been that many people have developed a shameless habit.\nIn my view, discussion finances often precludes a number of more worthwhile topics of conversation. Since the majority of useful financial research can be conducted online or done in professional settings, there are more pressing areas that should be discussed in daily life. For example, two friends having a conversation at a cafe would derive greater value from telling stories, joking with one another, or delving into a more serious topic related to politics, the arts, or psychology. Such discussions are likely to develop their understanding of the world and their emotional intelligence. Discussion of money, on the other hand, wastes valuable time and energy for little concrete outcome.\nIn conclusion, the inclination to speak of one’s personal finances stems from a fundamentally modern self-indulgence and often prevents the discussion of more enjoyable or meaningful topics. Naturally, these generalizations do not apply to all individuals.\n\nChatGPT’s essay:\nIn numerous nations, a growing trend has emerged where people openly discuss financial matters, including their earnings and expenditures, in everyday conversations. This phenomenon can be attributed to a range of social and economic factors, and i firmly believe that it is advantageous. Open dialogue about personal finances not only foster financial literacy but also encourages better money management and transparency within communities.\nOne primary reason for this trend is the rapid spread of financial education and the influence of digital media. With the advent of social media platforms and financial blogs, individuals have unprecedented access to information about budgeting, investing, and saving. This exposure has led many to feel more confident in discussing their financial situations, as they seek advice and validation from peers. Additionally, economic uncertainty in many regions has prompted individuals to share personal financial experiences as a way of coping and learning from one another. For instance, during periods of economic downturn, people often exchange tips on reducing expenses or finding alternative income sources. This collective sharing not only helps individuals navigate challenging time but also creates a support network where practical financial strategies are exchanged openly.\nThe open discussion of financial matters carries significant benefits for both individuals and society at large. Firstly, it promotes financial literacy by normalizing conversations about money, which can demystify complex financial topic and empower people to make informed decisions. As people become more knowledgeable about financial planning and investment strategies, they are better equipped to secure their futures and contribute to economic growth. Secondly, this trend encourages transparency and accountability, reducing the stigma around discussing personal finances. For example, when individuals talk about budgeting and debt management, it can inspire others to examine their own spending habits, ultimately leading to a more financially responsible community. By fostering an environment where financial struggles and successes are shared openly, society as a whole benefits from improved financial stability and reduced inequality.\nIn conclusion, the increasing tendency for individuals to discuss financial matters in everyday conversation is driven by greater access to information and a need for mutual support during uncertain economic times. I contend that this development is advantageous, as it enhances financial literacy, transparency, and responsible money management. Ultimately, such open dialogue can empower communities to achieve greater financial stability and economic well-being.\n\nGrammar structures\n\nComplex Sentence Structures\n\nUse of subordinate clauses to add detail. Example: “With the advent of social media platforms and financial blog, individuals have unprecedented access to information about budgeting, investing, and saving.”\n\nCause and Effect Constructions\n\nPhrases such as “can be attributed to” and “leads to” explain reasons and consequences. Example: “This phenomenon can be attributed to a range of social and economic factors.”\n\nConditional and Hypothetical Phrasing:\n\nImplicit conditionals that suggest potential outcomes or benefits. Example: “when individuals talk about budgeting and debt management, it can inspire others to examine their own spending habits.”\n\nComparative Structure:\n\nUsing comparisons to show differences in outcomes. Example: “For instance, during periods of economic downturn, people often exchange tips on reducing expenses or finding alternative income sources.”\n\nUse of Passive Voice:\n\nPassive constructions to emphasize processes or outcomes rather than actors.\nExample: “This phenomenon can be attributed to a range of social and economic factors.”\n\nLinking Devices and Transitional Phrases:\n\nWords and phrases such as “Firstly,” “Secondly,” “In conclusion,” and “For example” to organize ideas and guide the reader.\n\n\n\n\nLexical Items\n\nAdvanced Vocabulary:\n\nWords such as “unprecedented,” “demystify,” “transparency,” “accountability,” and “empower” elevate the formality and clarity of the argument.\n\nFinancial Terminology:\n\nSpecific terms related to finance like “budgeting,” “investing,” “saving,” “financial literacy,” “debt management,” and “spending habits.”\n\nDescriptive Adjectives:\n\nTerms that add detail and nuance, for example “open dialogue,” “collective sharing,” “rigorous, random testing,” and “financially responsible.”\n\nCollocations and Fixed Phrases:\n\nCommon phrases that support the essay’s theme such as “financial matters,” “economic uncertainty,” “personal financial experiences,” and “mutual support.”\n\nTransitional Vocabulary:\n\nWords that link ideas logically like “furthermore,” “additionally,” and “ultimately” ensure smooth progression throughout the essay.\n\nOpinion and Evaluation Language:\n\nPhrases that express clear stance such as “I firmly believe,” “it is essential,” “I contend that,” and “I contend that this development is advantageous.”"
  },
  {
    "objectID": "english/english-the-internet-books/index.html",
    "href": "english/english-the-internet-books/index.html",
    "title": "Ielts task 1 - Household work",
    "section": "",
    "text": "In the past, knowledge was contained in books. Nowadays, knowledge is uploaded to the internet. Do the advantages of this outweigh the disadvantages?\nIt is a fact of modern life that most information today is store online, rather than in physical books as it was in the past. In my opinion, this is a negative despite its obvious conveniences.\nSupporters of this trend can point to the numerous ways knowledge kept online can be disseminated and used. Online information is almost universally accessible, narrowing socioeconomic differences in society and allowing for various conveniences. First of all, users can find relevant information on search engines from a wide range of sources including user-generated sites like Wikipedia to news outlets like The New York Times and academic journals as well. Once an individual locates the information they are looking for, it is also easier to search within that article, share it with others, have it wherever they go and store more than would be possible if it were a hard copy.\nNonetheless, storing information online encourages a dependence on computer technology that is harmful for the human mind. In the past, if a person wanted to research a given topic, they would have to check out a library book or talk with an expert. This process was much slower and open to the possibility of bias. However, it also meant that individuals were more conscientious and patiently learned from fuller, more nuanced sources. Nowadays, most people read short articles, hastily written to attract more clicks and generate advertising revenue. These tap into a fundamental human need to be constantly engaged and feeling pleasure. Over time, this has led to a generation addicted to devices, lacking the healthy habits engendered by careful research and study.\nIn conclusion, despite the handiness of online information, the impact on the human psyche makes this a negative trend on level. Individuals must themselves find ways to counter these potentially injurious effects.\nSample positive essay:\nIn the digital age, the migration of knowledge from physical books to online platforms has become a defining feature of global society. While critics argue that this shift undermines intellectual depth and fosters dependency, I firmly believe the advantages of accessible, dynamic, and. collaborative information far surpass its drawbacks.\nProponent of online knowledge store emphasize its unparalleled capacity to democratize information. Unlike traditional books, which are more constrained by physical availability and cost, digital platforms grant instantaneous access to vast repositories of data for anyone with an internet connection. Students in remote villages, for instance, can now access lectures from Ivy League universities, download academic journals, or participate in global forums - opportunities once reserved for the socioeconomically privileged. Moreover, search engines and algorithms allow users to pinpoint precise information within seconds, a task that previously required hours of library search. The integration of multimedia - such as video, interactive graphs, and hyperlinks- further enriches understanding, catering to diverse learning styles. Crucially, digital storage eliminates the environmental toll of paper production, reducing deforestation and carbon emissions linked to printing and distributing physical books.\nDetractors, however, contend that the internet fosters misinformation, distraction, and superficial engagement. While valid, these concern are not insurmountable. It is true that unverified content proliferates online, yet this issue is counterbalanced by the rise of fact-checking tools, peer-reviewed databases, and digital literacy programs that empower users to discern credible sources. Similarly, although the constant influx of notifications and bite-sized content risks fragmenting attentions spans, individuals and educators can cultivate disciplined usage through structured learning frameworks and mindfulness practices. The digital divide-limited internet access in impoverished regions - remains a challenge, yet global initiatives to expand connectivity, such as satellite internet projects and affordable mobile data, are steadily bridging this gap.\nIn conclusion, the transition to online knowledge storage represents a net positive for humanity. Its capacity to democratize education, enhance efficiency, and promote sustainability far outweighs manageable risks like misinformation and distraction. To maximize benefits, society must prioritize digital literacy and equitable access, ensuring the internet remains a force for collective progress.\n\nSpeaking\n\nHow often do most people use the internet? Most people use the internet daily, if not constantly. Smartphones and Wi-Fi have made it ubiquitous whether for work, social media, or streaming. For instance, younger generations might spend 6+ hours online, while older adults might use it less frequently but still rely on it for news or banking.\nAre there drawbacks to its overuse? Absolutely. Overuse can lead to addiction, reduced productivity, and mental issues like anxiety or loneliness. Physically, screen time strains eyes and disrupts sleep. Socially, it erodes face-to-face interactions - think of families ignoring each other to scroll through phones. Moderation is key.\nWhat kind of information can people find online? Virtually anything. From academic research on Google Scholar to DIY tutorials on Youtube, and even niche forums discussing obscure hobbies. However, misinformation is rampant too - fake news, biased articles, or conspiracy theories. Critical thinking is essential to navigate this flood of data.\nHow was this changed the way people get their information? It’s deepen integration into daily life. Think smart homes controlled via apps, AI-driven healthcare consultations, or virtual reality workspaces. However, privacy concerns will grow, and the digital divide may widen unless access becomes universal. Education and jobs will increasingly depend on digital literacy."
  },
  {
    "objectID": "english/two-question-essay/index.html",
    "href": "english/two-question-essay/index.html",
    "title": "How to Structure a ‘Two Question’ Essay",
    "section": "",
    "text": "Let’s look at an example of this kind of question:\n*Nowadays governments are investing more in public transport such as buses and trains rather than in building new roads.*\n*What are the reasons for this?*\n*Is this a positive or negative development?*\n\nClear position\nFor this kind of task we need to discuss both questions. In the first question, we are given a situation and asked to explain why. The second question requires a clear opinion so we must choose a side. This is very important for our task response score. If we don’t choose a side and support that ‘position’ clearly, we will lose marks for our task response. We will get a maximum of 5 for task achievement if we do not choose a side.\nIt’s not important which side we choose as long as we choose one. Here are the options for the second questions:\n\nAgree: We think it’s a positive development that governments are spending more money on public transport. We support this opinion.\nDisagree: We think it’s a negative development that governments are spending more money on public transport. We support this opinion.\n\n\n\nClear essay structure\nDavid Lang recommend that we should give our opinion in the introduction as well as the conclusion (for all IELTS Task 2 questions). It’s OK to write a very general introduction and leave our opinion to the end. But it’s not as clear and we might run out of time before we’ve hot to our conclusion, so overall it’s a risky strategy. If we don’t choose a side and support that ‘position’ clearly, we will lose marks for our task response.\nTherefore, him suggested essay structure is four paragraphs. The opinion is presented in the introduction and re-stated in the conclusion. Ideally each body paragraph in the introduction and re-stated in the conclusion.\n\n\nPossible Essay structures\n1. Agree - we feel it’s a positive development that governments are spending more money on public transport.\nWe need at least one good reason why it’s happening and one good reason why it’s a positive development.\n\n\n\n\n\n\n\nParagraph 1 (intro)\nParaphrase both questions and state your overall opinion (you feel it’s a positive development)\n\n\nParagraph 2 (body 1)\nExplain and support your reason why governments are spending more money on public transport.\n\n\nParagraph 3 (body 2)\nExplain and support your reason why this is a positive development.\n\n\nParagraph 4 (conclusion)\nRe-state your overall opinion (you feel it’s a positive development) and  summarise your main reasons.\n\n\n\n2. Disagree: - we feel it’s a negative development that governments are spending more money on public transport.\nWe need at least one good reason why it’s happening and one good reason why it’s a negative development. Only the parts highlighted below are different from the first essay\n\n\n\n\n\n\n\n#### Paragraph 1 (intro)\n#### Paraphrase both questions and state your overall opinion (you feel it’s a negative development)\n\n\n#### Paragraph 2 (body 1)\n#### Explain and support your reason why governments are spending more money on public transport.\n\n\n#### Paragraph 3 (body 2)\n#### Explain and support your reason why this is a negative development.\n\n\n#### Paragraph 4 (conclusion)\n#### Re-state your overall opinion (you feel it’s a negative development) and  summarise your main reasons."
  },
  {
    "objectID": "english/present-time/index.html",
    "href": "english/present-time/index.html",
    "title": "Grammar Present time",
    "section": "",
    "text": "Present Simple\n\n\n\n\n\n\n\nUse to\nExample\n\n\n\n\nGeneral truths\nThe left-hand side of the brain controls the right-hand side of the body.\n\n\nCurrent habits\nI don’t always go to lectures that are early in the morning.\n\n\nPermanent situations and states\nAngie teaches French at a local adult education centre."
  },
  {
    "objectID": "english/performance-enhancing-drug/index.html",
    "href": "english/performance-enhancing-drug/index.html",
    "title": "Performance-Enhancing Drugs in Sports",
    "section": "",
    "text": "In recent years, there has been a rise in the use of performance-enhancing substances in sports. What are the consequences of doping for athletes? What measures should be taken to combat this issue?\nThere has been a considerable rise in detections for performance-enhancing substances in athletic competitions recently. In my opinion, the most likely consequences for athletes concern their careers and this infraction is best tackled through more sophisticated screening tests.\nIn the majority of cases, athletes risk lasting career repercussions when caught doping. The first and most obvious consequences are financial. An athlete who tests positive for a banned substance will certainly be banned for a defined amount of time, potentially permanently in serious cases, and will forfeit salary and prize money over this period. Beyond raw salary, most athletes receive endorsements and a worsening public image will undoubtedly damage those business relationships as well. Relatedly, an athlete’s legacy is also at stake. A well-known example of this would be the baseball player Barry Bonds who not only lost the final years of an illustrious career to suspensions but has also not been inducted into the Baseball Hall of Fame due to his past transgressions and numerous failed tests.\nIn order to combat the pervasiveness of performance-enhancing drugs, sporting authorities must invest in modernized testing procedures. The most complex testing is currently utilized in the Olympics, however there are flaws in this system as it is often compromised by corruption and the equally sophisticated doping methods of certain nations. A more pertinent case of study would be in sports such as boxing and MMA where testing can literally mean the difference between life and death. In boxing, athletes often agree to their own testing procedures contractually. This method is inferior to vesting power in an authority, such as the UFC, that can ensure tests are modern and fairly conducted. Most UFC athletes are tested at random points throughout the year as well as prior to and immediately after bouts. Applying such a policy across various sports would not only catch the bast majority of infractions but also, more importantly, deter abuse even beginning.\nIn conclusion, athletes risk reputational and financial damage by doping and this can be prevented as testing becomes more comprehensive and potent. These measures are necessary to safeguard the integrity of sporting competitions around the world.\n\nIn recent years, the use of performance-enhancing drugs in sports has increased, posing significant challenges to fair competition. Doping not only harm an athlete’s career and finances but also damages the trust between fans, sponsors, and sporting institutions. I firmly believe that to protect the integrity of sports, it is essential to implement more advanced and transparent testing procedures.\nAthletes who resort to doping face serious long-term consequences that extend beyond temporary suspensions. When an athletes tests positive for banned substances, they risk losing salaries, prize money, and lucrative endorsements. For example, a renowned baseball player might be suspended, causing them to forfeit not only their immediate earnings but also potential future opportunities, such as Hall of Fame induction. This example clearly shows that doping can irreversibly tarnish an athlete’s legacy and career, leading to both financial ruin and a permanent stain on their reputation.\nTo effectively counter the issue of doping, sports authorities must adopt modern, comprehensive testing protocols. While major competitions like the Olympics employ advanced testing, the current is sometimes undermined by corruption and the ever-evolving sophistication of doping techniques. In sports like boxing and mixed martial arts, for instance, some athletes are subjected to self-regulated tests and lack objectivity. In contrast, organizations such as the UFC implement rigorous, random testing both before and after events, ensuring a fairer system. Adopting these strict measures across all sports would not only detect infractions more reliably but also serve as a strong deterrent against doping. By ensuring that testing is both modern and impartial, the integrity of competitive sports can be maintained, safeguarding athletes’ careers and reputation of the sport.\nIn conclusion, doping leads to severe financial and reputational damage for athletes while undermining the fairness of sports competitions. Only through the implementation of advanced, transparent testing procedures can we hope to deter doping and preserve the true spirit of sportsmanship for future generations.\n\nVocabulary\nFor extra practice, write an antonym (opposite word) on a piece of paper to help you remember the new vocabulary: 1. There has been a considerable rise in detections - There has been a significant increase in identifications 2. performance-enhancing substances - performance-boosting substances 3. athletic competitions - sporting events 4. most likely consequences for - probable outcomes for"
  },
  {
    "objectID": "english/writing-by-hand/index.html",
    "href": "english/writing-by-hand/index.html",
    "title": "IELTS Essay Writing by Hand",
    "section": "",
    "text": "Question:\nFewer and fewer people today write by hand using a pen or pencil.\nWhat are the reasons for this?\nIs this a positive or a negative development?\nDavid Lang’s essay:\nToday, the act of writing by hand using a pen or pencil has become increasingly uncommon. This essay will delve into the reasons behind this trend and evaluate whether it is a positive or negative development for individuals and society as a whole.\nOne primary reason for the dwindling use of handwriting is the pervasive presence of technology. Inn an age dominated by computers, smartphones, and tables, individuals find it more convenient to type their thoughts and ideas rather than engaging in the laborious process of writing by hand. The speed and efficiency offered by digital devices have made it easier to edit, store, and share written content, rendering handwriting seemingly obsolete. To illustrate this, consider the rapid rise of communication platforms such as email, instant messaging apps, and social media. These platforms provide users with the ability to effortlessly send and receive messages, eliminating the need for handwritten letters or notes. This convenience extends to practical considerations for professionals in various fields, such as journalists, researchers, and writers, who often rely on digital platforms and word processors for their work. The ease of editing, revising, and sharing digital content outweighs the traditional pen-and-paper approach.\nWhile the decline of handwriting may offer certain advantages, it also brings forth potential negative consequences. Handwriting is an essential skill that fosters cognitive development, improves memory retention, and allows for creative expression. The act of physically forming letters and words engages different areas of the brain, promoting better comprehension and critical thinking skills. Losing this skill may hinder individuals ability to think and express themselves effectively. Most importantly, the personal touch and emotional connection associated with handwritten letters or cards cannot be replicated by digital alternatives. Handwritten messages convey sincerity, thoughtfulness, and a see of intimacy. Losing the art of handwriting may irrevocably erode these meaningful human connections.\nIn conclusion, the decline of handwriting can be attributed to the prevalence of technology and practical considerations. While there are advantages to digital communication and typing, the negative implications for cognitive development and personal connections are more significant and must be mitigated by the relevant authorities.\nArguing that it is a positive development\nIn today’s digital age, fewer and fewer people write by hand and using a pen or pencil. This shift can be attributed to pared technological advancements and changing educational practices, and I firmly believe that it is a positive development. The move away from handwriting not only enhances efficiency and accessibility but also reflects progress towards modern communication methods.\nThe decline in handwriting is primarily driven by the ubiquity of digital devices and evolving communication needs. With the widespread availability of computers, smartphones, and tablets, individuals find it more convenient to type and send messages instantly. Educational institutions are increasingly incorporating technology into their curricula, emphasizing digital literacy over traditional penmanship. Moreover, the rise of social media and email as primary communication tools has diminished the need for handwritten correspondence. These factors have collectively contributed to a cultural shift where efficiency and speed in communication are prioritized over the art of handwriting.\nTransitioning from handwriting to digital writing brings several significant benefits. Firstly, digital communication enhances productivity by enabling rapid information exchange and easy editing, which is especially beneficial in professional and academic settings. Additionally, typing reduces issues related to illegible handwriting, thereby minimizing misunderstandings. Digital platforms also offer vast storage and the ability to search through documents instantly, streamlining information management. Furthermore, this shift is environmentally friendly, as it reduces the consumption of paper and other stationery supplies. Overall, the adoption of digital writing foster greater accessibility and adaptability in our fast-pace, globalized world, contributing to more efficient and sustainable practices.\nIn conclusion, the decreasing reliance on handwriting in favor of digital communication is driven by technological advancements and evolving social practices. I contend that this development is positive, as it significantly boosts efficiency, clarity, and environmental sustainability while aligning with the demands of modern life. Embracing digital writing is not only a natural progression but also a vital step towards more effective and accessible communication in the 21st century."
  },
  {
    "objectID": "math/math-statistic-why-normal-distribution-formular/index.html",
    "href": "math/math-statistic-why-normal-distribution-formular/index.html",
    "title": "Why we have normal distribution formular",
    "section": "",
    "text": "To do\n\n\n\nThis is just a photo I took last time I was in Dundee.\n\n\nThat’s it! go for it!"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#i.-overview-of-statistical-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#i.-overview-of-statistical-inference",
    "title": "Statistical Inference and Learning",
    "section": "I. Overview of Statistical Inference",
    "text": "I. Overview of Statistical Inference\n\nDefinition:\nStatistical inference (often called “learning” in computer science) is the process of using data to deduce the underlying distribution \\(F\\) that generated the data. This may involve estimating the entire distribution or specific features (such as the mean).\nApplications:\n\nExtracting meaningful information from data\n\nMaking informed decisions and predictions\n\nServing as the foundation for more advanced topics in statistics and machine learning"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#ii.-modeling-approaches",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#ii.-modeling-approaches",
    "title": "Statistical Inference and Learning",
    "section": "II. Modeling Approaches",
    "text": "II. Modeling Approaches\n\nA. Parametric Models\n\nDefinition:\nA model defined by a finite number of parameters.\n\nExample (Normal Distribution):\n\\[\nf(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nCharacteristics:\n\nSimpler to analyze and interpret\n\nMore efficient when the assumptions hold true\n\n\n\n\nB. Nonparametric Models\n\nDefinition:\nModels that do not restrict the distribution to a finite-dimensional parameter space.\nExamples:\n\nEstimating the entire cumulative distribution function (cdf)\\(F\\)\n\nEstimating a probability density function (pdf) with smoothness assumptions (e.g., assuming the pdf belongs to a [[Sobolev space]])\n\nCharacteristics:\n\nGreater flexibility to model complex data\n\nFewer assumptions about the form of the distribution"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.1-one-dimensional-parametric-estimation--",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.1-one-dimensional-parametric-estimation--",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.1: One-Dimensional Parametric Estimation -",
    "text": "Example 6.1: One-Dimensional Parametric Estimation -\nScenario: We observe independent Bernoulli(\\(p\\)) random variables \\(X_1, X_2, \\dots, X_n\\).\nGoal: Estimate the unknown parameter \\(p\\) (the probability of success).\nEstimator: The natural estimator is the sample mean: \\[ \\hat{p}_n = \\frac{1}{n}\\sum_{i=1}^n X_i. \\] Key Points: - Unbiasedness: \\[E(\\hat{p}_n) = p.\\] Thus, the estimator is unbiased.\nVariance: Since \\[\\operatorname{Var}(X_i) = p(1-p)\\], the variance of the estimator is\n\\[ \\operatorname{Var}(\\hat{p}_n) = \\frac{p(1-p)}{n}. \\] - Consistency: As \\(n\\) increases, the variance shrinks, making \\(\\hat{p}_n\\) a consistent estimator of \\(p\\).\n\n# Example 6.1: One-Dimensional Parametric Estimation (Bernoulli)\nimport numpy as np\n\n# True parameter for Bernoulli distribution\np_true = 0.7\nn = 1000  # number of observations\n\n# Generate n independent Bernoulli(p) observations (0 or 1)\nX = np.random.binomial(1, p_true, n)\n\n# Estimator: sample mean is the natural estimator for p\np_hat = np.mean(X)\n\nprint(\"Example 6.1: Bernoulli Parameter Estimation\")\nprint(\"True p:\", p_true)\nprint(\"Estimated p:\", p_hat)"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.2-two-dimensional-parametric-estimation--",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.2-two-dimensional-parametric-estimation--",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.2: Two-Dimensional Parametric Estimation -",
    "text": "Example 6.2: Two-Dimensional Parametric Estimation -\nScenario: Suppose \\[X_1, X_2, \\dots, X_n\\] are independent observations from a distribution \\[F\\] whose probability density function is given by a parametric family: \\[ f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right). \\] - Goal: Estimate the two parameters: the mean \\[\\mu\\] and the standard deviation \\[\\sigma\\]. - Nuisance Parameter: If we are primarily interested in \\[\\mu\\], then \\[\\sigma\\] becomes a nuisance parameter—an additional parameter that must be estimated but is not of direct interest. - Key Points: - Multidimensionality: The estimation problem involves simultaneous estimation of \\[\\mu\\] and \\[\\sigma\\]. - Methods: Techniques such as maximum likelihood estimation (MLE) are commonly used, sometimes incorporating methods (like profile likelihood) to eliminate the effect of nuisance parameters when focusing on \\[\\mu\\]. . # Analytical Explanation of Two Nonparametric Estimation Examples\nBelow, we analyze and explain two examples that illustrate nonparametric estimation techniques: one for estimating the cumulative distribution function (CDF) and another for estimating the probability density function (PDF)."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.3-nonparametric-estimation-of-the-cdf",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.3-nonparametric-estimation-of-the-cdf",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.3: Nonparametric Estimation of the CDF",
    "text": "Example 6.3: Nonparametric Estimation of the CDF\n\nProblem Statement\n\nData:\nWe have independent observations \\[X_1, X_2, \\dots, X_n\\] drawn from an unknown distribution with CDF \\[F\\].\nObjective:\nEstimate the entire cumulative distribution function \\[F\\], assuming minimal assumptions—namely, that \\[F\\] is any valid CDF (denoted by \\[\\mathcal{F}_{\\text{ALL}}\\]).\n\n\n\nApproach\n\nEstimator:\nThe natural nonparametric estimator for the CDF is the empirical distribution function (EDF) defined as: \\[\n\\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\},\n\\] where \\[\\mathbf{1}\\{X_i \\le x\\}\\] is an indicator function that is 1 if \\[X_i \\le x\\] and 0 otherwise.\n\n\n\nWhy This Works\n\nMinimal Assumptions:\nNo specific parametric form for \\[F\\] is assumed; all that is required is that \\[F\\] is a valid CDF. This makes the method very general.\nConvergence Properties:\nThe Glivenko–Cantelli theorem guarantees that the empirical CDF converges uniformly to the true CDF: \\[\n\\sup_x \\left| \\hat{F}_n(x) - F(x) \\right| \\to 0 \\quad \\text{as} \\quad n \\to \\infty.\n\\] This property ensures that the estimator is consistent.\nIntuitive Interpretation:\nThe EDF simply calculates the proportion of observations less than or equal to a given value, which is the natural way to “build” the CDF from data."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.4-nonparametric-density-estimation",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.4-nonparametric-density-estimation",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.4: Nonparametric Density Estimation",
    "text": "Example 6.4: Nonparametric Density Estimation\n\nProblem Statement\n\nData:\nAgain, we have independent observations \\[X_1, X_2, \\dots, X_n\\] from a distribution with CDF \\[F\\]. Let the associated PDF be \\[f = F'\\].\nObjective:\nEstimate the PDF \\[f\\]. However, unlike the CDF, estimating the density function nonparametrically is not possible under the sole assumption that \\[F\\] is any CDF.\n\n\n\nNeed for Additional Assumptions\n\nIll-Posed Without Smoothness:\nThe space of all CDFs (denoted by \\[\\mathcal{F}_{\\text{ALL}}\\]) is too vast; a generic CDF need not be differentiable. Even if a density exists, it can be highly irregular, making consistent estimation difficult or impossible.\nIntroducing Smoothness via Sobolev Spaces:\nTo estimate \\[f\\] reliably, we assume that \\[f\\] belongs to a more restricted function class. One common assumption is that \\[f\\] lies in a Sobolev space (denoted by \\[\\mathcal{F}_{\\text{SOB}}\\]).\nFor instance, one might assume: \\[\n\\mathcal{F}_{\\text{SOB}} = \\left\\{ f \\in \\mathcal{F}_{\\text{DENS}} : \\int \\left(f^{(s)}(x)\\right)^2 dx &lt; \\infty \\right\\},\n\\] where:\n\n\\[\\mathcal{F}_{\\text{DENS}}\\] is the set of all probability density functions.\n\\[f^{(s)}(x)\\] denotes the \\[s\\]-th derivative of \\[f\\].\nThe condition \\[\\int \\left(f^{(s)}(x)\\right)^2 dx &lt; \\infty\\] ensures that \\[f\\] is not “too wiggly” or irregular.\n\n\n\n\nEstimation Methods\n\nKernel Density Estimation (KDE):\nWith the smoothness assumption in place, methods such as kernel density estimation can be employed. A kernel density estimator has the form: \\[\n\\hat{f}_n(x) = \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right),\n\\] where:\n\n\\[K(\\cdot)\\] is a smooth kernel function (e.g., Gaussian).\n\\[h\\] is a bandwidth parameter that controls the smoothness of the estimate.\n\n\n\n\nWhy These Assumptions are Necessary\n\nRegularization:\nThe smoothness condition imposed by the Sobolev space helps regularize the estimation problem. It restricts the set of possible densities to those that have bounded variation or a controlled number of oscillations.\nImproved Convergence:\nSmoothness assumptions lead to better convergence properties of the density estimator, allowing for rates of convergence that can be rigorously analyzed.\nPractical Feasibility:\nIn many real-world scenarios, the underlying density is indeed smooth (e.g., physical phenomena, economic variables), making this assumption both realistic and useful."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#summary",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#summary",
    "title": "Statistical Inference and Learning",
    "section": "Summary",
    "text": "Summary\n\nExample 6.3:\n\nTask: Estimate the CDF \\[F\\] from data with minimal assumptions.\nMethod: Use the empirical CDF \\[\\hat{F}_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\}\\].\nKey Property: Convergence guaranteed by the Glivenko–Cantelli theorem.\n\nExample 6.4:\n\nTask: Estimate the density \\[f\\] from data.\nChallenge: Estimation is ill-posed without additional assumptions.\nSolution: Assume that \\[f\\] is smooth by requiring it to belong to a Sobolev space (e.g., \\[\\mathcal{F}_{\\text{DENS}} \\cap \\mathcal{F}_{\\text{SOB}}\\]), then use methods like kernel density estimation.\nBenefit: Smoothness constraints make the problem well-posed and lead to estimators with favorable convergence properties.\n\n\nThese examples highlight the progression from estimating a distribution function under minimal assumptions to needing extra regularity conditions when estimating derivatives like the density."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iii.-core-concepts-in-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iii.-core-concepts-in-inference",
    "title": "Statistical Inference and Learning",
    "section": "III. Core Concepts in Inference",
    "text": "III. Core Concepts in Inference\n\n1. Point Estimation\n\nConcept:\nA point estimator is a function of the data, denoted as\n\\[\n\\hat{\\theta}_n = g(X_1, X_2, \\dots, X_n)\n\\]\nused to provide a single “best guess” for the unknown parameter \\[\\theta\\].\nKey Properties:\n\nBias:\n\\[\n\\text{bias}(\\hat{\\theta}_n) = E(\\hat{\\theta}_n) - \\theta\n\\]\nVariance and Standard Error (se):\n\\[\n\\text{se} = \\sqrt{Var(\\hat{\\theta}_n)}\n\\]\nMean Squared Error (MSE):\n\\[\n\\text{mse} = E\\left[(\\hat{\\theta}_n - \\theta)^2\\right] = \\text{bias}^2(\\hat{\\theta}_n) + Var(\\hat{\\theta}_n)\n\\]\nConsistency:\nAn estimator is consistent if\n\\[\n\\hat{\\theta}_n \\xrightarrow{P} \\theta \\quad \\text{as } n \\to \\infty\n\\]\nAsymptotic Normality:\nMany estimators satisfy\n\\[\n\\frac{\\hat{\\theta}_n - \\theta}{\\text{se}} \\approx N(0, 1)\n\\]\nfor large samples, which facilitates the construction of confidence intervals.\n\n\n\n\n2. Confidence Sets\n\nConcept:\nA confidence interval (or set) is a range constructed from the data that, over many repetitions of the experiment, contains the true parameter \\[\\theta\\] with a specified probability (coverage).\nExample (Normal-Based Interval):\nWhen \\[\\hat{\\theta}_n\\] is approximately normally distributed, an approximate \\[1-\\alpha\\] confidence interval is: \\[\nC_n = \\left( \\hat{\\theta}_n - z_{\\alpha/2}\\,\\text{se}, \\quad \\hat{\\theta}_n + z_{\\alpha/2}\\,\\text{se} \\right)\n\\] where \\[z_{\\alpha/2}\\] is the quantile of the standard Normal distribution such that \\[\nP(Z &gt; z_{\\alpha/2}) = \\frac{\\alpha}{2}.\n\\]\n\n\n\n3. Hypothesis Testing\n\nConcept:\nHypothesis testing involves formulating a null hypothesis \\[H_0\\] (a default statement, such as a coin being fair) and an alternative hypothesis \\[H_1\\], then using the data to decide whether there is sufficient evidence to reject \\[H_0\\].\nExample (Testing Coin Fairness):\n\\[\nH_0: p = 0.5 \\quad \\text{versus} \\quad H_1: p \\neq 0.5\n\\]\nProcess:\n\nDefine an appropriate test statistic (e.g., \\[T = |\\hat{p}_n - 0.5|\\])\n\nSet a significance level \\[\\alpha\\]\n\nDetermine the rejection region based on \\[\\alpha\\] or compute a p-value\n\nReject \\[H_0\\] if the test statistic falls into the rejection region"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iv.-frequentist-vs.-bayesian-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iv.-frequentist-vs.-bayesian-inference",
    "title": "Statistical Inference and Learning",
    "section": "IV. Frequentist vs. Bayesian Inference",
    "text": "IV. Frequentist vs. Bayesian Inference\n\nFrequentist Inference:\n\nTreats parameters as fixed but unknown\n\nFocuses on the properties of estimators over repeated sampling (e.g., confidence intervals, hypothesis tests)\n\nBayesian Inference:\n\nTreats parameters as random variables with prior distributions\n\nUses Bayes’ theorem to update beliefs in light of new data, allowing direct probability statements about parameters\n\nComparison:\n\nFrequentist methods emphasize long-run frequency properties.\n\nBayesian methods provide a framework for incorporating prior knowledge and making probabilistic statements about parameters."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#v.-additional-information",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#v.-additional-information",
    "title": "Statistical Inference and Learning",
    "section": "V. Additional Information",
    "text": "V. Additional Information\n\nBibliographic References\n\nElementary Level:\n\nDeGroot and Schervish (2002)\n\nLarsen and Marx (1986)\n\nIntermediate Level:\n\nCasella and Berger (2002)\n\nBickel and Doksum (2000)\n\nRice (1995)\n\nAdvanced Level:\n\nCox and Hinkley (2000)\n\nLehmann and Casella (1998)\n\nLehmann (1986)\n\nvan der Vaart (1998)\n\n\n\n\nExercises\n\nPoisson Estimation:\nFor \\[X_1, X_2, \\dots, X_n \\sim \\text{Poisson}(\\lambda)\\] with the estimator\n\\[\n\\hat{\\lambda} = \\frac{1}{n}\\sum_{i=1}^n X_i,\n\\]\ndetermine the bias, standard error, and mean squared error.\nUniform Distribution Estimation (Method 1):\nFor \\[X_1, X_2, \\dots, X_n \\sim \\text{Uniform}(0, \\theta)\\] and the estimator\n\\[\n\\hat{\\theta} = \\max\\{X_1, X_2, \\dots, X_n\\},\n\\]\ncalculate the bias, standard error, and mse.\nUniform Distribution Estimation (Method 2):\nFor the same model with the estimator\n\\[\n\\hat{\\theta} = 2X_{(n)},\n\\]\nwhere \\[X_{(n)}\\] is the maximum, compute the bias, standard error, and mse."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#vi.-key-takeaways",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#vi.-key-takeaways",
    "title": "Statistical Inference and Learning",
    "section": "VI. Key Takeaways",
    "text": "VI. Key Takeaways\n\nInference Fundamentals:\nLearning how to deduce properties of a population from a sample is central to statistics and machine learning.\nModel Choice:\n\nParametric models are simpler but rely on strong assumptions.\n\nNonparametric models offer flexibility with fewer assumptions.\n\nEstimator Evaluation:\nProperties such as bias, variance (or standard error), and mean squared error are essential in assessing the quality of estimators.\nConfidence and Testing:\n\nConfidence intervals quantify the uncertainty in estimates.\n\nHypothesis testing provides a formal framework for decision-making.\n\nPhilosophical Approaches:\nThe frequentist and Bayesian paradigms provide different perspectives on probability and inference, influencing how uncertainty is quantified and interpreted."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Development Indicators by Continent\n\n\n\n\n\n\n\n\n\n\n\nGapminder Analytics Group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFamily Man Retires at 39 – Extreme Early Retirement | FIRE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Nextcloud in Ubuntu Server\n\n\n\n\n\n\nStatistical\n\n\n\nYour post description\n\n\n\n\n\nFeb 26, 2025\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\nPost template\n\n\n\n\n\n\ntemplate\n\n\nany-category-you-want\n\n\n\nYour post description\n\n\n\n\n\nSep 11, 2024\n\n\nYour name\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/program-parity-invariance/parity-invariance.html",
    "href": "programming/program-parity-invariance/parity-invariance.html",
    "title": "Parity invariance",
    "section": "",
    "text": "Parity invariance means that the number of times a position is flipped determines its final value. If a position is flipped an odd number of times, its value changes, but if it is flipped an even number of times, it stays the same.\nConsider the array [1, 0, 0, 1, 0, 1, 1]. We start by flipping three consecutive elements to try and transform all 0s into 1s. First, flipping the subarray [0, 0, 1] at indices 1..3 changes the array to [1, 1, 1, 0, 0, 1, 1]. Then, flipping [0, 0, 1] at indices [3..5] gives [1, 1, 1, 1, 1, 0, 1]. Finally, flipping [1, 0, 1] at indices [1, 0, 1] at indices 4..6 results in [1, 1, 1, 1, 1, 1, 0, 1, 0].\nAt this point, we see that the 0s at positions 4 and 6 remain, and there is no way to flip them without also flipping the other elements. Since we can only flip three elements at a time, we cannot isolate these 0s in a way that allow us to change them to 1s. This happens because these positions were flipped an even number of times, so they retain their original value. Because of this parity constraint, the transformation is impossible."
  },
  {
    "objectID": "programming/leetcode-maximum-subarray/index.html",
    "href": "programming/leetcode-maximum-subarray/index.html",
    "title": "Maximum subarray",
    "section": "",
    "text": "Given an integer array nums, find the subarray with the largest sum, and return its sum.\nExample 1:\nInput: nums = [-2,1,-3,4,-1,2,1,-5,4] Output: 6 Explanation: The subarray [4,-1,2,1] has the largest sum 6. Example 2:\nInput: nums = [1] Output: 1 Explanation: The subarray [1] has the largest sum 1. Example 3:\nInput: nums = [5,4,-1,7,8] Output: 23 Explanation: The subarray [5,4,-1,7,8] has the largest sum 23.\nConstraints:\n1 &lt;= nums.length &lt;= 105 -104 &lt;= nums[i] &lt;= 104\nFollow up: If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle."
  },
  {
    "objectID": "programming/leetcode-maximum-subarray/index.html#brute-force",
    "href": "programming/leetcode-maximum-subarray/index.html#brute-force",
    "title": "Maximum subarray",
    "section": "Brute force",
    "text": "Brute force\n    # brute force approach\n\n    # find all left, and right \n    # cal sum of each\n    # return max\n    # T.C: O(n^2)\n\nfrom typing import List\ndef maxSubArray(self, nums: List[int]) -&gt; int:\n    left, ans = 0, nums[0]\n    for left in range(len(nums)):\n        cur = 0\n        for right in range(left, len(nums)):\n            cur += nums[right]\n            ans = max(ans, cur)\n\n    return ans\n\nit will TLE."
  },
  {
    "objectID": "programming/leetcode-house-robber/house-robber.html",
    "href": "programming/leetcode-house-robber/house-robber.html",
    "title": "House robber",
    "section": "",
    "text": "Topic: array, dymanic programming\n\nQuestion\nYou are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and it will automatically contact the police if two adjacent houses were broken into on the same night.\nGiven an integer array nums representing the amount of money of each house, return the maximum amount of money you can rob tonight without alerting the police.\nExample 1:\n\nInput: nums = [1,2,3,1]\nOutput: 4\nExplanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4.\n\nExample 2:\n\nInput: nums = [2,7,9,3,1]\nOutput: 12\nExplanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1). Total amount you can rob = 2 + 9 + 1 = 12.\n\nConstraints:\n1 &lt;= nums.length &lt;= 100\n0 &lt;= nums[i] &lt;= 400\n\n\nApproaches"
  },
  {
    "objectID": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html",
    "href": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html",
    "title": "How to approach most DP problems",
    "section": "",
    "text": "To solve a dp problem: https://leetcode.com/problems/house-robber/solutions/156523/from-good-to-great-how-to-approach-most-of-dp-problems/\nThis particular problem can be approached using the following sequence:"
  },
  {
    "objectID": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html#step-5-iterative-2-variables-bottom-up",
    "href": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html#step-5-iterative-2-variables-bottom-up",
    "title": "How to approach most DP problems",
    "section": "Step 5: Iterative + 2 variables (bottom-up)",
    "text": "Step 5: Iterative + 2 variables (bottom-up)\nIn the previous step, we use only memo[i] and memo[i-1], so going just 2 step back. We can hold them in 2 variables instead. This optimization is met in Fibonacci sequence creation and some other problems [[Optimize Fibonacci]]\n    def rob(self, nums: List[int]) -&gt; int:\n        if len(nums) == 0:\n            return 0\n        prev1, prev2 = 0, 0\n        for num in nums:\n            temp = prev1\n            prev1 = max(prev2 + num, prev1)\n            prev2 = temp\n        return prev1"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html",
    "title": "Divide Array Into Equal Pairs",
    "section": "",
    "text": "Topic: array, hash table, bit manipulation, counting"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#count-array",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#count-array",
    "title": "Divide Array Into Equal Pairs",
    "section": "Count array",
    "text": "Count array\nnums that have 2 * n intenger\ndivide nums into n pairs\n1 element in 1 pair\nelements in pair is equal\nreturn true if can devide to n pair,\nSo, we can use count array\nif all even \\`return true\\`\n\nelse \\`return false\\`\nTC: O(n)\nclass Solution:\n    def divideArray(self, nums: List[int]) -&gt; bool:\n    ans = True\n    count_array = [0]*(500+1)\n    for num in nums:\n        count_array[num] += 1\n    print(count_array)\n    for num in count_array:\n        if num % 2 != 0:\n            return False\n    return ans"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#map",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#map",
    "title": "Divide Array Into Equal Pairs",
    "section": "Map",
    "text": "Map\nlike approach 1, we can use map for that (better code)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        frequency = Counter(nums)\n        # check consecutive pairs in sorted array\n        return all(count % 2 == 0 for count in frequency.values())"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#bool-array",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#bool-array",
    "title": "Divide Array Into Equal Pairs",
    "section": "Bool array",
    "text": "Bool array\nan improve, use boolean array\nO(n)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n\n        max_num = max(nums)\n\n        needs_pair = [False] * (max_num + 1)\n\n        for num in nums:\n            needs_pair[num] = not needs_pair[num]\n\n        return not any(needs_pair[num] for num in nums)"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#sorted",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#sorted",
    "title": "Divide Array Into Equal Pairs",
    "section": "Sorted",
    "text": "Sorted\nsorted that can have TC: O(nlogn)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        nums.sort()\n        # check consecutive pairs in sorted array\n        return all(nums[i] == nums[i+1] for i in range (0, len(nums), 2))"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#hash-set",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#hash-set",
    "title": "Divide Array Into Equal Pairs",
    "section": "Hash set",
    "text": "Hash set\nwe can store a element when first meet it, and even get it, we remote from set\nwhen retrieve all, if set have element.\nhash set have TC of lookup, addition, removal in constant time.\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        unpaired = set()\n\n        for num in nums:\n            if num in unpaired:\n                unpaired.remove(num)\n            else:\n                unpaired.add(num)\n        return not unpaired"
  },
  {
    "objectID": "programming/convolution/index.html",
    "href": "programming/convolution/index.html",
    "title": "Image classification",
    "section": "",
    "text": "Image Classification\nThis is the task of assigning an input image one label from a fixed categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, many other seemingly distinct CV tasks (such as object detection, segmentation) can be reduced to image classification.\nExample: In the image blow in image classification model takes a single image and assigns probabilities to 4 label {cat, dog, hat, mug}. Computer view a image as an one large 3-dimensional array of numbers. In this example, the cat image is 248 pixel wide, 400 pixel tall, and has three color channel Red, Green, Blue. Therefore, the image consists of \\(248 \\times 400 \\times 3\\) numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is turn this quarter of a million numbers into a single label, such as “cat”. \nChallenges: Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for human to perform, it is worth considering the challenges involved from the perspective of a CV algorithm. As we present list of challenges below.\n\nViewpoint variation: A single instance of an object can be oriented in many ways with respect to the camera.\nScale variation: Visual classes often exhibit variation of their size (size in the real world, not only in terms of their extent in the image)\nDeformation: Many objects of interest are not rigid bodies and can be deformed in extremes ways.\nOcclusion: The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.\nIllumination conditions: The effect of illumination are drastic on the pixel level.\nBackground clutter: The objects of interest may blend into their environment, making them hard to identify.\nIntra-class variation: The classes of interest can be often be relatively broad, such as chair. Their are many different types of these objects, each with their own appearance. \n\nA good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.\n\nCross product of variations This refers to the combination of all possible variations (e.g. viewpoint, scale, deformation, occlusion, illumination, etc.) that can occur within a class. The term “cross production” here is metaphorical, inpsired by the mathematical concept of a Cartesian product, which generate all possible combinations of elements from multiple sets.\n\nExample: A chair might appear:\n\nRotate (viewpoint variation)\nPartially hidden (occlusion)\nUnder bright sunlight (illumination)\nWhile being non-rigid (deformation) The model must recognize it as a “chair” despite this complex combination of variations Invariance Requirement: The model must be invariant to these variations, meaning its prediction for a class should not change even when factors alter the object’s appearance.\n\n\nInter-class variation There are difference between distinct classes (e.g. chairs vs tables). A model must retain sensitivity to these differences to avoid confusing classes, even when they share superficial similarities.\n\nExample: A “stool” (class: chair) and a “small table” (class: table) might both appear at similar scales or under similar lighting. The model must distinguish them based on defining feature (e.g. height, presence of a backrest).\nSensitivity Requirement: The model must preserver discriminate features that separate classes, even when intra-class variations (e.g. deformation in chairs) are extreme. Data-driven approach: How might we go about writing an algorithm that can classify images into distinct categories? Unlike writing an algorithm for, for example, sorting a list of numbers, it is not obvious how one might write an algorithm for identifying cats in images. Therefore, instead of trying to specify what every one of the categories of interest look like directly in code, the approach that we will take is not unlike one we would take with a child: we’re going to provide the computer with many examples of each class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. This approach is referred to as a data-driven approach, since it relies on first accumulating a training dataset of labeled images. Here is an example of what such a dataset might look like:\n\n\n\nThe image classification pipeline We’ve seen that the task in Image Classification is to taken an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows: - Input: Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the learning set. - Learning: Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model. - Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the ground truth).\n\n\nNearest Neighbor Classifier\nAs our first approach, we will develop what we call a Nearest Neighbor Classifier. This classifier has nothing to do with Convolution Neural Networks and it is very rarely used in practice, but i will allow us to get an ideal about the basic approach to an image classification.\nExample image classification dataset: CIFAR-10. CIFAR-10 datasetThis dataset consist of 10 classes (for example airplane, automobile, bird, etc). These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images. In the image below we can see 10 random example images from each one of the 10 classes.\n Left: Example images from the CIFAR-10 dataset. Right: first column shows a few test images and next to each we show the top 10 nearest neighbors in the training set according to pixel-wise difference.\nSuppose now that we are given the CIFAR-10 training set of 50,000 images (5000 image for every one of the labels), and we wish to label the remaining 10,000. The nearest neighbor classifier will take a test image, compare it to every single one of the training images, and predict the label of the closest training image. In the image above and on the right we can see an example result of such a procedure for 10 example test images. Notice that in only about 3 of 10 examples of an image of the same class is retrieved, while in other 7 examples this is not the case. For example, in the 8th row the nearest training image to the horse head is a red car, presumably due to the strong black background. As a result, this image of a horse would in this case be mislabeled as a car.\nOne of the simplest possibilities is to compare the images pixel by pixel and add up all the differences. In other words, given two images and representing them as vectors \\(I_1, I_2\\), a reasonable choice for comparing them might be the L1 distance.\n\\[d_1(I_1, I_2)=\\Sigma_p |I_1^p - I_2^p|\\] Where the sum is taken over all pixels. Here is the procedure visualized:\n![[nneg.jpeg]]\nimport numpy as numpy\n\nclass NearestNeighbor(object):\n    def __init__(self):\n        pass\n    def train(self, X, y):\n        \"\"\" X is N \\times D where each row is an example. Y is 1-dimension of size N\"\"\"\n        # the nearest neighbor classifier simpply remembers all the training data\n        self.Xtr = X\n        self.ytr = y\n    def predict(self, X):\n        \"\"\" X is N \\times D where each row is an example we wish to predict label for\"\"\"\n        num_test = X.shape[0]\n        # lets make sure that the output type matches the input type\n        Ypred = np.zeros(num_test, dtype = self.dtype)\n        # loop over all test rows\n        for i in range(num_test):\n            # find the nearest training image to the i'th test image\n            # using the L1 distance (sum of absolute value differences)\n            distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n            min_index = np.argmin(distances) # get the index with smallest distance\n            Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n        return Ypred"
  },
  {
    "objectID": "programming/leetcode-longest-nice-subarray/leetcode-Longest-Nice-Subarray.html",
    "href": "programming/leetcode-longest-nice-subarray/leetcode-Longest-Nice-Subarray.html",
    "title": "leetcode-Longest-Nice-Subarray",
    "section": "",
    "text": "Topic: array, bit manipulation, slide windown\n\nQuestion\nYou are given an array nums consisting of positive integers.\nWe call a subarray of nums nice if the bitwise AND of every pair of elements that are in different positions in the subarray is equal to 0.\nReturn the length of the longest nice subarray.\nA subarray is a contiguous part of an array.\nNote that subarrays of length 1 are always considered nice.\nExample 1:\nInput: nums = [1,3,8,48,10] Output: 3 Explanation: The longest nice subarray is [3,8,48]. This subarray satisfies the conditions: - 3 AND 8 = 0. - 3 AND 48 = 0. - 8 AND 48 = 0. It can be proven that no longer nice subarray can be obtained, so we return 3.\nExample 2:\nInput: nums = [3,1,5,11,13] Output: 1 Explanation: The length of the longest nice subarray is 1. Any subarray of length 1 can be chosen.\nConstraints:\n1 &lt;= nums.length &lt;= 105\n1 &lt;= nums[i] &lt;= 109\n\n\nAnalysis\n    example 1:\n    [1, 3, 8, 48, 10]\n    3 in bin: 0011\n    8 in bin: 1000\n    48d = 1100000\n    10d = 1010\n    0011 AND 1000 = 0\n    3, 8, 48 AND = 0\n    10 and 8 not = 0\n    so, if pair AND = 0\n    that pair have no common bit\n    so, we store a bit array to check the state of bit\n    and if, to better, we just need to store number of bit in that array instead.\n    oh no, it wrong.\n    so if we must store a array.\n    no, we can use bitwise operator & to check if a AND b == 0 or not\n    and OR for cumulative bit\n    x = 5\n    # 101\n    x |= 3\n    # 3 == 011\n    # 101 |= 011 = 111\n    print(x)\n    # 7\n    to search for longest (can use i, and j) for check all the begin and end\n    improve it by two pointer to decrease TC from O(n^2) to O(n)\n    and now, how to get rid of num of left from cumulative bit in slide windown\n    check that case: [011, 100]\n    now, culmulative bit: 111\n    we want it after left += 1, is 100\n    in XOR: 111 XOR 011 == 100\n    XOR parameter in python is ^=\n\n\nCode\n    def longestNiceSubarray(self, nums: List[int]) -&gt; int:\n\n        cumulative_bit = 0\n\n        ans = 0\n\n        left = 0\n\n        for right in range(len(nums)):\n\n            # when AND not ease\n\n            while cumulative_bit & nums[right] != 0: # right can not cumulative, increase left until it can ease, use XOR for get rid of it\n\n                cumulative_bit ^= nums[left]\n\n                left += 1\n\n            # until can AND\n\n            # use OR to cumulative it\n\n            cumulative_bit |= nums[right]\n\n            ans = max(ans, right - left + 1)\n\n        return ans"
  },
  {
    "objectID": "programming/leetcode-count-good-triplets/index.html",
    "href": "programming/leetcode-count-good-triplets/index.html",
    "title": "ouse robber",
    "section": "",
    "text": "Given an array of integers arr, and three integers a, b and c. You need to find the number of good triplets.\nA triplet (arr[i], arr[j], arr[k]) is good if the following conditions are true:\n\n0 &lt;= i &lt; j &lt; k &lt; arr.length\n|arr[i] - arr[j]| &lt;= a\n|arr[j] - arr[k]| &lt;= b\n|arr[i] - arr[k]| &lt;= c\n\nWhere |x| denotes the absolute value of x.\nReturn the number of good triplets.\nExample 1:\nInput: arr = [3,0,1,1,9,7], a = 7, b = 2, c = 3 Output: 4 Explanation: There are 4 good triplets: [(3,0,1), (3,0,1), (3,1,1), (0,1,1)].\nExample 2:\nInput: arr = [1,1,2,2,3], a = 0, b = 0, c = 1 Output: 0 Explanation: No triplet satisfies all conditions.\nConstraints:\n\n3 &lt;= arr.length &lt;= 100\n0 &lt;= arr[i] &lt;= 1000\n0 &lt;= a, b, c &lt;= 1000"
  },
  {
    "objectID": "programming/leetcode-count-good-triplets/index.html#approach-2-optimized-enumeration",
    "href": "programming/leetcode-count-good-triplets/index.html#approach-2-optimized-enumeration",
    "title": "ouse robber",
    "section": "Approach 2: Optimized enumeration",
    "text": "Approach 2: Optimized enumeration\nWe need to eliminate one loop or make one of them constant-time.\nWe could try fixing (j,k) and then counting valid i &lt; j. That \\(O(n^2)\\) pairs, and if we can count the \\(i\\)s in \\(O(1)\\) each, we down to \\(O(n^2)\\) total.\n\nTranslate the \\(i\\)-constraints into an interval\nFor a given \\((j,k)\\), the condition on \\(i\\) are \\[|arr[i] - arr[j]| \\leq a\\] so, \\[arr[i] \\in [arr[j]-a, arr[j]+a]\\]\nand \\[|arr[i]-arr[k]|\\leq c\\] so, \\[arr[i]\\in[arr[k]-c, arr[k]+c]\\]\nTheir intersection is a single interval \\([l, r]\\). So we just need to count how many prior \\(i&lt;j\\) have \\(arr[i] \\in [l, r]\\).\n\n\nChoose a data structure for last range-count\nSince arr[i] is small-range, maintain a frequency array freq[0..M] for all seen indices \\(&lt;j\\), and its prefix sums sum[v] =\\(\\Sigma_{u=0}^v freq[u]\\)\nThen the count in \\([l, r]\\) is simply sum[r]-sum[l-1] in \\(O(1)\\).\n\n\nEnsure the \\(i&lt;j\\) ordering\nWe iterate \\(j\\) from 1 to \\(n\\). Before handling any \\((j, k)\\) pairs, our freq/sum reflects exact indices 1 through \\(j-1\\).\nFor each \\(k&gt;j\\), check \\(|arr[j] - arr[k]|\\leq b\\). If it passes, compute \\([l, r]\\) and do the \\(O(1)\\) range-sum query.\nAfter finishing all \\(k\\) for this \\(j\\), we insert \\(arr[j]\\) into freq (and update sum), before moving onto \\(j + 1\\).\nclass Solution:\n    def countGoodTriplets(self, arr: List[int], a: int, b: int, c: int) -&gt; int:\n        # ans: total count of valid (i,j,k)\n        ans = 0\n        \n        n = len(arr)\n        # total[v] will hold the prefix-sum of frequencies:\n        #   total[v] = number of i&lt;j with arr[i] ≤ v\n        # we assume arr[i] ∈ [0..1000], so we size total to 1001\n        total = [0] * 1001\n\n        # Move j from 0 to n-1\n        for j in range(n):\n            # Pair j with every k&gt;j\n            for k in range(j + 1, n):\n                # First check the |arr[j] - arr[k]| ≤ b constraint\n                if abs(arr[j] - arr[k]) &lt;= b:\n                    # Compute the interval of allowed arr[i] from j’s constraint\n                    lj, rj = arr[j] - a, arr[j] + a\n                    # Compute the interval of allowed arr[i] from k’s constraint\n                    lk, rk = arr[k] - c, arr[k] + c\n\n                    # Intersection [l..r] of the two intervals\n                    # also clamp to [0..1000] to stay in array bounds\n                    l = max(0, lj, lk)\n                    r = min(1000, rj, rk)\n\n                    # If the intersection is non-empty, count how many\n                    # prior i&lt;j have arr[i] in [l..r] via prefix sums\n                    if l &lt;= r:\n                        if l == 0:\n                            ans += total[r]\n                        else:\n                            ans += total[r] - total[l - 1]\n\n            # After processing all k for this j, we “add” arr[j] into our\n            # prefix-sum structure so that future iterations see it.\n            # We do this by incrementing total[v] for all v ≥ arr[j].\n            # That way total[v] remains = # of arr[i] ≤ v for i&lt;next j.\n            for v in range(arr[j], 1001):\n                total[v] += 1\n\n        return ans"
  },
  {
    "objectID": "programming/leetcodeminimum-operations-to-make-binary-array-elements-equal-to-one-i/3191.html",
    "href": "programming/leetcodeminimum-operations-to-make-binary-array-elements-equal-to-one-i/3191.html",
    "title": "3191. Minimum Operations to Make Binary Array Elements Equal to One I",
    "section": "",
    "text": "You are given a\nnums.\nYou can do the following operation on the array any number of times (possibly zero):\nChoose any 3 consecutive elements from the array and flip all of them.\nFlipping an element means changing its value from 0 to 1, and from 1 to 0.\nReturn the minimum number of operations required to make all elements in nums equal to 1. If it is impossible, return -1.\nExample 1:\nInput: nums = [0,1,1,1,0,0]\nOutput: 3\nExplanation: We can do the following operations:\nChoose the elements at indices 0, 1 and 2. The resulting array is nums = [1,0,0,1,0,0].\nChoose the elements at indices 1, 2 and 3. The resulting array is nums = [1,1,1,0,0,0].\nChoose the elements at indices 3, 4 and 5. The resulting array is nums = [1,1,1,1,1,1].\nExample 2:\nInput: nums = [0,1,1,1]\nOutput: -1\nExplanation: It is impossible to make all elements equal to 1.\nConstraints:\n3 &lt;= nums.length &lt;= 105\n0 &lt;= nums[i] &lt;= 1\n\nAnalysis\nSince we can only flip three consecutive elements at a time, isolated 0s or certain patterns of 0s may prevent us from turning everything into 1. If the number of 0s in certain positions makes it impossible to fully eliminate them using groups of three, the transformation cannot be achieved."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html",
    "href": "programming/disjoint-set-union/index.html",
    "title": "Disjoint Set Union",
    "section": "",
    "text": "This data structure provides the following capabilities. We are given several elements, each of which is a separate set. A DSU will have an operation to combine any two sets, and it will be able to tell in which set a specific element is. The classical version also introduces a third operation, it can create a set from a new element.\nThus the basic interface of this data structure consists of only three operations: - make_set() - create a new set consisting of the new element v. - union_sets(a, b) - merges the two specified sets (the set in which the element a is located, and the set in which the element b is located). - find_set(v) - return the representative (also called leader) of the set that contains the element v. This representative is an element of its corresponding set. It is selected in each set by the data structure itself (and can change over time, namely after union_sets call). This representative can be used to check if two elements are part of the same set or not. a and b are exactly in the same set, if find_set(a) == find_set(b). Otherwise they are in different sets.\nAs described on more detail later, the data structure allows us to do each of these operations in almost \\(O(1)\\) time on average.\nAlso in one of the subsections an alternative structure of a DSU is explained, which achieves a slower average complexity of \\(O(\\log n)\\), but can be more powerful than the regular DSU structure."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#naive-implementation",
    "href": "programming/disjoint-set-union/index.html#naive-implementation",
    "title": "Disjoint Set Union",
    "section": "Naive implementation",
    "text": "Naive implementation\nWe can already write the first implementation of the Disjoint Set Union data structure. It will be pretty inefficient at first, but later we can improve it using two optimizations, so that it will take nearly constant time for each function call.\nAs we said, all the information about the sets of elements will be kept in an array parent.\nTo create a new set (operation make_set(v)), we simply create a tree with root in the vertex x, meaning that it is its own ancestor.\nTo combine two sets (operation union_sets(a, b)), we first find the representative of the set in which a is located, and the representative of the set in which b is located. If the representatives are identical, that we have nothing to do, the sets are already merged. Otherwise, we can simply specify that one of the representatives is the parent of the other representative - thereby combining the two trees.\nFinally the implementation of the find representative function (operation find_set(v)): we simply climb the ancestors of the vertex v until we reach the root, i.e. a vertex such that the reference to the ancestor leads to itself. This operation is easily implemented recursively.\nvoid make_set(int v) {\n    parent[v] = v;\n}\n\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    return find_set(parent[v]);\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b)\n        parent[b] = a;\n}\nHowever this implementation is inefficient. It is easy to construct an example, so that the trees degenerate into long chains. In that case each call find_set(v) can take \\(O(n)\\) time.\nThis is far away from the complexity that we want to have (nearly constant time). Therefore we will consider two optimizations that will allow to significantly accelerate the work."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#path-compression-optimization",
    "href": "programming/disjoint-set-union/index.html#path-compression-optimization",
    "title": "Disjoint Set Union",
    "section": "Path compression optimization",
    "text": "Path compression optimization\nThis optimization is designed for speeding up find_set.\nIf we call find_set() for some vertex x, we actually find the representative p for all vertices that we visit on the path between v and the actually representative p. The trick is to make the paths for all those nodes shorter, by setting the parent of each visited vertex directly to p.\nWe can see the operation in the following image. On the left there is a tree, and on the right side there is the compressed tree after calling find_set(7), which shortens the paths for the visited node 7, 5, 3 and 2.\n![[DSU_path_compression.png]]\nThe new implementation of find_set is as follows:\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    return parent[v] = find_set(parent[v]);\n}\nThe simple implementation does what was intended: first find the representative of the set (root vertex), and then in the process of stack unwinding the visited nodes are attached directly to the representative.\nThis simple modification of the operation already achieves the time complexity \\(O(\\log n)\\) per call on average (here without proof). There is a second modification, that will make it even faster."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#union-by-size-rank",
    "href": "programming/disjoint-set-union/index.html#union-by-size-rank",
    "title": "Disjoint Set Union",
    "section": "Union by size / rank",
    "text": "Union by size / rank\nIn this optimization we will change the union_set operation. To be precise, we will change which tree gets attached to the other one. In the naive implementation the second tree always got attached to the first one. In practice that can lead to trees containing chains of length \\(O(n)\\). With this optimization we will avoid this by choosing very carefully which tree gets attached.\nThere are many possible heuristic that can be used. Most popular are the following two approaches: In the first approach we use the size of the trees as rank, and in the second one we use the depth of the tree (more precisely, the upper bound on the tree depth, because the depth will get smaller when applying path compression).\nIn both approaches the essence of the optimization is the same: we attach the tree with the lower rank to the one with the bigger rank.\nHere is the implementation of union by size:\nvoid make_set(int v) {\n    parent[v] = v;\n    size[v] = 1;\n}\n\nvoid union_sets(int a, int b){\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (size[a] &lt; size[b])\n            swap(a, b);\n        parent[b] = a;\n        size[a] += size[b];\n    }\n}\nand here is the implementation of union by rank based on the depth of the trees:\nvoid make_set(int v) {\n    parent[v] = v;\n    rank[v] = 0;\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (rank[a] &lt; rank[b])\n            swap(a, b);\n        parent[b] = a;\n        if (rank[a] == rank[b])\n            rank[a]++;\n    }\n}\nBoth optimizations are equivalent in terms of time and space complexity. So in practice we can use any of them."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#time-complexity",
    "href": "programming/disjoint-set-union/index.html#time-complexity",
    "title": "Disjoint Set Union",
    "section": "Time complexity",
    "text": "Time complexity\nAs mentioned before, if we combine both optimizations - path compression with union by size / rank - we will reach nearly constant time queries. It turns out, that the final amortized complexity is \\(O(\\alpha(n))\\), where \\(\\alpha (n)\\) is the inverse Ackermann function, which grows very slowly. In fact it grows so lowly, that it doesn’t exceed 4 for all reasonable \\(n\\) (approximately \\(n&lt;10^{600}\\)).\nAmortized complexity is the total time per operation, evaluated over a sequence of multiple operations. The idea is to guarantee the total time of the entire sequence, while allowing single operations to be much slower than the amortized time. E.g. in our case a single call might take \\(O(\\log n)\\) in the worst case, but if we do \\(m\\) such calls back to back we will end up with an average time of \\(O(\\alpha (n))\\).\nWe will also not present a proof for this time complexity, since it is quite long and complicated.\nAlso, it’s worth mentioning that DSU with union by size / rank, but without path compression works in \\(O(\\log n)\\) time per query."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#linking-by-index-coin-flip-linking",
    "href": "programming/disjoint-set-union/index.html#linking-by-index-coin-flip-linking",
    "title": "Disjoint Set Union",
    "section": "Linking by index / coin-flip linking",
    "text": "Linking by index / coin-flip linking\nBoth union by rank and union by size require that we store additional data for each set, and maintain these values during each union operation. There exist also a randomized algorithm, that simplifies the union operation a little bit: linking by index.\nWe assign each set a random value called the index, and we attach the set with the smaller index to the one with the larger one. It is likely that a bigger set will have a bigger than the smaller set, therefore this operation is closely related to union by size. In fact it can be proven, that this operation has the same time complexity as union by size. However in practice it is slightly slower than union by size."
  },
  {
    "objectID": "programming/leetcode-minimum-cost-walk-in-weight-graph/index.html",
    "href": "programming/leetcode-minimum-cost-walk-in-weight-graph/index.html",
    "title": "Minimum Cost Walk in Weighted Graph",
    "section": "",
    "text": "Question\nThere is an undirected weighted graph with n vertices labeled from 0 to n - 1.\nYou are given the integer n and an array edges, where edges[i] = [ui, vi, wi] indicates that there is an edge between vertices ui and vi with a weight of wi.\nA walk on a graph is a sequence of vertices and edges. The walk starts and ends with a vertex, and each edge connects the vertex that comes before it and the vertex that comes after it. It’s important to note that a walk may visit the same edge or vertex more than once.\nThe cost of a walk starting at node u and ending at node v is defined as the bitwise AND of the weights of the edges traversed during the walk. In other words, if the sequence of edge weights encountered during the walk is w0, w1, w2, …, wk, then the cost is calculated as w0 & w1 & w2 & … & wk, where & denotes the bitwise AND operator.\nYou are also given a 2D array query, where query[i] = [si, ti]. For each query, you need to find the minimum cost of the walk starting at vertex si and ending at vertex ti. If there exists no such walk, the answer is -1.\nReturn the array answer, where answer[i] denotes the minimum cost of a walk for query i.\nExample 1:\nInput: n = 5, edges = [[0,1,7],[1,3,7],[1,2,1]], query = [[0,3],[3,4]]\nOutput: [1,-1]\nExplanation:\nTo achieve the cost of 1 in the first query, we need to move on the following edges: 0-&gt;1 (weight 7), 1-&gt;2 (weight 1), 2-&gt;1 (weight 1), 1-&gt;3 (weight 7).\nIn the second query, there is no walk between nodes 3 and 4, so the answer is -1.\nExample 2:\nInput: n = 3, edges = [[0,2,7],[0,1,15],[1,2,6],[1,2,1]], query = [[1,2]]\nOutput: [0]\nExplanation:\nTo achieve the cost of 0 in the first query, we need to move on the following edges: 1-&gt;2 (weight 1), 2-&gt;1 (weight 6), 1-&gt;2 (weight 1).\nConstraints:\n2 &lt;= n &lt;= 105\n0 &lt;= edges.length &lt;= 105\nedges[i].length == 3\n0 &lt;= ui, vi &lt;= n - 1\nui != vi\n0 &lt;= wi &lt;= 105\n1 &lt;= query.length &lt;= 105\nquery[i].length == 2\n0 &lt;= si, ti &lt;= n - 1\nsi != ti\n\n\nAnalysis\nLet’s determine when the answer to a query is -1. This happens when no walk exists between the two nodes, meaning they belong to different connected components.\nNow, suppose the two nodes belong to the same connected component. What is the minimum cost of a walk connecting them? As mentioned, the optimal walk includes as many edges as possible. Since revisiting an edge does not affect the total score, we can freely traverse the edges of the component, meaning that we can move back and forth to reach all of them. Therefore, the best way to achieve the lowest cost is to visit every edge in the component.\nTo efficiently find and process the connected components of the graph, we use the Disjoint Set (Union-Find) data structure. When we Union two nodes, we merge their entire groups, as now a path exists between every node in one group and every node in the other. To maintain efficiency, the root of the larger group is chosen as the representative of the merge group. This minimizes the time needed for future Find operations by reducing the number of steps required to reach the current representative."
  },
  {
    "objectID": "programming/docker-intro/index.html",
    "href": "programming/docker-intro/index.html",
    "title": "Docker basics",
    "section": "",
    "text": "This chapter introduces containerization with Docker and relevant concepts such as image and volume. By the end of this chapter you are able to:"
  },
  {
    "objectID": "programming/docker-intro/index.html#what-is-devops",
    "href": "programming/docker-intro/index.html#what-is-devops",
    "title": "Docker basics",
    "section": "What is DevOps",
    "text": "What is DevOps\nThe term itself consists of two parts Dev and Ops. Dev refers to the development of software and Ops to operations. Simple definition for DevOps would be that it means the release, configuring, and monitoring of software is in the hand people who develop it."
  },
  {
    "objectID": "programming/docker-intro/index.html#what-is-docker",
    "href": "programming/docker-intro/index.html#what-is-docker",
    "title": "Docker basics",
    "section": "What is Docker",
    "text": "What is Docker\n\n“Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers.” - from Wikipedia(opens in a new tab).\n\nSo we get two definitions: 1. Docker is a set of tools to deliver software incontainers. 2. Containers are packages of software. \nThe above image illustrate how containers include the application and its dependencies. These containers are isolated so that they don’t interfere with each other or the software running outside of the containers. If we want to enable interactions between them, Docker offers tools to do so."
  },
  {
    "objectID": "programming/docker-intro/index.html#benefits-from-containers",
    "href": "programming/docker-intro/index.html#benefits-from-containers",
    "title": "Docker basics",
    "section": "Benefits from containers",
    "text": "Benefits from containers\nContainers package applications. In different scenarios: ### Scenario 1: Works on my machine\nLet’s first take a closer look into what happens in web development without containers following the chain above starting from “Plan”.\nFirst we plan an application. Then our team of 1-n developers create the software. It works on one’s computer. It may even go through a testing pipeline working perfectly. We send it to the server and it does not work.\nThis is known as the “works on my machine” problem. The only way to solve this is by finding out what in tarnation the developer had installed on their machine that made the application work.\nContainers solve this problem by allowing the developer to personally run the application inside a container, which then includes all of the dependencies required for the app to work.\n\nYou may still occasionally hear about “works in my container” issues - these are often just usage errors. ### Scenario 2: Isolated environments\n\nWe have 5 different Python applications. We need to deploy them to a server that already has an application requiring Python 2.7 and of course none of our applications are 2.7. What do we do now?\nSince containers package the software with all of its dependencies, we package the existing app and all 5 new ones with their respective Python versions and that’s it.\nSometimes different parts of a system may change over time, possibly leading to the application not working. These changes may be anything from an operating system update to changes in dependencies.\n\nScenario 3: Development\nWe are brought into a dev team. They run a web app that uses other services when running: a Postgres database, MongoDB, Redis and a number of others. Simple enough, we install whatever is required to run the application and all of the applications that it depends on …\nWhat a headache to start installing and then managing the development databases on your own machine.\nThankfully, by the time we are told to do that we are already a Docker expert. With one command we get an isolated application, like Postgres or Mongo, running in our machine.\n\n\nScenario 4: Scaling\nStarting and stopping a Docker container has little overhead. But when we run our own Netflix or Facebook, we want to meet the changing demand. With some advanced tooling that we will learn about in parts 2 and 3, we can spin up multiple containers instantly and load balance traffic between them.\nContainer orchestration will be discussed in parts 2 and 3. But the simplest example: what happens when one application dies? The orchestration system notices it, splits traffic between the working replicas, and spins up a new container to replace the dead one."
  },
  {
    "objectID": "programming/docker-intro/index.html#virtual-machines",
    "href": "programming/docker-intro/index.html#virtual-machines",
    "title": "Docker basics",
    "section": "Virtual machines",
    "text": "Virtual machines\n\nVM virtualizes the physical hardware. Each VM includes a full OS along with the necessary binaries and libraries, making them heavier and more resources-intensive. Containers, on the other hand, share the host OS kernel and only package the application and its dependencies, resulting in a more lightweight and efficient solution.\nVMs provide strong isolation and are suited for running multiple OS environments, but they have a performance overhead and longer startup times. Containers offer faster startup, better resource utilization, and high portability across different environments, though their isolation is at the process level, which may not be as robust as that of VMs. Overall, VMs could be used for scenarios needing complete OS environments, while containers excel in lightweight, efficient, and consistent application deployment.\nDocker relies on Linux kernels, which means that macOS and Windows cannot run Docker natively without some additional steps. Each operating system has its own solution for running docker."
  },
  {
    "objectID": "programming/docker-intro/index.html#image-and-containers",
    "href": "programming/docker-intro/index.html#image-and-containers",
    "title": "Docker basics",
    "section": "Image and containers",
    "text": "Image and containers\nSince we already know what containers are it’s easier to explain images through them: Containers are instances of images. A basic mistake is to confuse images and containers.\nCooking metaphor:\nThink of a container as a ready-to-eat meal that we can simply heat up and consume. An image, on the other hand, is the recipe and the ingredients for that meal.\nSo just like how we need a recipe and ingredients to make a meal, we need an image and a container runtime (Docker engine) to create a container. The image provides all the necessary instructions and dependencies for the container to run, just like a recipe provides the steps and ingredients to make a meal.\nIn short, an image is like a blueprint or template and the building material, while a container is an instance of that blueprint or template.\n\nImage\nA Docker image is a file. An image never changes; we cannot edit an existing file. Creating a new image happens by starting from a base image and adding new layers to it. We will talk about layers later, but we should think of images as immutable, they can not be changed after they are created.\nList all of ours image with docker image ls\n$ docker image ls\n  REPOSITORY      TAG      IMAGE ID       CREATED         SIZE\n  hello-world     latest   d1165f221234   9 days ago      13.3kB\nContainers are created from images, so when we ran hello-world twice we downloaded one image and created two of them from the single image.\nWell then, if images are used to create containers, where do images come from? This image file is built from an instructional file named Dockerfile that is parsed when we run docker image build.\nDockerfile is a file that is by default called Dockerfile, that looks something like this\nFROM &lt;image&gt;:&lt;tag&gt;\n\nRUN &lt;install some dependencies&gt;\n\nCMD &lt;command that is executed on `docker container run`&gt;\nand is the instruction set for building an image. We will look into Dockerfiles later when we to build our own images.\nIf we go back to the cooking metaphor, as Dockerfile provides the instructions needed to build an image we can think of that as the recipe for images. We’re now 2 recipes deep, as a Dockerfile is the recipe for an image and an image is the recipe for the container. The only difference is that Dockerfile is written by us, whereas image is written by our machine based on the Dockerfile."
  },
  {
    "objectID": "programming/docker-intro/index.html#container",
    "href": "programming/docker-intro/index.html#container",
    "title": "Docker basics",
    "section": "Container",
    "text": "Container\nContainers only contain what is required to execute an application; and we can start, stop and interact with them. They are isolated environments in the host machine with the ability to interact with each other and the host machine itself via defined method (TCP/UDP).\nList all of our containers with docker container ls\n$ docker container ls\n  CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\nWithout -a flag it will only print running containers.\n$ docker container ls -a\n  CONTAINER ID   IMAGE           COMMAND      CREATED          STATUS                      PORTS     NAMES\n  b7a53260b513   hello-world     \"/hello\"     5 minutes ago    Exited (0) 5 minutes ago              brave_bhabha\n  1cd4cb01482d   hello-world     \"/hello\"     8 minutes ago    Exited (0) 8 minutes ago              vibrant_bell"
  },
  {
    "objectID": "carried over.html",
    "href": "carried over.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "to use or do something at a later time than planned:\nThe performance has had to be carried over to/till next week because the repairs to the theatre aren’t finished yet."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Write more markdown here!\nI’am Hung"
  },
  {
    "objectID": "let things slide.html",
    "href": "let things slide.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "to not do anything about something or someone when you should try to change or correct that thing or person.\neg: You were late this morning. I’ll let it slide this time, but don’t let it happen again.\nIt may be best to let things slide this time."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "The Bernoulli distribution is a discrete probability distribution that models the outcome of a single trial with two possible outcomes: success (1) and failure (0).\n\n\n\nA Bernoulli random variable ( X ) takes the value: - ( X = 1 ) with probability ( p ) (success), - ( X = 0 ) with probability ( 1 - p ) (failure).\nMathematically, the probability mass function (PMF) is given by: \\[\nP(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}, \\ 0 \\leq p \\leq 1.\n\\]\n\n\n\n\n\n\nThe mean represents the expected outcome of the random variable: \\[\nE[X] = p.\n\\]\nWe expected value \\(E(x)\\) of a random variable \\(X\\) is given by: \\[\nE(X) = \\sigma x \\dot P(X = x)\n\\] For a Bernoulli random variable: \\[\nE(X) = 1 \\dot p + 0 \\dot( 1 - p) = p\n\\]\n\n\n\nThe variance measures how much the outcomes deviate from the mean: \\[\n\\operatorname{Var}(X) = p(1 - p).\n\\] The variance of a random variable \\(X\\) measures how much the values of \\(X\\) deviate from its mean: \\[\nVar(X) = E[(X-E(X))^2]\n\\] expand this: \\[\nVar(X) = E(X^2 - 2pX + p^2)\n\\] Since \\(p^2\\) is constant and \\(E(X)= p\\), we have: \\[\nVar(X) = E(X^2) - 2pE(X) + p^2\n\\]\nFor a Bernoulli variable, \\(X^2 = X\\) (because \\(1^2 = 1\\) and \\(0^2=0\\)): \\[\nE(X^2) = E(X) = p\n\\] Substituting back, \\[\nVar(X) = p - 2p^2 + p = p - p^2 = p(1)\n\\] ### 📝 Standard Deviation The standard deviation is the square root of the variance: \\[\n\\sigma = \\sqrt{p(1 - p)}.\n\\]\n\n\n\nSkewness measures the asymmetry of the distribution: \\[\n\\gamma_1 = \\frac{1 - 2p}{\\sqrt{p(1 - p)}}.\n\\]\n\n\n\nThe kurtosis of the Bernoulli distribution is: \\[\n\\gamma_2 = \\frac{1 - 6p(1 - p)}{p(1 - p)}.\n\\]\n\n\n\nThe estimator for \\(p\\) based on \\(n\\) independent observations \\(X_1, X_2, \\dots, X_n\\) is the sample mean:\n\\[\n\\hat{p}_n = \\frac{1}{n}\\sigma^{n}_{i=1}X_i.\n\\]\n\n\n\n\n\nDomain: ( x {0, 1} ).\nParameter: Single parameter ( p ), where ( 0 p ).\nSupport: The distribution is defined on two points: 0 and 1.\nMemoryless: The Bernoulli distribution is not memoryless.\nSpecial Case:\n\nIf ( p = 0.5 ), the distribution is symmetric.\nIf ( p ), the distribution is skewed.\n\n\n\n\n\n\n\nBinomial Distribution:\nThe Bernoulli distribution is a special case of the Binomial distribution with ( n = 1 ): \\[\n\\text{Bernoulli}(p) = \\text{Binomial}(n=1, p).\n\\]\nGeometric Distribution:\nA geometric random variable models the number of Bernoulli trials until the first success.\nBeta Distribution (Conjugate Prior):\nIn Bayesian statistics, the Beta distribution is the conjugate prior for the Bernoulli likelihood.\n\n\n\n\n\n\nModeling Binary Outcomes:\n\nCoin flips (Heads/Tails)\nPass/Fail tests\nYes/No survey responses\nOn/Off states in systems\n\nMachine Learning:\n\nLogistic regression for binary classification.\nBernoulli Naive Bayes classifiers.\n\nStatistical Inference:\n\nEstimating proportions (e.g., percentage of people supporting a policy).\n\n\n\n\n\n\nimport numpy as np\n\n# Parameters\np = 0.7  # Probability of success\nn = 1000  # Number of trials\n\n# Simulate Bernoulli trials\ndata = np.random.binomial(n=1, p=p, size=n)\n\n# Estimating p\np_estimate = np.mean(data)\n\nprint(f\"True probability p: {p}\")\nprint(f\"Estimated probability p̂: {p_estimate:.4f}\")"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#definition",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#definition",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "A Bernoulli random variable ( X ) takes the value: - ( X = 1 ) with probability ( p ) (success), - ( X = 0 ) with probability ( 1 - p ) (failure).\nMathematically, the probability mass function (PMF) is given by: \\[\nP(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}, \\ 0 \\leq p \\leq 1.\n\\]"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#properties-of-the-bernoulli-distribution",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#properties-of-the-bernoulli-distribution",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "The mean represents the expected outcome of the random variable: \\[\nE[X] = p.\n\\]\nWe expected value \\(E(x)\\) of a random variable \\(X\\) is given by: \\[\nE(X) = \\sigma x \\dot P(X = x)\n\\] For a Bernoulli random variable: \\[\nE(X) = 1 \\dot p + 0 \\dot( 1 - p) = p\n\\]\n\n\n\nThe variance measures how much the outcomes deviate from the mean: \\[\n\\operatorname{Var}(X) = p(1 - p).\n\\] The variance of a random variable \\(X\\) measures how much the values of \\(X\\) deviate from its mean: \\[\nVar(X) = E[(X-E(X))^2]\n\\] expand this: \\[\nVar(X) = E(X^2 - 2pX + p^2)\n\\] Since \\(p^2\\) is constant and \\(E(X)= p\\), we have: \\[\nVar(X) = E(X^2) - 2pE(X) + p^2\n\\]\nFor a Bernoulli variable, \\(X^2 = X\\) (because \\(1^2 = 1\\) and \\(0^2=0\\)): \\[\nE(X^2) = E(X) = p\n\\] Substituting back, \\[\nVar(X) = p - 2p^2 + p = p - p^2 = p(1)\n\\] ### 📝 Standard Deviation The standard deviation is the square root of the variance: \\[\n\\sigma = \\sqrt{p(1 - p)}.\n\\]\n\n\n\nSkewness measures the asymmetry of the distribution: \\[\n\\gamma_1 = \\frac{1 - 2p}{\\sqrt{p(1 - p)}}.\n\\]\n\n\n\nThe kurtosis of the Bernoulli distribution is: \\[\n\\gamma_2 = \\frac{1 - 6p(1 - p)}{p(1 - p)}.\n\\]\n\n\n\nThe estimator for \\(p\\) based on \\(n\\) independent observations \\(X_1, X_2, \\dots, X_n\\) is the sample mean:\n\\[\n\\hat{p}_n = \\frac{1}{n}\\sigma^{n}_{i=1}X_i.\n\\]"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#key-characteristics",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#key-characteristics",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Domain: ( x {0, 1} ).\nParameter: Single parameter ( p ), where ( 0 p ).\nSupport: The distribution is defined on two points: 0 and 1.\nMemoryless: The Bernoulli distribution is not memoryless.\nSpecial Case:\n\nIf ( p = 0.5 ), the distribution is symmetric.\nIf ( p ), the distribution is skewed."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#relationship-to-other-distributions",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#relationship-to-other-distributions",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Binomial Distribution:\nThe Bernoulli distribution is a special case of the Binomial distribution with ( n = 1 ): \\[\n\\text{Bernoulli}(p) = \\text{Binomial}(n=1, p).\n\\]\nGeometric Distribution:\nA geometric random variable models the number of Bernoulli trials until the first success.\nBeta Distribution (Conjugate Prior):\nIn Bayesian statistics, the Beta distribution is the conjugate prior for the Bernoulli likelihood."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#applications-of-the-bernoulli-distribution",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#applications-of-the-bernoulli-distribution",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Modeling Binary Outcomes:\n\nCoin flips (Heads/Tails)\nPass/Fail tests\nYes/No survey responses\nOn/Off states in systems\n\nMachine Learning:\n\nLogistic regression for binary classification.\nBernoulli Naive Bayes classifiers.\n\nStatistical Inference:\n\nEstimating proportions (e.g., percentage of people supporting a policy)."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#python-example-simulating-a-bernoulli-random-variable",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#python-example-simulating-a-bernoulli-random-variable",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "import numpy as np\n\n# Parameters\np = 0.7  # Probability of success\nn = 1000  # Number of trials\n\n# Simulate Bernoulli trials\ndata = np.random.binomial(n=1, p=p, size=n)\n\n# Estimating p\np_estimate = np.mean(data)\n\nprint(f\"True probability p: {p}\")\nprint(f\"Estimated probability p̂: {p_estimate:.4f}\")"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "ANTI-CAPITALIST SOFTWARE LICENSE (v 1.4)\nCopyright © 2024 Erick Ratamero\nThis is anti-capitalist software, released for free use by individuals and organizations that do not operate by capitalist principles.\nPermission is hereby granted, free of charge, to any person or organization (the “User”) obtaining a copy of this software and associated documentation files (the “Software”), to use, copy, modify, merge, distribute, and/or sell copies of the Software, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or modified versions of the Software.\nThe User is one of the following:\n\n\nAn individual person, laboring for themselves\nA non-profit organization\nAn educational institution\nAn organization that seeks shared profit for all of its members, and allows non-members to set the cost of their labor\n\n\nIf the User is an organization with owners, then all owners are workers and all workers are owners with equal equity and/or equal vote.\nIf the User is an organization, then the User is not law enforcement or military, or working for or under either.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "english/household-work/index.html",
    "href": "english/household-work/index.html",
    "title": "Ielts task 1 - Household work",
    "section": "",
    "text": "The bar chart below shows the average duration of household women did (unemployed, part-time employed and full-time) when compared to men who had full-time work in the UK between 1985 and 2005.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\nThe bar chart details hours of housework completed in the UK from 1985 to 2005 by fully employed males and females with various states of employment. Looking from an overall perspective, it is readily apparent that time spent on housework rose for all groups with the exception of unemployed women, which was stable overall. However, the latter category undertook by far the most housework, while men with full-time jobs did comparatively little.\nUnemployed women led throughout the period in terms of hours, at a peak of 7 hours in 1985 and 2005 and a slight dip to 6.5 hours in between. In contrast, men with full-time work were unchanged at just 1 hour in 1985 and 1995, though there was a slight rise to 1.5 hours in 2005.\nWomen with part-time employment were slightly lower than their unemployed counterparts throughout the period, beginning at 5.3 hours in 1985, before declining to 5 hours over the next 5 years, and then finishing the period with a surge to 6 hours. The figures for women with full-time employment resembled those of men, though they were marginally higher at 2, 1.5, and 2.8 hours in 1985, 1995, and 2005, respectively."
  },
  {
    "objectID": "english/household-work/index.html#part-3",
    "href": "english/household-work/index.html#part-3",
    "title": "Ielts task 1 - Household work",
    "section": "Part 3",
    "text": "Part 3\n\nDo you think tidiness is important in the workplace? Absolutely. A tidy workplace not only creates a more professional image but also boosts efficiency. When things are organized, it’s easier to find what you need, which saves time and reduce stress. Moreover, a clean environment can promote better focus and even improve team morale.\nWhy do some people struggle to stay tidy? There could be a few reasons. Some people simply lead very busy lives and don’t prioritize tidiness, while others may not have developed habits of organization from a young age. In some cases, people might not even notice the mess around them - they’ve grown accustomed to it. For a few, it could also be tied to mental health challenges, like anxiety or ADHD, which can effect one’s ability to maintain order.\nHow can parents teach children to be tidy? The most effective way is by setting a good example. If children see their parents consistently keeping things organized, they’re more likely to adopt those habits themselves. It also helps to make tidying part of a daily routine and turn it into something fun or rewarding - for instance, though games or praise. Teaching them to take pride in their space can go a long way.\nDo you think technology helps people stay more organized and tidy? Definitely. There are so many apps and digital tools now that can help people manages their time, declutter digitally, and keep track of tasks or schedules. Even simple things like calendar reminder or to-do list apps can encourage tidier habits. That said, technology can also contribute to digital clutter if not used mindfully.\nIs there a difference in tidiness between younger and older generations? In some ways, yes. Older generations often place a high value on physical tidiness, perhaps because they grew up in a time when everything had to be done manually. Younger people, on the other hand, might prioritize digital organization - like keeping their phones or laptops neatly arranged. However, i think personal habits play a bigger role than age."
  },
  {
    "objectID": "english/english-health-services/index.html",
    "href": "english/english-health-services/index.html",
    "title": "Health Services",
    "section": "",
    "text": "Health services are a basic necessity. However, private companies have made them quite costly for ordinary individuals Do the advantages of private health care outweigh its disadvantage"
  },
  {
    "objectID": "english/english-health-services/index.html#analysis",
    "href": "english/english-health-services/index.html#analysis",
    "title": "Health Services",
    "section": "Analysis",
    "text": "Analysis\nAdvantages of Private Health Care: - Efficiency and shorter Waiting Times: Private hospitals offer quicker services compare to public hospitals, where long waiting lists are common. - Better facilities and advanced technology: Many private health institution provide cutting-edge medical technology, specialized treatments, and higher standards of care. - Greater Patient Choice: Individuals can select their preferred doctors, hospitals, and treatment plans, leading to more personalized care. Disadvantages of Private Health Care: - High costs and Limited Accessibility: Private health care is expensive, making it inaccessible for low-income individuals who cannot afford quality treatment. - Profit-driven System: The focus on profit can sometimes lead to unnecessary medical procedures, overcharging, or prioritizing financial gain over patient well-being. - Health inequality: A reliance on private health care can widen the gap between the rich and the poor, as only those who can afford it receive top-tier medical services. ## Sample essay Health care is an essential service, yet private medical facilities often make it unaffordable for many individuals. While private health care offers significant advantages such as efficiency, superior technology, and greater patient choice, it also has considerable drawbacks, particularly its high costs and contribution to social inequality. In my view, although private health care enhances medical standards, its disadvantages outweigh its benefits due to its exclusive and financial burden on ordinary citizens.\nOne of the key benefits of private health care is it efficiency. Unlike public hospitals, which often suffer from overcrowding and long waiting times, private institutions provide quicker access to medical attention. This is particularly crucial for emergency treatments or specialized procedures that require immediate intervention. Additionally, private hospitals invest heavily in advanced medical technology and modern infrastructure, ensuring high-quality treatment and specialized care. For instance, many private institutions offer robotic surgeries and personalized cancer treatments, which are not always available in public hospitals. Furthermore, private health care allows patients to choose their doctors and treatment plans, leading to a more personalized and comfortable experience.\nHowever, the drawbacks of private health care are significant. The most pressing concern is its exorbitant cost, making it unaffordable for a large portion of the population. Many life-saving procedures and medications are out of reach for lower-income individuals, forcing them to rely on overstretched public health systems. Furthermore, because private hospitals overate on a profit-driven model, there is a risk of unnecessary procedures being performed to maximize revenue, sometimes at the expensive of patient welfare. Lastly, the presence of an expensive private health care sector creates a divide between the wealthy and the poor, exacerbating health inequalities. In countries without strong public health care, the underprivileged often suffer from preventable diseases simply due to financial constraints.\nIn conclusion, while private health care provides high-quality services and efficiency, its high costs and potential for exploitation make it a less viable option for the general population. A well-functioning public health system remains crucial to ensuring that medical care is accessible to all, regardless of financial status. Governments should regulate private health care costs and prioritize investment in public health services to balance quality with accessibility."
  },
  {
    "objectID": "english/english-health-services/index.html#structure",
    "href": "english/english-health-services/index.html#structure",
    "title": "Health Services",
    "section": "Structure",
    "text": "Structure\n1. Health care is an essential service, yet private medical facilities often make it unaffordable for many individuals. 2. While private health care offers significant advantages such as efficiency, superior technology, and greater patient choice, it also has considerable drawbacks, particularly its high costs and contribution to social inequality. 3. In my view, although private health care enhances medical standards, its disadvantages outweigh its benefits due to its exclusive and financial burden on ordinary citizens. 1.Paraphrase the question. 2.Mention both advantages and disadvantages. 3.Clearly state your opinion.* 1. One of the key benefits of private health care is its efficiency, as it ensures faster access to treatment, advanced medical technology, and personalized care. 2. Unlike public hospitals, which often suffer from overcrowding and long waiting times, private institutions provide quicker access to medical attention. 3. This is particularly crucial for emergency treatments or specialized procedures that require immediate intervention. Additionally, private hospitals invest heavily in advanced medical technology and modern infrastructure, ensuring high-quality treatment and specialized care. 4. For instance, many private institutions offer robotic surgeries and personalized cancer treatments, which are not always available in public hospitals. 5. Furthermore, private health care allows patients to choose their doctors and treatment plans, leading to a more personalized and comfortable experience. 1.Write a topic sentence with a clear main idea at the end. 2.Explain our main idea 3.Develop it fully 4.Develop it with specific or hypothetical examples 5.Better to have more detail\n1. However, the drawbacks of private health care are significant. 2. The most pressing concern is its exorbitant cost, making it unaffordable for a large portion of the population. 3. Many life-saving procedures and medications are out of reach for lower-income individuals, forcing them to rely on overstretched public health systems. 4. Furthermore, because private hospitals overate on a profit-driven model, there is a risk of unnecessary procedures being performed to maximize revenue, sometimes at the expensive of patient welfare. Lastly, the presence of an expensive private health care sector creates a divide between the wealthy and the poor, exacerbating health inequalities. 5. In countries without strong public health care, the underprivileged often suffer from preventable diseases simply due to financial constraints.\n1.Write a new topic sentence with a new main idea at the end 2.Explain our new main idea. 3.Include specific details and examples. 4.Add as much information as we can and make sure it links logically. 5.This country is a bit long - aim for about 275 words.\n1. In conclusion, while private health care provides high-quality services and efficiency, its high costs and potential for exploitation make it a less viable option for the general population. 2. A well-functioning public health system remains crucial to ensuring that medical care is accessible to all, regardless of financial status. Governments should regulate private health care costs and prioritize investment in public health services to balance quality with accessibility.\n1.Summaries our main idea. 2.Include a final thought."
  },
  {
    "objectID": "english/having-children-at-older-age/index.html",
    "href": "english/having-children-at-older-age/index.html",
    "title": "IELTS Essay Having Children at an Older Age",
    "section": "",
    "text": "Question:\nSome people choose to have their first child at an older age. What are the reasons? Do the advantages outweigh the disadvantages?\nDavid Lang’s sample In recent years, an increasing number of individuals have opted to start their families later in life. This trend raises the question of why more people are choosing to have their first child at an older age and whether the benefits of doing so justify the drawbacks.\nOne of the primary reasons for delaying parenthood is the pursuit of career stability in a worrying economic climate. In today’s competitive job market, many individuals, particularly women, prefer to establish themselves professionally before starting a family. By focusing on their careers in their twenties and early thirties, they can achieve financial security and attain higher positions, which often translates to job satisfaction and greater resources for their future children. This growing trend is reflected in how mainstream society now places significant emphasis on personal development and achieving personal goals before embarking on parenthood. For that reason, many young adults prioritize their own personal and financial welfare, a situation that engenders an atmosphere less conductive to having children early in life.\nThere are legitimate disadvantages connected with having children later in life. One of the primary concerns is the potential health risks for both the mother and the child. Advanced maternal age is often associated with a higher risk of complications such as gestational diabetes, hypertension, and chromosomal abnormalities. However, despite these drawbacks, the advantages of having children later in life due to greater maturity overshadow the hypothetical drawbacks. The emotional maturity and life experience that older parents bring to child-rearing can result in a more stable and nurturing environment. Older parents are likely to be more financially secure and better prepared to provide for their children’s education and well-being. The ability to make well-considered decisions and the patience developed over time can contribute to a more thoughtful and deliberate approach to parenting.\nIn conclusion, while there are valid concerns regarding the health risks associated with having children later in life, the benefits, such as financial stability, emotional maturity, and better preparation for parenthood, outweigh these disadvantages. As societal norms and medical advancements continue to evolve, the rend of delaying parenthood is likely to remain prevalent, reflecting the changing priorities and values of modern individuals.\nSample, disadvantages over advantages.\nAn increasing number of individuals are choosing to have their first child at an older age. This trend is driven by factors such as career prioritization, financial security, and a desire for greater personal maturity. While some argue that the benefits of delayed parenthood - like improved financial stability and enhanced life experience - outweigh the disadvantages, I strongly disagree. The potential health risks, fertility issues, and other practical challenges mean that the disadvantages ultimately outweigh any advantages.\nModern society has shifted priorities, and many couples now postpone parenthood for several reasons. Primarily, individuals strive to establish their careers and achieve financial stability before starting a family. This approach allows prospective parents to create a secure environment for their child, free from economic uncertainty. Furthermore, many believe that greater life experience and emotional maturity can lead to more thoughtful parenting decisions. As a result, the decision to delay having a first child is often seen as a strategic move to optimize one’s personal and professional circumstances.\nDespite these reasons, the drawbacks of having a first child at an older age are significant. Health concerns are paramount; older mothers face increased risks of pregnancy complications, such as gestational diabetes and hypertension, which can jeopardize both maternal and infant health. Additionally, advancing age is closely linked to decreased fertility, often necessitating expensive and emotionally draining fertility treatments. Beyond health, older parents might experience reduced physical stamina, which is crucial for the demanding nature of childcare, and a larger generational gap that can affect the parent-child relationship. These challenges make the prospect of delayed parenthood less attractive, as the risks and long-term implications overshadow the benefits of waiting.\nIn conclusion, although delaying parenthood may provide some advantages in terms of financial and emotional preparedness, the significant health risks, fertility challenges, and practical issues associated with having a first child later in life outweigh these benefit. Therefore, i contend that the disadvantages of postponing parenthood ultimately make it a less favorable option compared to earlier family planning.\nSample, advantages over disadvantages.\nIn recent years, an increasing number of individuals have chosen to have their first child at an older age. This trend is largely influenced by factors such as the pursuit of career goals, the desire for financial stability, and the accumulation of life experience before entering parenthood. I firmly believe that these advantages not only lead to more secure and thoughtful family planning but also contribute to improved parenting outcomes, ultimately outweighing the potential disadvantages.\nModern societal shifts have prompted many prospective parents to postpone starting a family. Primarily, individuals are keen to establish a stable career and secure a robust financial foundation before welcoming a child. This approach allows them to create a nurturing environment where the child’s needs are met without the stress of economic uncertainty. Furthermore, waiting until later in life often results in enhanced emotional maturity and life experience, enabling parents to make more considered decisions and effectively manage the challenges of raising a child. These reasons underscore why many see value in delaying parenthood as a strategic, long-term investment in both personal and familial well-being.\nThe advantages of choosing to have a child at an older age are manifold. With a well-established career and financial security, older parents are better equipped to provide for their child’s educational and extracurricular needs, contributing to the child’s overall development. In addition, greater maturity and life experience often translate into more patient, empathetic, and balanced parenting. Such parents typically possess enhanced problem-solving skills and resilience, which can be invaluable when navigating the inevitable challenges of childcare. Moreover, studies have shown that older parents tend to invest more time and resources in their children’s upbringing, resulting in a more stable and nurturing home environment. Collectively, these advantages not only promote healthier family dynamics but also pave the way for the child’s long-term success and well-being.\nIn conclusion, while there are inherent challenges associated with delayed parenthood, the benefits- ranging from financial stability and career establishment to increased emotional maturity and effective parenting- clearly outweigh the drawbacks. By choosing to have a child at an older age, many individuals are better prepared to offer a secure, supportive, and enriching environment for their children, ultimately leading to more positive outcomes for families."
  },
  {
    "objectID": "english/english-topic-sentence/index.html",
    "href": "english/english-topic-sentence/index.html",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "",
    "text": "The standard essay structure for a IELTS Task 2 essay is four paragraphs - an introduction, two body paragraphs and a conclusion.\nThe topic sentence is the first sentence of each body paragraph. It tells the examiner the main idea of the paragraph so it functions like a mini introduction."
  },
  {
    "objectID": "english/english-topic-sentence/index.html#analyze-sample-essays",
    "href": "english/english-topic-sentence/index.html#analyze-sample-essays",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "Analyze sample essays",
    "text": "Analyze sample essays\n\nLook at some task 2 sample essays.\nHighlight the topic sentence(s) - what is the topic? What is the main idea?\nRead the body paragraph(s) - do all the supporting ideas relate to that idea?"
  },
  {
    "objectID": "english/english-topic-sentence/index.html#practice-writing-topic-sentences-for-sample-essays",
    "href": "english/english-topic-sentence/index.html#practice-writing-topic-sentences-for-sample-essays",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "Practice writing topic sentences for sample essays",
    "text": "Practice writing topic sentences for sample essays\n\nLook at some task 2 sample essay.\nCover up the first sentence of the body paragraph(s).\nRead the body again - what is the topic and main idea ?\nWrite a topic sentence for that paragraph, then compare it with the original one. ## Analyze our own writing\nLook back through our old task 2 essays.\nAre the topic sentences good? If not, write new ones. ## Practice writing topic sentences for new question\nLook at some sample task 2 question (there is one below)\nAnalyze the question and brainstorm main ideas.\nChoose the main topic and main idea for each body paragraph.\nWrite our topic sentences."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html",
    "href": "english/english-writting-zoos/zoos.html",
    "title": "Zoos",
    "section": "",
    "text": "Zoos have long been a subject of debate. While some people argue that zoos play a crucial role in conserving endangered species and educating the public about wildlife, others contend that confining animals to artificial environments is unethical. Discuss both views and give your own opinion.\n\nAnalysis\nAdvantage: pros: Conservation zoos often participate in breeding programs and reintroduction efforts for endangered species. Education: They provide a unique opportunity for the public to learn about wildlife and environmental issues. Research: Zoos support scientific research and rehabilitation programs that can benefit species survival\nDisadvantage Cons: Ethical concerns: Confining animals can lead to physical and psychological stress, raising serious animal welfare issues. Natural Behavior: The artificial habitats in zoos may prevent animals from expressing their natural behaviors, potentially leading to a diminished quality of life.\n\n\nSample\nIn recent years, zoos have become a focal point of heated debate. On the one hand, proponents argue that zoos are indispensable for conserving endangered species and educating the public. On the other hand, critics maintain that confining animals to unnatural settings is inherently unethical. In my view, while the welfare of captive animals must be a priority, the benefits of well-managed zoos in conservation and education are substantial.\nSupporters of zoos highlight their critical role in preserving biodiversity. With numerous species facing extinction, zoos provide a controlled environment where endangered animals can be bred and nurtured away from the threats of habitat loss and poaching. Many institutions have pioneered successful breeding programs, subsequently reintroducing species into the wild. Moreover, zoos serve as a vital educational platform, offering visitors first-hand exposure to wildlife. This experience not only fosters environmental awareness but also inspires future generations to engage in conservation efforts. Additionally, ongoing research in these settings has led to significant advancements in veterinary medicine and species rehabilitation.\nConversely, opponents argue that the confinement of animals infringes on their natural rights. Critics assert that even the most well-intentioned zoos cannot replicate the complex ecosystems that wild animals depend on, often leading to behavioral and psychological issues. The ethical debate is further intensified by instances of inadequate care and substandard living conditions in some facilities. While these concerns are valid, they underscore the need for stringent regulations rather than the outright dismissal of zoos as an institution.\nIn conclusion, although the ethical implications of captivity are significant, I believe that the conservation, education, and research benefits provided by zoos justify their existence - provided that animal welfare is rigorously safeguarded. This balanced approach can ensure that zoos continue to contribute positively to both wildlife preservation and public understanding of environmental issues."
  },
  {
    "objectID": "english/english-farm-animals-vegetables/index.html",
    "href": "english/english-farm-animals-vegetables/index.html",
    "title": "Farm animals and Vegetables",
    "section": "",
    "text": "It has been suggested that primary children should learn how to grow vegetables and keep animals.\nDo the advantages of this outweigh the disadvantages?\nDavid Lang’s essay:\nSome today have argued that it would be a sensible policy to teach children how to farm and raise animals at school. In my opinion, though this would require enormous resources, it would be beneficial in terms of both physical and intellectual development.\nThose arguing against this proposal point out the resources needed. In order to implement such an ambitious programs, schools would firstly need land nearby where animals and crops be raised. This might be possible within a school’s pre-existing grounds, but there would still need to be significant investment in infrastructure. Schools would therefore have to reallocate a significant portion of their budget, potentially reducing the money for crucial facilities, staff, and teaching resources.\nHowever, the advantages for the holistic development of children is overwhelming. Students today spend more time than ever before passively using technological devices. By spending more time outdoors learning about animals and crops, students will better understand their position within the natural food chain. Overtime, this may lead them to adopt environmentally friendlier, compassionate lifestyle choices such as becoming vegetarians or shopping locally rather than buying from large chains. Moreover, the work outdoors would necessarily involve physical exercise. Since obesity is a growing concern in most countries, this time outside could help to improve standards of physical fitness and establish healthy lifelong habits.\nIn conclusion, despite the money this reform would require, schools should adopt it when possible so as to achieve a more complete education for students. Governments should consider the long-term effects when deciding on such policies.\n\nChat gpt\nThe proposal to incorporate agricultural and animal husbandry education into primary school curricula has garnered attention. In my view, while such initiatives offer certain benefits, the potential drawbacks, particularly concerning resource allocation and health risks, are more significant.\nAdvocates highlight that engaging children in growing vegetables and caring for animals can foster responsibility and a connection to nature. These activities may enhance students’ understanding of food sources and promote environmental stewardship. However, implementing such programs requires substantial resources. Schools would need to invest in appropriate land, infrastructure, and trained personnel to oversee these activities. This financial commitment could divert funds from essential academic subjects, potentially compromising the quality of core education.\nMoreover, there are health and safety concerns associated with young children handling soil and animals. Exposure to allergens, zoonotic diseases, and physical injuries are risks that cannot be overlooked. Ensuring proper hygiene and safety measures would necessitate additional oversight and training, further straining school resources.\nIn conclusion, while teaching primary children to cultivate plants and care for animals has educational merits, the associated disadvantages, particularly in terms of resource allocation and health risks, outweigh the benefits. Schools should prioritize core academic subjects and consider integrating environmental education through less resource-intensive methods.\n💡 Why David Lang’s Essay Feels Simple but Scores High\n\nClear structure\n\nIntroduction: Directly paraphrases the question + gives a clear opinion.\nBody 1: Discusses the opposite view.\nBody 2: Presents his own view (which he supports).\nConclusion: Restates the opinion and wraps up.\n👉 Why it works: Examiners love clarity. They’re not grading you on how “fancy” your structure is — just that it’s logical and easy to follow.\n\nPrecise vocabulary, not fancy vocabulary\n\nHe uses natural, topic-specific language, like:\n“holistic development”\n“environmentally friendlier”\n“passively using technological devices”\n👉 Why it works: Band 9 vocabulary is about being accurate and appropriate, not sounding like a thesaurus.\n\nBalanced ideas + developed examples\n\nHe acknowledges the disadvantages (cost, resources) and then shows why advantages outweigh them (fitness, habits, awareness). He explains his points — not just states them.\n👉 Why it works: IELTS cares about depth more than how many ideas you list. One strong idea with support = better than five weak ones.\n\nGrammar is natural and controlled\n\nHe uses complex sentences when needed, not to show off.\n👉 Example:\n“Since obesity is a growing concern in most countries, this time outside could help…”\nThat’s complex, but clear. No over-complicated structures.\n✅ How YOU Can Write Like David Lang\n\nFollow a clean structure\n\nIntro: Paraphrase + opinion\nBody 1: Opposing view\nBody 2: Your view (with examples)\nConclusion: Restate opinion\n\nUse precise, topic-relevant words\n\nLearn how to talk naturally about big topics (education, health, environment) with words that fit.\n\nDon’t list ideas — explain them\n\nPick 1–2 strong points per paragraph and expand with why, how, and an example.\n\nWrite like you’re explaining, not performing\n\nPretend you’re talking to a smart friend, not an examiner. Make it natural and reasoned, not robotic or flowery.\n✏️ Step 1: Write the Introduction\n✅ What we need:\nParaphrase the question\nGive a clear opinion\n🧠 Example:\nSome believe that it would be beneficial for young children to be taught farming skills at school. However, I believe that the drawbacks of such a policy outweigh the potential benefits.\n🔍 Why this works:\nIt’s short.\nClear opinion.\nNo fancy words. Just focused.\nWould you like to try writing your own version of the introduction before we move on?\nOr should we go ahead to Body Paragraph 1?"
  },
  {
    "objectID": "posts/post_template/index.html",
    "href": "posts/post_template/index.html",
    "title": "Post template",
    "section": "",
    "text": "Just write more markdown here!"
  },
  {
    "objectID": "posts/post_template/index.html#this-is-a-subsection",
    "href": "posts/post_template/index.html#this-is-a-subsection",
    "title": "Post template",
    "section": "This is a subsection",
    "text": "This is a subsection\nSubsection text! You get it now I assume\nThis is how you reference an image in your blog post\n\n\n\nThis is just a photo I took last time I was in Dundee.\n\n\nThat’s it! go for it!"
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html",
    "href": "posts/extreme-early-retirement/index.html",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Tim, a man from Regina, Saskatchewan, Canada, retired from engineering at the age of 39, after 11 years of planning and working towards his goal of early retirement, and his wife plans to retire from her childcare career in a few years (00:00:07).\nTim’s family achieved early retirement through a financial strategy known as FIRE, which involves drastically reducing spending, saving a large percentage of income, and investing the savings (00:00:20).\nTim first considered the idea of early retirement in 2006 and started saving money, cutting costs, and figuring out what matters to him and his family (00:00:53).\nWhen Tim and his wife started working towards early retirement, they had student loan debt and other financial issues, but they paid off their existing debt and then focused on reducing their spending (00:01:12).\n\n\n\n\n\nTo reduce their spending, Tim and his wife looked at their bills, such as their power bill, and found ways to lower them, such as changing to LED lightbulbs (00:01:48).\nThey also reduced their grocery bill by buying generic versions of products and making things from scratch, which often tasted better and saved them money (00:02:02).\nTim and his wife have a focused approach to spending, only eating out a couple of times a year, and making sure their spending is aligned with what matters to them (00:02:31).\nTim uses a phrase to help him avoid overspending, which is to ask himself if he really needs something and if he can solve a problem without spending money (00:02:53).\nTim and his wife have paid off their mortgage and car loans in advance, which has helped them in their retirement (00:03:23).\n\n\n\n\n\nThe individual has achieved extreme early retirement at the age of 39, with a significant amount of assets and no debt, and currently resides in Regina, Saskatchewan, Canada, where housing costs are relatively low, with an average housing cost of around $278,000 (00:03:50).\nThe majority of their investments are in straightforward strategies, including exchange-traded funds that mirror major indexes, allowing for a simple and low-maintenance approach to managing their finances (00:04:01).\nDespite the simplicity of their investment strategy, they still experience worries about the stock market, but have trained themselves not to check it frequently, focusing instead on long-term performance rather than short-term fluctuations (00:04:11).\nInitially, they found the wild gyrations of the stock market to be psychologically challenging, and having a larger cushion would have been helpful for peace of mind (00:04:27).\n\n\n\n\n\nOne of the downsides of early retirement is the unusual conversations with people who question their decision to retire at a young age, leading them to reframe their explanation to focus on pursuing a writing career (00:04:44).\nThey started a blog in 2006 to document their early retirement journey, which includes their doubts, learning experiences, and mistakes, and also wrote a book called “Three at Forty Five” to discuss the importance of happiness in early retirement (00:04:57).\nThe first year of early retirement was problematic, involving a major lifestyle change and a process of deprogramming themselves to realize they didn’t have to work as much, and instead focus on enjoyable activities (00:05:29).\nTo cope with the transition, they created to-do lists that included fun items, and have pursued various hobbies such as making their own wine and beer, playing Dungeons & Dragons, and creating terrain pieces (00:05:52).\n\n\n\n\n\nThey have been married for 19 years and have two sons, aged 14 and 11, and have taught them about money management, focusing on delayed gratification, saving, and responsible spending (00:06:23).\nThey gave their children an allowance from a young age and allowed them to spend it on whatever they wanted, teaching them valuable lessons about money management (00:06:47).\nA strategy was implemented to teach children the value of saving money by explaining that waiting to buy something now can lead to affording bigger things later, resulting in two kids who default to saving money without knowing what they’re saving for (00:07:17).\nA friend’s advice was taken to heart, which stated that kids are a constant drain on finances, but the expenses change as they age, and it’s essential to get used to it (00:07:43).\nA system was set up to take out cash every month for the kids and put it aside, which averaged out to the same amount of money over the course of the year, with the wife managing the cash flow to prepare for upcoming expenses like winter boots (00:07:49).\nRegistered education savings plans were saved for the kids’ college and university education, which was fully filled before leaving work, allowing the money to grow in interest (00:08:18).\n\n\n\n\n\nThe wife decided to continue working, which was factored into the plan, assuming she would work for a few more years to offset some of the spending and provide an extra cushion (00:08:50).\nA plan was written out in advance to prepare for potential stock market declines, including options like taking a loan or getting a part-time job, to help stay calm and rational during emotional times (00:09:28).\nHaving a written plan helped to prepare for a major stock market decline a year into retirement, which ultimately worked out well in the long run (00:09:08).\nIt’s recommended to write out an investment plan while calm and rational to prepare for potential future declines and have a clear plan of action (00:09:37).\n\n\n\n\n\nThe family’s current home is a daycare, which is quite spacious, and they plan to downsize in the long term, with any savings from lower taxes, water bills, and other expenses being used to offset potential losses (00:10:14).\nLiving in Canada provides benefits such as government-paid healthcare costs, which have been over-contributed to, covering basic healthcare costs, but not dental, prescription eyewear, or other expenses that are self-insured and paid out-of-pocket (00:10:40).\nGovernment benefits like the Canada Pension Plan and Old Age Security are factored into long-term plans, but with conservative assumptions, such as only one spouse collecting Old Age Security (00:11:05).\n\n\n\n\n\nA job at the local library was taken to work with great people, enjoy the work, and earn a supplemental income, which is used to fund extra activities, and has helped achieve a better work-life balance (00:11:30).\nReduced working hours have improved overall happiness, and retirement has provided extra time to focus on desired activities and spend quality time with family, particularly children (00:12:10).\n\n\n\n\n\nResources on early retirement, including Tim’s blog and book, are available in the video description for those interested (00:12:43)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#early-retirement-journey-and-financial-strategies",
    "href": "posts/extreme-early-retirement/index.html#early-retirement-journey-and-financial-strategies",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Tim, a man from Regina, Saskatchewan, Canada, retired from engineering at the age of 39, after 11 years of planning and working towards his goal of early retirement, and his wife plans to retire from her childcare career in a few years (00:00:07).\nTim’s family achieved early retirement through a financial strategy known as FIRE, which involves drastically reducing spending, saving a large percentage of income, and investing the savings (00:00:20).\nTim first considered the idea of early retirement in 2006 and started saving money, cutting costs, and figuring out what matters to him and his family (00:00:53).\nWhen Tim and his wife started working towards early retirement, they had student loan debt and other financial issues, but they paid off their existing debt and then focused on reducing their spending (00:01:12)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#spending-reduction-and-financial-management",
    "href": "posts/extreme-early-retirement/index.html#spending-reduction-and-financial-management",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "To reduce their spending, Tim and his wife looked at their bills, such as their power bill, and found ways to lower them, such as changing to LED lightbulbs (00:01:48).\nThey also reduced their grocery bill by buying generic versions of products and making things from scratch, which often tasted better and saved them money (00:02:02).\nTim and his wife have a focused approach to spending, only eating out a couple of times a year, and making sure their spending is aligned with what matters to them (00:02:31).\nTim uses a phrase to help him avoid overspending, which is to ask himself if he really needs something and if he can solve a problem without spending money (00:02:53).\nTim and his wife have paid off their mortgage and car loans in advance, which has helped them in their retirement (00:03:23)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#investment-strategy-and-market-volatility",
    "href": "posts/extreme-early-retirement/index.html#investment-strategy-and-market-volatility",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The individual has achieved extreme early retirement at the age of 39, with a significant amount of assets and no debt, and currently resides in Regina, Saskatchewan, Canada, where housing costs are relatively low, with an average housing cost of around $278,000 (00:03:50).\nThe majority of their investments are in straightforward strategies, including exchange-traded funds that mirror major indexes, allowing for a simple and low-maintenance approach to managing their finances (00:04:01).\nDespite the simplicity of their investment strategy, they still experience worries about the stock market, but have trained themselves not to check it frequently, focusing instead on long-term performance rather than short-term fluctuations (00:04:11).\nInitially, they found the wild gyrations of the stock market to be psychologically challenging, and having a larger cushion would have been helpful for peace of mind (00:04:27)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#challenges-and-adaptation-in-early-retirement",
    "href": "posts/extreme-early-retirement/index.html#challenges-and-adaptation-in-early-retirement",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "One of the downsides of early retirement is the unusual conversations with people who question their decision to retire at a young age, leading them to reframe their explanation to focus on pursuing a writing career (00:04:44).\nThey started a blog in 2006 to document their early retirement journey, which includes their doubts, learning experiences, and mistakes, and also wrote a book called “Three at Forty Five” to discuss the importance of happiness in early retirement (00:04:57).\nThe first year of early retirement was problematic, involving a major lifestyle change and a process of deprogramming themselves to realize they didn’t have to work as much, and instead focus on enjoyable activities (00:05:29).\nTo cope with the transition, they created to-do lists that included fun items, and have pursued various hobbies such as making their own wine and beer, playing Dungeons & Dragons, and creating terrain pieces (00:05:52)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#family-life-and-financial-education-for-children",
    "href": "posts/extreme-early-retirement/index.html#family-life-and-financial-education-for-children",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "They have been married for 19 years and have two sons, aged 14 and 11, and have taught them about money management, focusing on delayed gratification, saving, and responsible spending (00:06:23).\nThey gave their children an allowance from a young age and allowed them to spend it on whatever they wanted, teaching them valuable lessons about money management (00:06:47).\nA strategy was implemented to teach children the value of saving money by explaining that waiting to buy something now can lead to affording bigger things later, resulting in two kids who default to saving money without knowing what they’re saving for (00:07:17).\nA friend’s advice was taken to heart, which stated that kids are a constant drain on finances, but the expenses change as they age, and it’s essential to get used to it (00:07:43).\nA system was set up to take out cash every month for the kids and put it aside, which averaged out to the same amount of money over the course of the year, with the wife managing the cash flow to prepare for upcoming expenses like winter boots (00:07:49).\nRegistered education savings plans were saved for the kids’ college and university education, which was fully filled before leaving work, allowing the money to grow in interest (00:08:18)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#wifes-continued-work-and-financial-planning",
    "href": "posts/extreme-early-retirement/index.html#wifes-continued-work-and-financial-planning",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The wife decided to continue working, which was factored into the plan, assuming she would work for a few more years to offset some of the spending and provide an extra cushion (00:08:50).\nA plan was written out in advance to prepare for potential stock market declines, including options like taking a loan or getting a part-time job, to help stay calm and rational during emotional times (00:09:28).\nHaving a written plan helped to prepare for a major stock market decline a year into retirement, which ultimately worked out well in the long run (00:09:08).\nIt’s recommended to write out an investment plan while calm and rational to prepare for potential future declines and have a clear plan of action (00:09:37)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#housing-plans-and-government-benefits",
    "href": "posts/extreme-early-retirement/index.html#housing-plans-and-government-benefits",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The family’s current home is a daycare, which is quite spacious, and they plan to downsize in the long term, with any savings from lower taxes, water bills, and other expenses being used to offset potential losses (00:10:14).\nLiving in Canada provides benefits such as government-paid healthcare costs, which have been over-contributed to, covering basic healthcare costs, but not dental, prescription eyewear, or other expenses that are self-insured and paid out-of-pocket (00:10:40).\nGovernment benefits like the Canada Pension Plan and Old Age Security are factored into long-term plans, but with conservative assumptions, such as only one spouse collecting Old Age Security (00:11:05)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#part-time-work-and-enhanced-work-life-balance",
    "href": "posts/extreme-early-retirement/index.html#part-time-work-and-enhanced-work-life-balance",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "A job at the local library was taken to work with great people, enjoy the work, and earn a supplemental income, which is used to fund extra activities, and has helped achieve a better work-life balance (00:11:30).\nReduced working hours have improved overall happiness, and retirement has provided extra time to focus on desired activities and spend quality time with family, particularly children (00:12:10)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#resources-and-further-information",
    "href": "posts/extreme-early-retirement/index.html#resources-and-further-information",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Resources on early retirement, including Tim’s blog and book, are available in the video description for those interested (00:12:43)."
  },
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "Programming",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 15, 2025\n\n\nouse robber\n\n\nHung \n\n\n\n\nApr 4, 2025\n\n\nMaximum subarray\n\n\nHung \n\n\n\n\nMar 21, 2025\n\n\nImage classification\n\n\n \n\n\n\n\nMar 21, 2025\n\n\nDisjoint Set Union\n\n\nKim Hung Bui\n\n\n\n\nMar 21, 2025\n\n\nMinimum Cost Walk in Weighted Graph\n\n\nKim Hung Bui\n\n\n\n\nMar 21, 2025\n\n\nDocker basics\n\n\nKim Hung Bui\n\n\n\n\nMar 20, 2025\n\n\n3191. Minimum Operations to Make Binary Array Elements Equal to One I\n\n\nHung \n\n\n\n\nMar 20, 2025\n\n\nParity invariance\n\n\nHung \n\n\n\n\nMar 18, 2025\n\n\nleetcode-Longest-Nice-Subarray\n\n\nHung \n\n\n\n\nMar 17, 2025\n\n\nDivide Array Into Equal Pairs\n\n\nHung \n\n\n\n\nMar 17, 2025\n\n\nHow to approach most DP problems\n\n\nKim Hung Bui\n\n\n\n\nMar 17, 2025\n\n\nHouse robber\n\n\nHung \n\n\n\n\n\nNo matching items"
  }
]