[
  {
    "objectID": "math/math-statistic-why-normal-distribution-formular/index.html",
    "href": "math/math-statistic-why-normal-distribution-formular/index.html",
    "title": "Why we have normal distribution formular",
    "section": "",
    "text": "To do\n\n\n\nThis is just a photo I took last time I was in Dundee.\n\n\nThat’s it! go for it!"
  },
  {
    "objectID": "math/convex-set/index.html",
    "href": "math/convex-set/index.html",
    "title": "Convex set",
    "section": "",
    "text": "An affine set is one that contains the entire line through any two of its points. Concretely, if \\(x_1,x_2\\) lie in an affine set \\(S\\), then for all $ hetaR$ the combination\n\\[\nx(\\theta) = \\theta x_1 + (1-\\theta) x_2\n\\]\nalso lies in \\(S\\). Special cases:\nFor \\(\\theta \\in[0,1]\\), \\(x(\\theta)\\) traces the line segment between \\(x_1\\) and \\(x_2\\).\nFor \\(\\theta&lt;0\\) or \\(\\theta&gt;1\\), it extends beyond the endpoints.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define two points x1 and x2\nx1 = np.array([1, 2])\nx2 = np.array([4, 0])\n\n# Sample theta values\nthetas = np.array([-0.2, 0.0, 0.6, 1.0, 1.2])\npoints = [(theta * x1 + (1 - theta) * x2) for theta in thetas]\n\n# Generate full line between extended range of theta\ntheta_full = np.linspace(-0.4, 1.4, 100)\nline = np.array([theta * x1 + (1 - theta) * x2 for theta in theta_full])\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.plot(line[:, 0], line[:, 1], 'k-', label='Line through x1 and x2')\nplt.scatter(*x1, color='red', zorder=5)\nplt.text(x1[0]+0.1, x1[1]+0.1, 'x1', fontsize=9)\nplt.scatter(*x2, color='blue', zorder=5)\nplt.text(x2[0]+0.1, x2[1]-0.3, 'x2', fontsize=9)\n\n# Plot labeled theta points\nfor theta, pt in zip(thetas, points):\n    plt.scatter(pt[0], pt[1], s=40)\n    plt.text(pt[0]+0.1, pt[1]+0.1, f'θ = {theta}', fontsize=8)\n\nplt.axis('equal')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.title('Affine Set: Line through x1 and x2')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExample: The solution set of linear equation\n\n\\[\n{x| Ax=b}\n\\]\nis affine\n\n\n\n\n\n\nEvery affine set is a solution set of linear equations\n\n\n\nEvery affine set \\(S \\subseteq \\mathbb{R}^n\\) can be expressed as: \\[\nS = \\{x \\in \\mathbb{R}^n \\mid A x = b\\}\n\\] for some matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) and vector \\(b \\in \\mathbb{R}^m\\).\n\n\n\n\n\n\n\n\nProof sketch\n\n\n\nLet \\(S\\) be an affine set. Pick any point \\(x_0 \\in S\\), and define: \\[\nV = \\{x - x_0 \\mid x \\in S\\}\n\\]\nThis set \\(V\\) is a vector subspace of \\(\\mathbb{R}^n\\), because affine sets are closed under affine combinations, and subtraction cancels the translation offset.\nSince \\(V\\) is a subspace, it can be written as the null space of some matrix \\(A \\in \\mathbb{R}^{m \\times n}\\): \\[\nV = \\{x \\in \\mathbb{R}^n \\mid A x = 0\\}\n\\]\nHence the original affine set is: \\[\nS = \\{x \\in \\mathbb{R}^n \\mid A(x - x_0) = 0\\}\n= \\{x \\in \\mathbb{R}^n \\mid A x = A x_0\\}\n\\]\nLet \\(b = A x_0\\). Then: \\[\nS = \\{x \\in \\mathbb{R}^n \\mid A x = b\\}\n\\]\nSo \\(S\\) is the solution set of a system of linear equations.\n✅ Q.E.D."
  },
  {
    "objectID": "math/convex-set/index.html#affine-sets",
    "href": "math/convex-set/index.html#affine-sets",
    "title": "Convex set",
    "section": "",
    "text": "An affine set is one that contains the entire line through any two of its points. Concretely, if \\(x_1,x_2\\) lie in an affine set \\(S\\), then for all $ hetaR$ the combination\n\\[\nx(\\theta) = \\theta x_1 + (1-\\theta) x_2\n\\]\nalso lies in \\(S\\). Special cases:\nFor \\(\\theta \\in[0,1]\\), \\(x(\\theta)\\) traces the line segment between \\(x_1\\) and \\(x_2\\).\nFor \\(\\theta&lt;0\\) or \\(\\theta&gt;1\\), it extends beyond the endpoints.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define two points x1 and x2\nx1 = np.array([1, 2])\nx2 = np.array([4, 0])\n\n# Sample theta values\nthetas = np.array([-0.2, 0.0, 0.6, 1.0, 1.2])\npoints = [(theta * x1 + (1 - theta) * x2) for theta in thetas]\n\n# Generate full line between extended range of theta\ntheta_full = np.linspace(-0.4, 1.4, 100)\nline = np.array([theta * x1 + (1 - theta) * x2 for theta in theta_full])\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.plot(line[:, 0], line[:, 1], 'k-', label='Line through x1 and x2')\nplt.scatter(*x1, color='red', zorder=5)\nplt.text(x1[0]+0.1, x1[1]+0.1, 'x1', fontsize=9)\nplt.scatter(*x2, color='blue', zorder=5)\nplt.text(x2[0]+0.1, x2[1]-0.3, 'x2', fontsize=9)\n\n# Plot labeled theta points\nfor theta, pt in zip(thetas, points):\n    plt.scatter(pt[0], pt[1], s=40)\n    plt.text(pt[0]+0.1, pt[1]+0.1, f'θ = {theta}', fontsize=8)\n\nplt.axis('equal')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.title('Affine Set: Line through x1 and x2')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExample: The solution set of linear equation\n\n\\[\n{x| Ax=b}\n\\]\nis affine\n\n\n\n\n\n\nEvery affine set is a solution set of linear equations\n\n\n\nEvery affine set \\(S \\subseteq \\mathbb{R}^n\\) can be expressed as: \\[\nS = \\{x \\in \\mathbb{R}^n \\mid A x = b\\}\n\\] for some matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) and vector \\(b \\in \\mathbb{R}^m\\).\n\n\n\n\n\n\n\n\nProof sketch\n\n\n\nLet \\(S\\) be an affine set. Pick any point \\(x_0 \\in S\\), and define: \\[\nV = \\{x - x_0 \\mid x \\in S\\}\n\\]\nThis set \\(V\\) is a vector subspace of \\(\\mathbb{R}^n\\), because affine sets are closed under affine combinations, and subtraction cancels the translation offset.\nSince \\(V\\) is a subspace, it can be written as the null space of some matrix \\(A \\in \\mathbb{R}^{m \\times n}\\): \\[\nV = \\{x \\in \\mathbb{R}^n \\mid A x = 0\\}\n\\]\nHence the original affine set is: \\[\nS = \\{x \\in \\mathbb{R}^n \\mid A(x - x_0) = 0\\}\n= \\{x \\in \\mathbb{R}^n \\mid A x = A x_0\\}\n\\]\nLet \\(b = A x_0\\). Then: \\[\nS = \\{x \\in \\mathbb{R}^n \\mid A x = b\\}\n\\]\nSo \\(S\\) is the solution set of a system of linear equations.\n✅ Q.E.D."
  },
  {
    "objectID": "english/performance-enhancing-drug/index.html",
    "href": "english/performance-enhancing-drug/index.html",
    "title": "Performance-Enhancing Drugs in Sports",
    "section": "",
    "text": "In recent years, there has been a rise in the use of performance-enhancing substances in sports. What are the consequences of doping for athletes? What measures should be taken to combat this issue?\nThere has been a considerable rise in detections for performance-enhancing substances in athletic competitions recently. In my opinion, the most likely consequences for athletes concern their careers and this infraction is best tackled through more sophisticated screening tests.\nIn the majority of cases, athletes risk lasting career repercussions when caught doping. The first and most obvious consequences are financial. An athlete who tests positive for a banned substance will certainly be banned for a defined amount of time, potentially permanently in serious cases, and will forfeit salary and prize money over this period. Beyond raw salary, most athletes receive endorsements and a worsening public image will undoubtedly damage those business relationships as well. Relatedly, an athlete’s legacy is also at stake. A well-known example of this would be the baseball player Barry Bonds who not only lost the final years of an illustrious career to suspensions but has also not been inducted into the Baseball Hall of Fame due to his past transgressions and numerous failed tests.\nIn order to combat the pervasiveness of performance-enhancing drugs, sporting authorities must invest in modernized testing procedures. The most complex testing is currently utilized in the Olympics, however there are flaws in this system as it is often compromised by corruption and the equally sophisticated doping methods of certain nations. A more pertinent case of study would be in sports such as boxing and MMA where testing can literally mean the difference between life and death. In boxing, athletes often agree to their own testing procedures contractually. This method is inferior to vesting power in an authority, such as the UFC, that can ensure tests are modern and fairly conducted. Most UFC athletes are tested at random points throughout the year as well as prior to and immediately after bouts. Applying such a policy across various sports would not only catch the bast majority of infractions but also, more importantly, deter abuse even beginning.\nIn conclusion, athletes risk reputational and financial damage by doping and this can be prevented as testing becomes more comprehensive and potent. These measures are necessary to safeguard the integrity of sporting competitions around the world.\n\nIn recent years, the use of performance-enhancing drugs in sports has increased, posing significant challenges to fair competition. Doping not only harm an athlete’s career and finances but also damages the trust between fans, sponsors, and sporting institutions. I firmly believe that to protect the integrity of sports, it is essential to implement more advanced and transparent testing procedures.\nAthletes who resort to doping face serious long-term consequences that extend beyond temporary suspensions. When an athletes tests positive for banned substances, they risk losing salaries, prize money, and lucrative endorsements. For example, a renowned baseball player might be suspended, causing them to forfeit not only their immediate earnings but also potential future opportunities, such as Hall of Fame induction. This example clearly shows that doping can irreversibly tarnish an athlete’s legacy and career, leading to both financial ruin and a permanent stain on their reputation.\nTo effectively counter the issue of doping, sports authorities must adopt modern, comprehensive testing protocols. While major competitions like the Olympics employ advanced testing, the current is sometimes undermined by corruption and the ever-evolving sophistication of doping techniques. In sports like boxing and mixed martial arts, for instance, some athletes are subjected to self-regulated tests and lack objectivity. In contrast, organizations such as the UFC implement rigorous, random testing both before and after events, ensuring a fairer system. Adopting these strict measures across all sports would not only detect infractions more reliably but also serve as a strong deterrent against doping. By ensuring that testing is both modern and impartial, the integrity of competitive sports can be maintained, safeguarding athletes’ careers and reputation of the sport.\nIn conclusion, doping leads to severe financial and reputational damage for athletes while undermining the fairness of sports competitions. Only through the implementation of advanced, transparent testing procedures can we hope to deter doping and preserve the true spirit of sportsmanship for future generations.\n\nVocabulary\nFor extra practice, write an antonym (opposite word) on a piece of paper to help you remember the new vocabulary: 1. There has been a considerable rise in detections - There has been a significant increase in identifications 2. performance-enhancing substances - performance-boosting substances 3. athletic competitions - sporting events 4. most likely consequences for - probable outcomes for"
  },
  {
    "objectID": "english/writing-by-hand/index.html",
    "href": "english/writing-by-hand/index.html",
    "title": "IELTS Essay Writing by Hand",
    "section": "",
    "text": "Question:\nFewer and fewer people today write by hand using a pen or pencil.\nWhat are the reasons for this?\nIs this a positive or a negative development?\nDavid Lang’s essay:\nToday, the act of writing by hand using a pen or pencil has become increasingly uncommon. This essay will delve into the reasons behind this trend and evaluate whether it is a positive or negative development for individuals and society as a whole.\nOne primary reason for the dwindling use of handwriting is the pervasive presence of technology. Inn an age dominated by computers, smartphones, and tables, individuals find it more convenient to type their thoughts and ideas rather than engaging in the laborious process of writing by hand. The speed and efficiency offered by digital devices have made it easier to edit, store, and share written content, rendering handwriting seemingly obsolete. To illustrate this, consider the rapid rise of communication platforms such as email, instant messaging apps, and social media. These platforms provide users with the ability to effortlessly send and receive messages, eliminating the need for handwritten letters or notes. This convenience extends to practical considerations for professionals in various fields, such as journalists, researchers, and writers, who often rely on digital platforms and word processors for their work. The ease of editing, revising, and sharing digital content outweighs the traditional pen-and-paper approach.\nWhile the decline of handwriting may offer certain advantages, it also brings forth potential negative consequences. Handwriting is an essential skill that fosters cognitive development, improves memory retention, and allows for creative expression. The act of physically forming letters and words engages different areas of the brain, promoting better comprehension and critical thinking skills. Losing this skill may hinder individuals ability to think and express themselves effectively. Most importantly, the personal touch and emotional connection associated with handwritten letters or cards cannot be replicated by digital alternatives. Handwritten messages convey sincerity, thoughtfulness, and a see of intimacy. Losing the art of handwriting may irrevocably erode these meaningful human connections.\nIn conclusion, the decline of handwriting can be attributed to the prevalence of technology and practical considerations. While there are advantages to digital communication and typing, the negative implications for cognitive development and personal connections are more significant and must be mitigated by the relevant authorities.\nArguing that it is a positive development\nIn today’s digital age, fewer and fewer people write by hand and using a pen or pencil. This shift can be attributed to pared technological advancements and changing educational practices, and I firmly believe that it is a positive development. The move away from handwriting not only enhances efficiency and accessibility but also reflects progress towards modern communication methods.\nThe decline in handwriting is primarily driven by the ubiquity of digital devices and evolving communication needs. With the widespread availability of computers, smartphones, and tablets, individuals find it more convenient to type and send messages instantly. Educational institutions are increasingly incorporating technology into their curricula, emphasizing digital literacy over traditional penmanship. Moreover, the rise of social media and email as primary communication tools has diminished the need for handwritten correspondence. These factors have collectively contributed to a cultural shift where efficiency and speed in communication are prioritized over the art of handwriting.\nTransitioning from handwriting to digital writing brings several significant benefits. Firstly, digital communication enhances productivity by enabling rapid information exchange and easy editing, which is especially beneficial in professional and academic settings. Additionally, typing reduces issues related to illegible handwriting, thereby minimizing misunderstandings. Digital platforms also offer vast storage and the ability to search through documents instantly, streamlining information management. Furthermore, this shift is environmentally friendly, as it reduces the consumption of paper and other stationery supplies. Overall, the adoption of digital writing foster greater accessibility and adaptability in our fast-pace, globalized world, contributing to more efficient and sustainable practices.\nIn conclusion, the decreasing reliance on handwriting in favor of digital communication is driven by technological advancements and evolving social practices. I contend that this development is positive, as it significantly boosts efficiency, clarity, and environmental sustainability while aligning with the demands of modern life. Embracing digital writing is not only a natural progression but also a vital step towards more effective and accessible communication in the 21st century."
  },
  {
    "objectID": "english/basic-structures/index.html",
    "href": "english/basic-structures/index.html",
    "title": "5 basic structures of sentences in english",
    "section": "",
    "text": "All sentences in English are included in 1 of 5 structures. The verb used will decide the structure of the sentence. We should use the subject + verb structure as the core structure when speaking in English, after that, we choose a verb and complete the sentence.\n\n\n\n\n5 simple structure in english\n\n\n\n\n1\nIf we want to say “subject do something”, we use structure 1 “subject + verb”. Verbs which can be used in that structure are “to be” and transitive verbs.She stay at the student center.\n\n\n2\nTo talk about what the subject is like, we use structure 2: “subject + verb + complement”. Verbs which can be used in in the structure “to be” and “linking verbs” like: become, look, sound, smell, seem, …. In that case, complement after that verb is ‘adjective’ or ‘noun phrase’.Her brother is a soccer player"
  },
  {
    "objectID": "english/financial-matters/index.html",
    "href": "english/financial-matters/index.html",
    "title": "IELTS Essay Financial Matters",
    "section": "",
    "text": "Question:\nIn numerous nations, there is a growing trend for individuals to frequently discuss financial matters, such as their earnings or expenditure, in everyday conversation. What are the reasons behind this phenomenon? Is it advantageous or detrimental?\n\nThere is an increasing tendency for individuals in the modern world to openly discuss their finances in daily conversation. In my opinion, this is a result of a general lack of self-awareness and is decidedly negative given the opportunity cost it engenders.\nThe primary reason many people are prone to talk their financial dealings originates in society generally. In the past, a sense of modesty or concern for decorum might have compelled reticence. Today, some of the most widely discussed topics in the media, online, in film, and in literature concern cryptocurrencies, the real estate market, stock equities, the price of gold, and other investments. A person discussing these matters is likely only interested in subtle boasting if venting frustrations without awareness of the emotions or engagement of their interlocutor. The result over time has been that many people have developed a shameless habit.\nIn my view, discussion finances often precludes a number of more worthwhile topics of conversation. Since the majority of useful financial research can be conducted online or done in professional settings, there are more pressing areas that should be discussed in daily life. For example, two friends having a conversation at a cafe would derive greater value from telling stories, joking with one another, or delving into a more serious topic related to politics, the arts, or psychology. Such discussions are likely to develop their understanding of the world and their emotional intelligence. Discussion of money, on the other hand, wastes valuable time and energy for little concrete outcome.\nIn conclusion, the inclination to speak of one’s personal finances stems from a fundamentally modern self-indulgence and often prevents the discussion of more enjoyable or meaningful topics. Naturally, these generalizations do not apply to all individuals.\n\nChatGPT’s essay:\nIn numerous nations, a growing trend has emerged where people openly discuss financial matters, including their earnings and expenditures, in everyday conversations. This phenomenon can be attributed to a range of social and economic factors, and i firmly believe that it is advantageous. Open dialogue about personal finances not only foster financial literacy but also encourages better money management and transparency within communities.\nOne primary reason for this trend is the rapid spread of financial education and the influence of digital media. With the advent of social media platforms and financial blogs, individuals have unprecedented access to information about budgeting, investing, and saving. This exposure has led many to feel more confident in discussing their financial situations, as they seek advice and validation from peers. Additionally, economic uncertainty in many regions has prompted individuals to share personal financial experiences as a way of coping and learning from one another. For instance, during periods of economic downturn, people often exchange tips on reducing expenses or finding alternative income sources. This collective sharing not only helps individuals navigate challenging time but also creates a support network where practical financial strategies are exchanged openly.\nThe open discussion of financial matters carries significant benefits for both individuals and society at large. Firstly, it promotes financial literacy by normalizing conversations about money, which can demystify complex financial topic and empower people to make informed decisions. As people become more knowledgeable about financial planning and investment strategies, they are better equipped to secure their futures and contribute to economic growth. Secondly, this trend encourages transparency and accountability, reducing the stigma around discussing personal finances. For example, when individuals talk about budgeting and debt management, it can inspire others to examine their own spending habits, ultimately leading to a more financially responsible community. By fostering an environment where financial struggles and successes are shared openly, society as a whole benefits from improved financial stability and reduced inequality.\nIn conclusion, the increasing tendency for individuals to discuss financial matters in everyday conversation is driven by greater access to information and a need for mutual support during uncertain economic times. I contend that this development is advantageous, as it enhances financial literacy, transparency, and responsible money management. Ultimately, such open dialogue can empower communities to achieve greater financial stability and economic well-being.\n\nGrammar structures\n\nComplex Sentence Structures\n\nUse of subordinate clauses to add detail. Example: “With the advent of social media platforms and financial blog, individuals have unprecedented access to information about budgeting, investing, and saving.”\n\nCause and Effect Constructions\n\nPhrases such as “can be attributed to” and “leads to” explain reasons and consequences. Example: “This phenomenon can be attributed to a range of social and economic factors.”\n\nConditional and Hypothetical Phrasing:\n\nImplicit conditionals that suggest potential outcomes or benefits. Example: “when individuals talk about budgeting and debt management, it can inspire others to examine their own spending habits.”\n\nComparative Structure:\n\nUsing comparisons to show differences in outcomes. Example: “For instance, during periods of economic downturn, people often exchange tips on reducing expenses or finding alternative income sources.”\n\nUse of Passive Voice:\n\nPassive constructions to emphasize processes or outcomes rather than actors.\nExample: “This phenomenon can be attributed to a range of social and economic factors.”\n\nLinking Devices and Transitional Phrases:\n\nWords and phrases such as “Firstly,” “Secondly,” “In conclusion,” and “For example” to organize ideas and guide the reader.\n\n\n\n\nLexical Items\n\nAdvanced Vocabulary:\n\nWords such as “unprecedented,” “demystify,” “transparency,” “accountability,” and “empower” elevate the formality and clarity of the argument.\n\nFinancial Terminology:\n\nSpecific terms related to finance like “budgeting,” “investing,” “saving,” “financial literacy,” “debt management,” and “spending habits.”\n\nDescriptive Adjectives:\n\nTerms that add detail and nuance, for example “open dialogue,” “collective sharing,” “rigorous, random testing,” and “financially responsible.”\n\nCollocations and Fixed Phrases:\n\nCommon phrases that support the essay’s theme such as “financial matters,” “economic uncertainty,” “personal financial experiences,” and “mutual support.”\n\nTransitional Vocabulary:\n\nWords that link ideas logically like “furthermore,” “additionally,” and “ultimately” ensure smooth progression throughout the essay.\n\nOpinion and Evaluation Language:\n\nPhrases that express clear stance such as “I firmly believe,” “it is essential,” “I contend that,” and “I contend that this development is advantageous.”"
  },
  {
    "objectID": "english/english-recycle-paper/index.html",
    "href": "english/english-recycle-paper/index.html",
    "title": "Task 1 Recycle paper",
    "section": "",
    "text": "David Lang\nThe diagram details how recycled paper is made. Looking from an overall perspective, it is readily apparent that the process involves three main stages beginning with initial collection and transportation, followed by cleaning and processing, and lastly the rolling of the final product. The process is entirely man-made.\nIn order to produce recycled paper, used paper is first sourced and transported by conveyor belt to a processing plant. The used paper is then made into unfiltered pulp by using water and various chemicals. The pulp must then be filtered with water being removed.\nAfter filtering is complete, the filtered paper pulp is sent by a tube to a container with water and soap to be further cleaned. Air is pumped into the container and ink is removed. There is then an additional cleaning with water and chemicals before the final rolling states commence. To create useable recycled paper rolls, the cleaned and filtered pulp is placed on a conveyor belt where heated rollers flatten the paper and remove excess water.\nChat GPT\n\nOverall process: cyclical but shown in a linear step-by-step format.\nNumber of stages: 4 main stages - making pulp, filtering, cleaning, and making paper.\nRaw material: Used paper.\nEnd product: Recycled paper rolls.\nMaterials/chemicals used: Water\nKey step:\n\n\nUsed paper transported on a conveyor belt.\nMixed with water and chemicals - pulp.\nFiltered to remove larger impurities.\nCleaned using soap, air, water -&gt; removes ink.\nFinal cleaned pulp used to make paper -&gt; heated rollers + drying.\n\nUseful vocab: - Recycled materials - Industrial process - Mechanical separation - De-inking - Environmental sustainability\nThe diagram illustrates the step-by-step process involved in the production of recycled paper from used paper materials. The procedure consists of four main stages, starting with the creation of pulp and culminating in the manufacture of new paper rolls.\nIn the first stage, used paper is placed on a conveyor belt and carried to a machine where it is mixed with water and chemicals. This mixture breaks the paper down into unfiltered pulp. The pulp then moves to the next stage where is it filtered. This step removes large unwanted materials, leaving behind filtered pulp.\nIn the third stage, the filtered pulp is cleaned using water, soap, and air. This helps separate and remove ink from the pulp. Water and additional chemicals are also added to enhance the cleaning process. As a result, the pulp becomes cleaner and suitable for making new paper.\nFinally, the cleaned pulp is transferred onto a conveyor belt and passed through heated rollers. These rollers press and dry the material, forming sheets of recycled paper, which are then rolled up for reuse.\n\nEnvironment\n\nHow are environmental problems dealt with in your country? In my country, environmental issues are taken seriously, but there’s still a long way to go. The government has made some rules about pollution and waste, and there are programs to plant more trees. But I think sometimes the laws are not enforced strictly, and people still throw garbage in the wrong places. In my country, environmental problems are addressed through a combination of legislation and public awareness campaigns. For instance, the government has introduced stricter regulations on industrial emissions and waste disposal. However, enforcement remains inconsistent, and public participation is not as widespread as it could be, which limits the overall effectiveness of these measures.\nWhat can be done to make people recycle more often? I think if people know more about the benefits of recycling, they might do it more. Also, the government can give rewards or discounts to people who recycle. Putting more recycling bins around cities would help too. To encourage more frequent recycling, a combination of education and incentives could be effective. Public awareness campaigns can highlight the environmental and economic benefits of recycling, while practical steps- like providing more accessible recycling bins and offering financial incentives - can make the habit easier to adopt. Legislation that mandates in households and businesses could also play a significant role.\nWhat is the most pressing environmental problem? In my opinion, the biggest problem is air pollution, especially in big cities. The air quality is really bad because of cars and factories. It affects people’s health, and it’s hard to fix quickly. Arguably, climate change is the most urgent environmental issue we face today. It not only exacerbates other problems like extreme weather and rising sea levels but also poses serious threats to biodiversity and food security. While localized issues like air pollution are severe, climate change has a truly global impact that requires immediate and coordinated action.\nIs recycling a common practice in your country? Recycling is becoming more common, but it’s not something everyone does. Some people separate their trash at home, but many still throw everything together. It depends a lot on where you live - cities usually have more systems in place. Recycling is practiced in many parts of the country, particularly in urban areas where infrastructure is more developed. However, it’s far from universal. While some individuals and communities are highly conscientious, others lack access to proper facilities or are simply unaware of how to recycle effectively. So, it’s growing, but still inconsistent.\nAre governments or individuals more responsible? I think both are responsible. The government needs to make good rules and make people follow the, but individuals also need to care about the environment and take action. It’s a shared responsibility. While both governents and individuals play vital roles, I believe governents bear greater responsibility due to their ability to enact policies, regulate industries, and invest in sustainable infrastructure. That said, individual behavior is equally crucial, as collective action often stes fro personal choices. Ideally, both should work hand in hand to address environental issues effectively."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html",
    "href": "english/english-writting-zoos/zoos.html",
    "title": "Zoos",
    "section": "",
    "text": "Zoos have long been a subject of debate. While some people argue that zoos play a crucial role in conserving endangered species and educating the public about wildlife, others contend that confining animals to artificial environments is unethical. Discuss both views and give your own opinion."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#part-2",
    "href": "english/english-writting-zoos/zoos.html#part-2",
    "title": "Zoos",
    "section": "Part 2",
    "text": "Part 2\nQuestion Describe a place where you can see or interact with animals. You should say: - what a place is - where it is - what you can do there and explain how you feel about this place."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#sample-answer",
    "href": "english/english-writting-zoos/zoos.html#sample-answer",
    "title": "Zoos",
    "section": "Sample answer",
    "text": "Sample answer\nOne place that comes to mind is the Singapore Zoo, which I visited a couple of years ago. It’s located in the northern part of the city and is widely regarded as one of the most ethical and well-designed zoos in the world.\nWhat impressed me most was the open-concept layout. Instead of case, many animals live in naturalistic habitats separated from visitors by moats or hidden barriers. I was able to observe orangutans swinging freely in trees above the walkways and giraffes roaming spacious enclosures.\nThere were also interactive sessions, such as feeding times and educational talks by zookeepers, where I learned a lot about endangered species and conservation efforts. The zoo also partners with various wildlife protection organizations and supports breeding programs for rare species.\nI felt genuinely inspired by the experience. It made me realize that when zoos are operated responsibly, they can help preserve biodiversity and raise public awareness about the threats facing wildlife today. I felt the zoo feeling hopeful and more committed to supporting conservation causes."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#part-3",
    "href": "english/english-writting-zoos/zoos.html#part-3",
    "title": "Zoos",
    "section": "Part 3",
    "text": "Part 3\n\nQ1: Do you think zoos are necessary in today’s world?\nYes, I believe they are still necessary, but only under strict ethical and scientific standards. With habitat destruction accelerating and many species on the brink of extinction, zoos can serve as safe havens for breeding and research. That said, zoos that exist solely for entertainment, with poor living conditions, should be phased out.\n\n\nQ2: How can zoos educate the public?\nZoos have a unique ability to engage people emotionally by allowing close encounters with animals they would never see in the wild. Interactive exhibits, live demonstrations, and conservation talks can make complex environmental issues accessible and compelling, especially for children.\n\n\nQ3: What alternatives to zoos do you think could replace their role in education and conservation?\nWildlife sanctuaries and virtual reality experiences might be viable alternatives. Sanctuaries allow animals to live in more natural settings, while technology can simulate immersive encounters without keeping animals in captivity. However, these solutions aren’t always scalable or practical for endangered species preservation, so ethical zoos still have a role to play. # Gre essay\n\nThe director of a local zoo has claimed that the zoo must expand its exotic animal exhibit to attract more visitors. He cites a recent survey showing that 85% of new zoo visitors came specifically to see exotic animals. Based on this, he concludes that investing in new exotic species will increase overall ticket sales and boost the zoo’s revenue."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#real-test-thinking-process",
    "href": "english/english-writting-zoos/zoos.html#real-test-thinking-process",
    "title": "Zoos",
    "section": "Real-test thinking process",
    "text": "Real-test thinking process\n\nRead prompt carefully\nIdentify assumptions, conclusions, evidence. - Conclusion: Expanding exotic animal exhibit -&gt; more visitors -&gt; more revenue. - Evidence: 85% of new visitors came for exotic animals. - Assumptions: - Exotic animals are the reason they came. - New visitors = future revenue - Expansion will not affect other zoo aspects negatively. - Survey was accurate and representative."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#outline-essay",
    "href": "english/english-writting-zoos/zoos.html#outline-essay",
    "title": "Zoos",
    "section": "Outline essay",
    "text": "Outline essay\nPlan our 4-5 paragraph structure - Intro: Restate the argument + claim it’s flawed - Body 1: Question the link between visitor intent and exotic animals - Body 2: Weak survey design or narrow scope. - Body 3: Ignored costs/downsides of expansion. - Conclusion: Suggest further research/data before acting"
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#sample-gre-argument-essay",
    "href": "english/english-writting-zoos/zoos.html#sample-gre-argument-essay",
    "title": "Zoos",
    "section": "Sample GRE Argument essay",
    "text": "Sample GRE Argument essay\nThe argument that expanding the exotic animal exhibit will increase zoo revenue is unconvincing, as it rests on several questionable assumptions and lacks sufficient supporting evidence. Without addressing these issues, the zoo may risk investing in an initiative that fails to produce the expected benefits.\nFirst, the argument assumes a direct causal relationship between exotic animals and increased attendance. While the director cites a survey indicating that 85% of new visitors came specifically to see exotic species, this data may be misleading. The survey does not clarify whether these visitors would have attended the zoo regardless of the exhibit, nor does it prove that exotic animals were the primary motivator. Other factors such as promotions, seasonal events, or group tours may have influenced their visit. Without clear evidence of causation, the conclusion remains speculative.\nSecondly, the reliability of the survey itself is questionable. The argument does not provide information about the sample size, methodology, or representativeness of the surveyed visitors. For instance, if the survey was conducted only during a temporary exotic animal showcase, the data may not reflect general visitor preferences. Additionally, focusing solely on new visitors ignores the preferences of returning guests, who may value other aspects of the zoo such as local wildlife, interactive experiences, or conservation programs.\nFurthermore, the argument overlooks potential downsides and costs associated with expanding the exhibit. Exotic animals often require specialized care, habitats, and veterinary expertise, which can significantly increase operating expenses. If the new exhibit fails to attract sufficient new visitors or if existing patrons are alienated by the change, the zoo might face financial losses rather than gains. A cost-benefit analysis would be necessary to determine whether the investment is justified.\nIn conclusion, the argument makes a hasty recommendation based on incomplete and possibly flawed evidence. To strengthen the case, the director should conduct more comprehensive research, including long-term visitor trends, preferences of different demographics, and a detailed financial projection. Only then can the zoo make an informed decision about expanding its exotic animal exhibit."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#introduction",
    "href": "english/english-writting-zoos/zoos.html#introduction",
    "title": "Zoos",
    "section": "Introduction",
    "text": "Introduction\nThe argument presented concludes that [paraphrase conclusion]. While this may seem reasonable at first glance, the argument relies on several questionable assumptions and lacks critical evidence. Without addressing these flaws, the conclusion remains unconvincing."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#body-paragraph-1---questionable-assumption",
    "href": "english/english-writting-zoos/zoos.html#body-paragraph-1---questionable-assumption",
    "title": "Zoos",
    "section": "Body Paragraph 1 - Questionable Assumption",
    "text": "Body Paragraph 1 - Questionable Assumption\nFirst, the argument assumes that [state key assumption]. However, this assumption may not hold true because [explain why it could be false or oversimplified]. If [state alternative possibility], the conclusion would no longer be valid. Therefore, without stronger evidence linking [premise] to conclusion, the argument is significantly weakened."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#body-paragraph-2---flawed-evidence-or-methodology",
    "href": "english/english-writting-zoos/zoos.html#body-paragraph-2---flawed-evidence-or-methodology",
    "title": "Zoos",
    "section": "Body Paragraph 2 - Flawed Evidence or Methodology",
    "text": "Body Paragraph 2 - Flawed Evidence or Methodology\nSecondly, the evidence provided is insufficient or potentially flawed. For example, [identify weakness: small sample size, biased survey, outdated data, etc.]. Without details about [methodology, demographics, time period, etc.], it is difficult to trust the data as a solid basis for the conclusion. The argument would be stronger with [mention what kind of evidence would help]."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#body-paragraph-3---ignored-alternatives-or-consequences",
    "href": "english/english-writting-zoos/zoos.html#body-paragraph-3---ignored-alternatives-or-consequences",
    "title": "Zoos",
    "section": "Body Paragraph 3 - Ignored Alternatives or Consequences",
    "text": "Body Paragraph 3 - Ignored Alternatives or Consequences\nFurthermore, the argument ignores other possible explanations or consequences. Even if [the main assumption is true], it is possible that [state another variable, downside, or unintended effect]. The argument fails to consider [additional cost, risk, or complexity], which could undermine the proposed benefits."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#conclusion",
    "href": "english/english-writting-zoos/zoos.html#conclusion",
    "title": "Zoos",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, the argument is based on unproven assumptions, lacks reliable evidence, and ignores important alternatives. To strengthen the argument, the author would need to provide [better data/methodology], consider [other relevant factors], and support the claim with a thorough analysis. Until then, the conclusion remains logically unpersuasive."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html#plug-and-play-phrases",
    "href": "english/english-writting-zoos/zoos.html#plug-and-play-phrases",
    "title": "Zoos",
    "section": "Plug-and-play Phrases",
    "text": "Plug-and-play Phrases\n\nThe argument assumes that …\nHowever, this assumption is not necessarily valid because…\nThe evidence is also questionable, as it does not specify …\nMoreover, the argument overlooks…\nTo improve the argument, the author should provide…\nWithout this, the reasoning remains flawed and speculative"
  },
  {
    "objectID": "english/english-the-internet-books/index.html",
    "href": "english/english-the-internet-books/index.html",
    "title": "Ielts task 1 - Household work",
    "section": "",
    "text": "In the past, knowledge was contained in books. Nowadays, knowledge is uploaded to the internet. Do the advantages of this outweigh the disadvantages?\nIt is a fact of modern life that most information today is store online, rather than in physical books as it was in the past. In my opinion, this is a negative despite its obvious conveniences.\nSupporters of this trend can point to the numerous ways knowledge kept online can be disseminated and used. Online information is almost universally accessible, narrowing socioeconomic differences in society and allowing for various conveniences. First of all, users can find relevant information on search engines from a wide range of sources including user-generated sites like Wikipedia to news outlets like The New York Times and academic journals as well. Once an individual locates the information they are looking for, it is also easier to search within that article, share it with others, have it wherever they go and store more than would be possible if it were a hard copy.\nNonetheless, storing information online encourages a dependence on computer technology that is harmful for the human mind. In the past, if a person wanted to research a given topic, they would have to check out a library book or talk with an expert. This process was much slower and open to the possibility of bias. However, it also meant that individuals were more conscientious and patiently learned from fuller, more nuanced sources. Nowadays, most people read short articles, hastily written to attract more clicks and generate advertising revenue. These tap into a fundamental human need to be constantly engaged and feeling pleasure. Over time, this has led to a generation addicted to devices, lacking the healthy habits engendered by careful research and study.\nIn conclusion, despite the handiness of online information, the impact on the human psyche makes this a negative trend on level. Individuals must themselves find ways to counter these potentially injurious effects.\nSample positive essay:\nIn the digital age, the migration of knowledge from physical books to online platforms has become a defining feature of global society. While critics argue that this shift undermines intellectual depth and fosters dependency, I firmly believe the advantages of accessible, dynamic, and. collaborative information far surpass its drawbacks.\nProponent of online knowledge store emphasize its unparalleled capacity to democratize information. Unlike traditional books, which are more constrained by physical availability and cost, digital platforms grant instantaneous access to vast repositories of data for anyone with an internet connection. Students in remote villages, for instance, can now access lectures from Ivy League universities, download academic journals, or participate in global forums - opportunities once reserved for the socioeconomically privileged. Moreover, search engines and algorithms allow users to pinpoint precise information within seconds, a task that previously required hours of library search. The integration of multimedia - such as video, interactive graphs, and hyperlinks- further enriches understanding, catering to diverse learning styles. Crucially, digital storage eliminates the environmental toll of paper production, reducing deforestation and carbon emissions linked to printing and distributing physical books.\nDetractors, however, contend that the internet fosters misinformation, distraction, and superficial engagement. While valid, these concern are not insurmountable. It is true that unverified content proliferates online, yet this issue is counterbalanced by the rise of fact-checking tools, peer-reviewed databases, and digital literacy programs that empower users to discern credible sources. Similarly, although the constant influx of notifications and bite-sized content risks fragmenting attentions spans, individuals and educators can cultivate disciplined usage through structured learning frameworks and mindfulness practices. The digital divide-limited internet access in impoverished regions - remains a challenge, yet global initiatives to expand connectivity, such as satellite internet projects and affordable mobile data, are steadily bridging this gap.\nIn conclusion, the transition to online knowledge storage represents a net positive for humanity. Its capacity to democratize education, enhance efficiency, and promote sustainability far outweighs manageable risks like misinformation and distraction. To maximize benefits, society must prioritize digital literacy and equitable access, ensuring the internet remains a force for collective progress.\n\nSpeaking\n\nHow often do most people use the internet? Most people use the internet daily, if not constantly. Smartphones and Wi-Fi have made it ubiquitous whether for work, social media, or streaming. For instance, younger generations might spend 6+ hours online, while older adults might use it less frequently but still rely on it for news or banking.\nAre there drawbacks to its overuse? Absolutely. Overuse can lead to addiction, reduced productivity, and mental issues like anxiety or loneliness. Physically, screen time strains eyes and disrupts sleep. Socially, it erodes face-to-face interactions - think of families ignoring each other to scroll through phones. Moderation is key.\nWhat kind of information can people find online? Virtually anything. From academic research on Google Scholar to DIY tutorials on Youtube, and even niche forums discussing obscure hobbies. However, misinformation is rampant too - fake news, biased articles, or conspiracy theories. Critical thinking is essential to navigate this flood of data.\nHow was this changed the way people get their information? It’s deepen integration into daily life. Think smart homes controlled via apps, AI-driven healthcare consultations, or virtual reality workspaces. However, privacy concerns will grow, and the digital divide may widen unless access becomes universal. Education and jobs will increasingly depend on digital literacy."
  },
  {
    "objectID": "english/see-technique/index.html",
    "href": "english/see-technique/index.html",
    "title": "SEE technique in speaking",
    "section": "",
    "text": "🌸𝓦𝓱𝓪𝓽 𝓲𝓼 𝔂𝓸𝓾𝓻 𝓯𝓪𝓿𝓸𝓻𝓲𝓽𝓮 𝓴𝓲𝓷𝓭 𝓸𝓯 𝓶𝓾𝓼𝓲𝓬?🌸\n𝐋𝐞𝐯𝐞𝐥: 𝐄𝐚𝐬𝐲✅ ☘️ 𝐒 (𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭) = Well, to be honest, I like listening to pop music the most. ☘️ 𝐄 (𝐄𝐱𝐩𝐥𝐚𝐧𝐚𝐭𝐢𝐨𝐧) = I love it because it helps me to relieve stress, especially after a long day of work and study. Moreover, I usually listen to a lot of pop songs of famous singers and bands. ☘️ 𝐄 (𝐄𝐱𝐚𝐦𝐩𝐥𝐞) = For example, I usually stream “Roar” by Katy Perry on Spotify. I always feel super confident whenever I listen to it. 𝐋𝐞𝐯𝐞𝐥: 𝐃𝐢𝐟𝐟𝐢𝐜𝐮𝐥𝐭 🔥 ☘️ 𝐒 (𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭) = When it comes to this question, I instantly think about pop music, which is the music genre I have fallen in love with since I was a kid. ☘️ 𝐄 (𝐄𝐱𝐩𝐥𝐚𝐧𝐚𝐭𝐢𝐨𝐧) = There are so many reasons not to love it. But I guess I am a huge fan of pop music because it helps me to chill out, especially after a nerve-racking days of work and study. ☘️ 𝐄 (𝐄𝐱𝐚𝐦𝐩𝐥𝐞) = There’s actually one song that I often listen to and that is “Roar” by Katy Perry. It it not only catchy, but also has a very meaningful message of being fearless regardless of challenges ahead."
  },
  {
    "objectID": "english/transitive-verb/transitive verbs.html",
    "href": "english/transitive-verb/transitive verbs.html",
    "title": "Transitive Verb an Intransitive Verb",
    "section": "",
    "text": "A transitive verb is a verb that requires an object to receive the action.\nExample Sentences:\nSome other examples of transitive verbs are “address,” “borrow,” “bring,” “discuss,” “raise,” “offer,” “pay,” “write,” “promise,” and “have.”"
  },
  {
    "objectID": "english/transitive-verb/transitive verbs.html#direct-and-indirect-objects",
    "href": "english/transitive-verb/transitive verbs.html#direct-and-indirect-objects",
    "title": "Transitive Verb an Intransitive Verb",
    "section": "Direct and Indirect Objects",
    "text": "Direct and Indirect Objects\nA transitive verb can take more than one object.\n\nDonovan gave his sister a laptop.\n\nIn this sentence, there is an indirect object, “his sister,” and a direct object, “a laptop.” However, there is another way to say this same idea using a prepositional phrase.\n\nDonovan gave a laptop to his sister. In English, an indirect object may come between a transitive verb and the direct object, as the first example sentence about Donovan, or the indirect object could be in the form of a prepositional phrase, like the second example sentence about Donovan.\n\nAn indirect object is only needed if the action is being done to or for somebody, when using a transitive verb, we need to include a direct object, but we may not need to include in indirect object."
  },
  {
    "objectID": "english/transitive-verb/transitive verbs.html#finding-the-object",
    "href": "english/transitive-verb/transitive verbs.html#finding-the-object",
    "title": "Transitive Verb an Intransitive Verb",
    "section": "Finding the object",
    "text": "Finding the object\nWe can figure out the object by using the question format: “The subject did what?” or “The subject ‘verb’ what?” The instructor addressed what the student’s question\nMiriam borrowed what? the methodology book\nCan you bring what? your copy of the textbook\nThe speaker discussed what? different marketing strategies\nThe committee members will raise what? money\nWe can find the indirect by asking the question “To whom?” or “For whom?”\nDonovan gave a laptop to whom? his sister"
  },
  {
    "objectID": "english/line-chart-social-center/index.html",
    "href": "english/line-chart-social-center/index.html",
    "title": "IELTS Essay- Line Chart Social Center",
    "section": "",
    "text": "David Lang essay:\nThe line chart details participants according to activity at a social center in the Australian city of Melbourne from 2000 to 2020. Looking from an overall perspective, it is readily apparent that participants for all activities increased to varying degrees except in terms of martial arts and amateur drama. Film club was by far the most popular activity throughout, though table tennis demonstrated the strongest growth.\nIn 2000, 64 individuals participated in the film club, a figure that dipped to 60 in 2010 and then more than recovered to finish the period at 65. The trend for martial arts was similarly stable with a beginning data point of 38, a high of 39 in 2010, and a final figure of 37.\nThe remaining activities started the period lower with 27 participants for amateur drama, 18 for table tennis, and 0 for musical performances (unavailable at the time). Over the next 10 years, amateur dramatics commenced a decline that would end at just 5 by 2020, while table tennis underwent the inverse transformation, peaking at 52 by the same year. Musical performances were only an option starting in 2005, with participants growing and leveling off to just under 20 by the end of the period."
  },
  {
    "objectID": "english/english-topic-sentence/index.html",
    "href": "english/english-topic-sentence/index.html",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "",
    "text": "The standard essay structure for a IELTS Task 2 essay is four paragraphs - an introduction, two body paragraphs and a conclusion.\nThe topic sentence is the first sentence of each body paragraph. It tells the examiner the main idea of the paragraph so it functions like a mini introduction."
  },
  {
    "objectID": "english/english-topic-sentence/index.html#analyze-sample-essays",
    "href": "english/english-topic-sentence/index.html#analyze-sample-essays",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "Analyze sample essays",
    "text": "Analyze sample essays\n\nLook at some task 2 sample essays.\nHighlight the topic sentence(s) - what is the topic? What is the main idea?\nRead the body paragraph(s) - do all the supporting ideas relate to that idea?"
  },
  {
    "objectID": "english/english-topic-sentence/index.html#practice-writing-topic-sentences-for-sample-essays",
    "href": "english/english-topic-sentence/index.html#practice-writing-topic-sentences-for-sample-essays",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "Practice writing topic sentences for sample essays",
    "text": "Practice writing topic sentences for sample essays\n\nLook at some task 2 sample essay.\nCover up the first sentence of the body paragraph(s).\nRead the body again - what is the topic and main idea ?\nWrite a topic sentence for that paragraph, then compare it with the original one. ## Analyze our own writing\nLook back through our old task 2 essays.\nAre the topic sentences good? If not, write new ones. ## Practice writing topic sentences for new question\nLook at some sample task 2 question (there is one below)\nAnalyze the question and brainstorm main ideas.\nChoose the main topic and main idea for each body paragraph.\nWrite our topic sentences."
  },
  {
    "objectID": "paid attention to.html",
    "href": "paid attention to.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": ": to concentrate on\nPay (close/careful) attention to what she says."
  },
  {
    "objectID": "let things slide.html",
    "href": "let things slide.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "to not do anything about something or someone when you should try to change or correct that thing or person.\neg: You were late this morning. I’ll let it slide this time, but don’t let it happen again.\nIt may be best to let things slide this time."
  },
  {
    "objectID": "programming/leetcode-word-ladder/index.html",
    "href": "programming/leetcode-word-ladder/index.html",
    "title": "127. Word Ladder",
    "section": "",
    "text": "A transformation sequence from word beginWord to word endWord using a dictionary wordList is a sequence of words beginWord -&gt; s1 -&gt; s2 -&gt; ... -&gt; sk such that:\nExample 1:\nExample 2:\nConstraints:\n1 &lt;= beginWord.length &lt;= 10 endWord.length == beginWord.length 1 &lt;= wordList.length &lt;= 5000 wordList[i].length == beginWord.length beginWord, endWord, and wordList[i] consist of lowercase English letters. beginWord != endWord All the words in wordList are unique."
  },
  {
    "objectID": "programming/leetcode-word-ladder/index.html#the-pitfalls-of-naïve-backtracking",
    "href": "programming/leetcode-word-ladder/index.html#the-pitfalls-of-naïve-backtracking",
    "title": "127. Word Ladder",
    "section": "2. The Pitfalls of Naïve Backtracking",
    "text": "2. The Pitfalls of Naïve Backtracking\nIt’s tempting to reach for a simple recursive strategy:\ndef ladderLength_naive(begin, end, word_dict):\n    def backtrack(current, remaining, depth):\n        if current == end:\n            return depth\n        best = float('inf')\n        for word in list(remaining):\n            # only recurse on one-letter neighbors\n            if sum(a != b for a,b in zip(current, word)) == 1:\n                remaining.remove(word)\n                cand = backtrack(word, remaining, depth + 1)\n                if cand:\n                    best = min(best, cand)\n                remaining.add(word)\n        return best if best != float('inf') else 0\n\n    return backtrack(begin, set(word_dict), 1)\nWhy it stumbles 1. Exponential Recursion Every choice spawns a new branch, and many branches share subproblems—leading to redundant work.\n\nNo Early Stopping You might explore a deep chain long after a shorter route exists elsewhere.\nBacktracking Overhead Adding/removing words from the working set at each step is costly and easy to get wrong.\n\nIn practice, this approach chokes on even modestly sized dictionaries."
  },
  {
    "objectID": "programming/finetunning/index.html",
    "href": "programming/finetunning/index.html",
    "title": "Finetuning",
    "section": "",
    "text": "Finetuning is a process of adapting a model to a specific task by further training the whole model or part of the model. It is one of the three very common AI engineering techniques used to adapt a model to specific needs, alongside prompt engineering and Retrieval-Augmented Generation (RAG). While prompt-based methods like prompt engineering and RAG influence a model’s quality solely through inputs without modifying the model itself, finetuning adapts a model by adjusting its weights. Finetuning techniques are generally more complicated and require more data compared to prompt-based methods. However, they can significantly improve a model’s quality, latency, and cost. Adapting a model by changing its weights makes things possible that aren’t otherwise, such as adapting it to a new task it wasn’t exposed to during initial training.\nFinetuning is considered part of a model’s training process, specifically an extension of model pre-training. Training that happens after pre-training is referred to as finetuning, and it can take various forms. Chapter 2 discusses two types of finetuning: supervised finetuning and preference finetuning.\nThe goal of finetuning is to get a base model, which has some but not all of the necessary capabilities, to perform well enough for a specific task. Finetuning improves sample efficiency, meaning a model can learn the desired behavior with fewer examples than training from scratch. For instance, while training a model for legal question answering from scratch might require millions of examples, finetuning a good base model might only require a few hundred. Finetuning can enhance various aspects of a model, including its domain-specific capabilities (like coding or medical question answering) and safety, but it is most often used to improve the model’s instruction-following ability, especially to adhere to specific output styles and formats.\n\n1. When to Finetune:\n\nEnhancing domain-specific capabilities: If a model struggles with a specific domain (e.g., a less common SQL dialect or customer-specific queries), finetuning on relevant data can help.\nImproving instruction following and structured outputs: Finetuning is the most effective and general approach to get models to generate outputs in a desired format. While prompting is less reliable, finetuning a model on examples following the desired format is much more reliable. For certain tasks like classification, modifying the model’s architecture before finetuning by adding a classifier head can guarantee the output format.\nBias mitigation: Finetuning with carefully curated data can counteract biases present in the base model’s training data. For example, finetuning on data with female CEOs or texts authored by women/African authors can reduce gender and racial biases.\nDistillation: Finetuning a smaller model to imitate the behavior of a larger model using data generated by the larger model is a common approach called distillation. This makes the smaller model cheaper and faster to use in production.\nOptimizing token usage (historically): Before prompt caching, finetuning could help optimize token usage by training the model on examples instead of including them in every prompt, resulting in shorter, cheaper, and lower-latency prompts. Although prompt caching has reduced this benefit, finetuning still removes the limitation of context length on the number of examples used.\nExtending context length: Long-context finetuning requires modifying the model’s architecture and can increase the maximum context length, though it is harder to do and the resulting model might degrade on shorter sequences.\n\n\n\n2. Reasons Not to Finetune:\n\nPerformance degradation on other tasks: Finetuning for a specific task can sometimes degrade performance on other tasks.\nHigh up-front investment and continual maintenance: Finetuning requires significant resources, including acquiring high-quality annotated data (which can be slow and expensive) and ML knowledge to evaluate base models, monitor training, and debug.\nServing complexity: Once finetuned, serving the model requires figuring out hosting (in-house or API) and inference optimization, which is non-trivial for large models.\nPace of base model improvement: New base models are constantly being developed and may improve faster than a finetuned model can be updated.\nPrompting might be sufficient: Many practitioners find that after complaints about prompting’s ineffectiveness, refining the prompt experiment process shows that prompting alone can be sufficient."
  },
  {
    "objectID": "programming/program-parity-invariance/parity-invariance.html",
    "href": "programming/program-parity-invariance/parity-invariance.html",
    "title": "Parity invariance",
    "section": "",
    "text": "Parity invariance means that the number of times a position is flipped determines its final value. If a position is flipped an odd number of times, its value changes, but if it is flipped an even number of times, it stays the same.\nConsider the array [1, 0, 0, 1, 0, 1, 1]. We start by flipping three consecutive elements to try and transform all 0s into 1s. First, flipping the subarray [0, 0, 1] at indices 1..3 changes the array to [1, 1, 1, 0, 0, 1, 1]. Then, flipping [0, 0, 1] at indices [3..5] gives [1, 1, 1, 1, 1, 0, 1]. Finally, flipping [1, 0, 1] at indices [1, 0, 1] at indices 4..6 results in [1, 1, 1, 1, 1, 1, 0, 1, 0].\nAt this point, we see that the 0s at positions 4 and 6 remain, and there is no way to flip them without also flipping the other elements. Since we can only flip three elements at a time, we cannot isolate these 0s in a way that allow us to change them to 1s. This happens because these positions were flipped an even number of times, so they retain their original value. Because of this parity constraint, the transformation is impossible."
  },
  {
    "objectID": "programming/leetcodeminimum-operations-to-make-binary-array-elements-equal-to-one-i/3191.html",
    "href": "programming/leetcodeminimum-operations-to-make-binary-array-elements-equal-to-one-i/3191.html",
    "title": "3191. Minimum Operations to Make Binary Array Elements Equal to One I",
    "section": "",
    "text": "You are given a\nnums.\nYou can do the following operation on the array any number of times (possibly zero):\nChoose any 3 consecutive elements from the array and flip all of them.\nFlipping an element means changing its value from 0 to 1, and from 1 to 0.\nReturn the minimum number of operations required to make all elements in nums equal to 1. If it is impossible, return -1.\nExample 1:\nInput: nums = [0,1,1,1,0,0]\nOutput: 3\nExplanation: We can do the following operations:\nChoose the elements at indices 0, 1 and 2. The resulting array is nums = [1,0,0,1,0,0].\nChoose the elements at indices 1, 2 and 3. The resulting array is nums = [1,1,1,0,0,0].\nChoose the elements at indices 3, 4 and 5. The resulting array is nums = [1,1,1,1,1,1].\nExample 2:\nInput: nums = [0,1,1,1]\nOutput: -1\nExplanation: It is impossible to make all elements equal to 1.\nConstraints:\n3 &lt;= nums.length &lt;= 105\n0 &lt;= nums[i] &lt;= 1\n\nAnalysis\nSince we can only flip three consecutive elements at a time, isolated 0s or certain patterns of 0s may prevent us from turning everything into 1. If the number of 0s in certain positions makes it impossible to fully eliminate them using groups of three, the transformation cannot be achieved."
  },
  {
    "objectID": "programming/leetcode-maximum-subarray/index.html",
    "href": "programming/leetcode-maximum-subarray/index.html",
    "title": "Maximum subarray",
    "section": "",
    "text": "Given an integer array nums, find the subarray with the largest sum, and return its sum.\nExample 1:\nInput: nums = [-2,1,-3,4,-1,2,1,-5,4] Output: 6 Explanation: The subarray [4,-1,2,1] has the largest sum 6. Example 2:\nInput: nums = [1] Output: 1 Explanation: The subarray [1] has the largest sum 1. Example 3:\nInput: nums = [5,4,-1,7,8] Output: 23 Explanation: The subarray [5,4,-1,7,8] has the largest sum 23.\nConstraints:\n1 &lt;= nums.length &lt;= 105 -104 &lt;= nums[i] &lt;= 104\nFollow up: If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle."
  },
  {
    "objectID": "programming/leetcode-maximum-subarray/index.html#brute-force",
    "href": "programming/leetcode-maximum-subarray/index.html#brute-force",
    "title": "Maximum subarray",
    "section": "Brute force",
    "text": "Brute force\n    # brute force approach\n\n    # find all left, and right \n    # cal sum of each\n    # return max\n    # T.C: O(n^2)\n\nfrom typing import List\ndef maxSubArray(self, nums: List[int]) -&gt; int:\n    left, ans = 0, nums[0]\n    for left in range(len(nums)):\n        cur = 0\n        for right in range(left, len(nums)):\n            cur += nums[right]\n            ans = max(ans, cur)\n\n    return ans\n\nit will TLE."
  },
  {
    "objectID": "programming/leetcode-longest-nice-subarray/leetcode-Longest-Nice-Subarray.html",
    "href": "programming/leetcode-longest-nice-subarray/leetcode-Longest-Nice-Subarray.html",
    "title": "leetcode-Longest-Nice-Subarray",
    "section": "",
    "text": "Topic: array, bit manipulation, slide windown\n\nQuestion\nYou are given an array nums consisting of positive integers.\nWe call a subarray of nums nice if the bitwise AND of every pair of elements that are in different positions in the subarray is equal to 0.\nReturn the length of the longest nice subarray.\nA subarray is a contiguous part of an array.\nNote that subarrays of length 1 are always considered nice.\nExample 1:\nInput: nums = [1,3,8,48,10] Output: 3 Explanation: The longest nice subarray is [3,8,48]. This subarray satisfies the conditions: - 3 AND 8 = 0. - 3 AND 48 = 0. - 8 AND 48 = 0. It can be proven that no longer nice subarray can be obtained, so we return 3.\nExample 2:\nInput: nums = [3,1,5,11,13] Output: 1 Explanation: The length of the longest nice subarray is 1. Any subarray of length 1 can be chosen.\nConstraints:\n1 &lt;= nums.length &lt;= 105\n1 &lt;= nums[i] &lt;= 109\n\n\nAnalysis\n    example 1:\n    [1, 3, 8, 48, 10]\n    3 in bin: 0011\n    8 in bin: 1000\n    48d = 1100000\n    10d = 1010\n    0011 AND 1000 = 0\n    3, 8, 48 AND = 0\n    10 and 8 not = 0\n    so, if pair AND = 0\n    that pair have no common bit\n    so, we store a bit array to check the state of bit\n    and if, to better, we just need to store number of bit in that array instead.\n    oh no, it wrong.\n    so if we must store a array.\n    no, we can use bitwise operator & to check if a AND b == 0 or not\n    and OR for cumulative bit\n    x = 5\n    # 101\n    x |= 3\n    # 3 == 011\n    # 101 |= 011 = 111\n    print(x)\n    # 7\n    to search for longest (can use i, and j) for check all the begin and end\n    improve it by two pointer to decrease TC from O(n^2) to O(n)\n    and now, how to get rid of num of left from cumulative bit in slide windown\n    check that case: [011, 100]\n    now, culmulative bit: 111\n    we want it after left += 1, is 100\n    in XOR: 111 XOR 011 == 100\n    XOR parameter in python is ^=\n\n\nCode\n    def longestNiceSubarray(self, nums: List[int]) -&gt; int:\n\n        cumulative_bit = 0\n\n        ans = 0\n\n        left = 0\n\n        for right in range(len(nums)):\n\n            # when AND not ease\n\n            while cumulative_bit & nums[right] != 0: # right can not cumulative, increase left until it can ease, use XOR for get rid of it\n\n                cumulative_bit ^= nums[left]\n\n                left += 1\n\n            # until can AND\n\n            # use OR to cumulative it\n\n            cumulative_bit |= nums[right]\n\n            ans = max(ans, right - left + 1)\n\n        return ans"
  },
  {
    "objectID": "programming/docker-intro/index.html",
    "href": "programming/docker-intro/index.html",
    "title": "Docker basics",
    "section": "",
    "text": "This chapter introduces containerization with Docker and relevant concepts such as image and volume. By the end of this chapter you are able to:"
  },
  {
    "objectID": "programming/docker-intro/index.html#what-is-devops",
    "href": "programming/docker-intro/index.html#what-is-devops",
    "title": "Docker basics",
    "section": "What is DevOps",
    "text": "What is DevOps\nThe term itself consists of two parts Dev and Ops. Dev refers to the development of software and Ops to operations. Simple definition for DevOps would be that it means the release, configuring, and monitoring of software is in the hand people who develop it."
  },
  {
    "objectID": "programming/docker-intro/index.html#what-is-docker",
    "href": "programming/docker-intro/index.html#what-is-docker",
    "title": "Docker basics",
    "section": "What is Docker",
    "text": "What is Docker\n\n“Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers.” - from Wikipedia(opens in a new tab).\n\nSo we get two definitions: 1. Docker is a set of tools to deliver software incontainers. 2. Containers are packages of software. \nThe above image illustrate how containers include the application and its dependencies. These containers are isolated so that they don’t interfere with each other or the software running outside of the containers. If we want to enable interactions between them, Docker offers tools to do so."
  },
  {
    "objectID": "programming/docker-intro/index.html#benefits-from-containers",
    "href": "programming/docker-intro/index.html#benefits-from-containers",
    "title": "Docker basics",
    "section": "Benefits from containers",
    "text": "Benefits from containers\nContainers package applications. In different scenarios: ### Scenario 1: Works on my machine\nLet’s first take a closer look into what happens in web development without containers following the chain above starting from “Plan”.\nFirst we plan an application. Then our team of 1-n developers create the software. It works on one’s computer. It may even go through a testing pipeline working perfectly. We send it to the server and it does not work.\nThis is known as the “works on my machine” problem. The only way to solve this is by finding out what in tarnation the developer had installed on their machine that made the application work.\nContainers solve this problem by allowing the developer to personally run the application inside a container, which then includes all of the dependencies required for the app to work.\n\nYou may still occasionally hear about “works in my container” issues - these are often just usage errors. ### Scenario 2: Isolated environments\n\nWe have 5 different Python applications. We need to deploy them to a server that already has an application requiring Python 2.7 and of course none of our applications are 2.7. What do we do now?\nSince containers package the software with all of its dependencies, we package the existing app and all 5 new ones with their respective Python versions and that’s it.\nSometimes different parts of a system may change over time, possibly leading to the application not working. These changes may be anything from an operating system update to changes in dependencies.\n\nScenario 3: Development\nWe are brought into a dev team. They run a web app that uses other services when running: a Postgres database, MongoDB, Redis and a number of others. Simple enough, we install whatever is required to run the application and all of the applications that it depends on …\nWhat a headache to start installing and then managing the development databases on your own machine.\nThankfully, by the time we are told to do that we are already a Docker expert. With one command we get an isolated application, like Postgres or Mongo, running in our machine.\n\n\nScenario 4: Scaling\nStarting and stopping a Docker container has little overhead. But when we run our own Netflix or Facebook, we want to meet the changing demand. With some advanced tooling that we will learn about in parts 2 and 3, we can spin up multiple containers instantly and load balance traffic between them.\nContainer orchestration will be discussed in parts 2 and 3. But the simplest example: what happens when one application dies? The orchestration system notices it, splits traffic between the working replicas, and spins up a new container to replace the dead one."
  },
  {
    "objectID": "programming/docker-intro/index.html#virtual-machines",
    "href": "programming/docker-intro/index.html#virtual-machines",
    "title": "Docker basics",
    "section": "Virtual machines",
    "text": "Virtual machines\n\nVM virtualizes the physical hardware. Each VM includes a full OS along with the necessary binaries and libraries, making them heavier and more resources-intensive. Containers, on the other hand, share the host OS kernel and only package the application and its dependencies, resulting in a more lightweight and efficient solution.\nVMs provide strong isolation and are suited for running multiple OS environments, but they have a performance overhead and longer startup times. Containers offer faster startup, better resource utilization, and high portability across different environments, though their isolation is at the process level, which may not be as robust as that of VMs. Overall, VMs could be used for scenarios needing complete OS environments, while containers excel in lightweight, efficient, and consistent application deployment.\nDocker relies on Linux kernels, which means that macOS and Windows cannot run Docker natively without some additional steps. Each operating system has its own solution for running docker."
  },
  {
    "objectID": "programming/docker-intro/index.html#image-and-containers",
    "href": "programming/docker-intro/index.html#image-and-containers",
    "title": "Docker basics",
    "section": "Image and containers",
    "text": "Image and containers\nSince we already know what containers are it’s easier to explain images through them: Containers are instances of images. A basic mistake is to confuse images and containers.\nCooking metaphor:\nThink of a container as a ready-to-eat meal that we can simply heat up and consume. An image, on the other hand, is the recipe and the ingredients for that meal.\nSo just like how we need a recipe and ingredients to make a meal, we need an image and a container runtime (Docker engine) to create a container. The image provides all the necessary instructions and dependencies for the container to run, just like a recipe provides the steps and ingredients to make a meal.\nIn short, an image is like a blueprint or template and the building material, while a container is an instance of that blueprint or template.\n\nImage\nA Docker image is a file. An image never changes; we cannot edit an existing file. Creating a new image happens by starting from a base image and adding new layers to it. We will talk about layers later, but we should think of images as immutable, they can not be changed after they are created.\nList all of ours image with docker image ls\n$ docker image ls\n  REPOSITORY      TAG      IMAGE ID       CREATED         SIZE\n  hello-world     latest   d1165f221234   9 days ago      13.3kB\nContainers are created from images, so when we ran hello-world twice we downloaded one image and created two of them from the single image.\nWell then, if images are used to create containers, where do images come from? This image file is built from an instructional file named Dockerfile that is parsed when we run docker image build.\nDockerfile is a file that is by default called Dockerfile, that looks something like this\nFROM &lt;image&gt;:&lt;tag&gt;\n\nRUN &lt;install some dependencies&gt;\n\nCMD &lt;command that is executed on `docker container run`&gt;\nand is the instruction set for building an image. We will look into Dockerfiles later when we to build our own images.\nIf we go back to the cooking metaphor, as Dockerfile provides the instructions needed to build an image we can think of that as the recipe for images. We’re now 2 recipes deep, as a Dockerfile is the recipe for an image and an image is the recipe for the container. The only difference is that Dockerfile is written by us, whereas image is written by our machine based on the Dockerfile."
  },
  {
    "objectID": "programming/docker-intro/index.html#container",
    "href": "programming/docker-intro/index.html#container",
    "title": "Docker basics",
    "section": "Container",
    "text": "Container\nContainers only contain what is required to execute an application; and we can start, stop and interact with them. They are isolated environments in the host machine with the ability to interact with each other and the host machine itself via defined method (TCP/UDP).\nList all of our containers with docker container ls\n$ docker container ls\n  CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\nWithout -a flag it will only print running containers.\n$ docker container ls -a\n  CONTAINER ID   IMAGE           COMMAND      CREATED          STATUS                      PORTS     NAMES\n  b7a53260b513   hello-world     \"/hello\"     5 minutes ago    Exited (0) 5 minutes ago              brave_bhabha\n  1cd4cb01482d   hello-world     \"/hello\"     8 minutes ago    Exited (0) 8 minutes ago              vibrant_bell"
  },
  {
    "objectID": "Lecture 8.html",
    "href": "Lecture 8.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "Anh -&gt; cut vat the ( vd xe) -&gt; trich xuat vector dac trung -&gt; save db\nCó nhận xét kết quả của tập dữ liệu sinh ra ở mỗi kết quả dòng lênh.\nNhận xédt trong markdown"
  },
  {
    "objectID": "posts/nextcloud-install/index.html",
    "href": "posts/nextcloud-install/index.html",
    "title": "Install Nextcloud in Ubuntu Server",
    "section": "",
    "text": "https://ubuntu.com/download/server"
  },
  {
    "objectID": "posts/nextcloud-install/index.html#linuxserver.io-nextcloud-doesdoes-not-useuse-postgresql-environmentenvironment-variablesvariables-to-connect-to-the-databasedatabase.",
    "href": "posts/nextcloud-install/index.html#linuxserver.io-nextcloud-doesdoes-not-useuse-postgresql-environmentenvironment-variablesvariables-to-connect-to-the-databasedatabase.",
    "title": "Install Nextcloud in Ubuntu Server",
    "section": "❗ LinuxServer.io Nextcloud doesdoes not useuse PostgreSQL environmentenvironment variablesvariables to connect to the databasedatabase.",
    "text": "❗ LinuxServer.io Nextcloud doesdoes not useuse PostgreSQL environmentenvironment variablesvariables to connect to the databasedatabase.\nThat’s the issueissue rightright therethere.\n\nUnlikeUnlike the officialofficial nextcloud imageimage, linuxserver/nextcloud doesdoes not read POSTGRES_USER, POSTGRES_PASSWORD, etc. from the environmentenvironment.\n\nInsteadInstead, it expects youyou to configureconfigure the databasedatabase entirelyentirely throughthrough the web UI.\n\nservices:\n  db:\n    image: postgres:16\n    container_name: nextcloud-postgres\n    environment:\n      POSTGRES_DB: nextclouddb\n      POSTGRES_USER: nextcloud\n      POSTGRES_PASSWORD: MyStrongPassword123!\n    volumes:\n      - /opt/nextcloud/postgres:/var/lib/postgresql/data\n    restart: unless-stopped\n\n  nextcloud:\n    image: lscr.io/linuxserver/nextcloud:latest\n    container_name: nextcloud\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Etc/UTC\n    volumes:\n      - /opt/nextcloud/config:/config\n      - /opt/nextcloud/data:/data\n    ports:\n      - 443:443\n    depends_on:\n      - db\n    restart: unless-stopped"
  },
  {
    "objectID": "posts/nextcloud-install/index.html#errors",
    "href": "posts/nextcloud-install/index.html#errors",
    "title": "Install Nextcloud in Ubuntu Server",
    "section": "Errors:",
    "text": "Errors:\nBad gateway:  Because your Portainer HTTPS endpoint is using a self-signed certificate, Cloudflared (by default) will try to validate it—and fail—so you get a 502 even though curl -vk https://127.0.0.1:9443 works locally.\nHere’s how to fix that when you’re using the Web Dashboard to manage your tunnel:\n\nEnable “Disable TLS Verification” on the Hostname In the Cloudflare dashboard go to Zero Trust → Access → Tunnels.\n\nClick your tunnel, then open the Public hostnames tab.\nFind the row for portainer.kimhungbui.com and click Edit.\nUnder Origin Connection you should see a toggle or checkbox labeled something like “Disable TLS verification” (or “Skip certificate validation”).\nEnable that option and save.\nThis tells Cloudflared not to reject your Portainer’s self-signed cert.\n\n\nIt work:"
  },
  {
    "objectID": "mathematic.html",
    "href": "mathematic.html",
    "title": "Mathematic",
    "section": "",
    "text": "Development Indicators by Continent\n\n\n\n\n\n\n\n\n\n\n\nGapminder Analytics Group\n\n\n\n\n\n\n\n\n\n\n\n\nConvex set\n\n\n\n\n\n\nmath\n\n\n\nYour post description\n\n\n\n\n\nJun 9, 2025\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\nConvex optimization\n\n\n\n\n\n\nmath\n\n\n\nYour post description\n\n\n\n\n\nJun 9, 2025\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference and Learning\n\n\n\n\n\n\nStatistical\n\n\n\nYour post description\n\n\n\n\n\nFeb 26, 2025\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Randome Variable\n\n\n\n\n\n\nmath\n\n\nstatistic\n\n\n\nBernoulli Randome Variable\n\n\n\n\n\nSep 11, 2024\n\n\nKim Hung Bui\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we have normal distribution formular\n\n\n\n\n\n\nmath\n\n\nstatistic\n\n\n\nYour post description\n\n\n\n\n\nSep 11, 2024\n\n\nYour name\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Family Man Retires at 39 – Extreme Early Retirement | FIRE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevelopment Indicators by Continent\n\n\n\n\n\n\n\n\n\n\n\nGapminder Analytics Group\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Nextcloud in Ubuntu Server\n\n\n\n\n\n\nStatistical\n\n\n\nYour post description\n\n\n\n\n\nFeb 26, 2025\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\nPost template\n\n\n\n\n\n\ntemplate\n\n\nany-category-you-want\n\n\n\nYour post description\n\n\n\n\n\nSep 11, 2024\n\n\nYour name\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "english.html",
    "href": "english.html",
    "title": "English",
    "section": "",
    "text": "IELTS essay template\n\n\n\n\n\n\nenglish\n\n\n\nEssay template for IELTS\n\n\n\n\n\nJun 16, 2025\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\nAgriculture\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ncause and solution\n\n\n\nYour post description\n\n\n\n\n\nJun 11, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Cambridge 16 Essay - Driverless Vehicles\n\n\n\n\n\n\nenglish\n\n\nadvantage disadvantage\n\n\n\nYour post description\n\n\n\n\n\nJun 3, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Essay- Line Chart Social Center\n\n\n\n\n\n\nenglish\n\n\ntime\n\n\n\nYour post description\n\n\n\n\n\nJun 2, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nTask 1 Recycle paper\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\nprocess\n\n\n\nYour post description\n\n\n\n\n\nApr 19, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nTransitive Verb an Intransitive Verb\n\n\n\n\n\n\nenglish\n\n\ngrammar\n\n\n\nYour post description\n\n\n\n\n\nApr 16, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nFarm animals and Vegetables\n\n\n\n\n\n\nenglish\n\n\nadvantage disadvantage\n\n\n\nYour post description\n\n\n\n\n\nApr 8, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIelts task 1 - Household work\n\n\n\n\n\n\nenglish\n\n\nadvantage disadvantage\n\n\n\nYour post description\n\n\n\n\n\nApr 6, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Structure a ‘Two Question’ Essay\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nHow to Structure a ‘Two Question’ Essay\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Essay Having Children at an Older Age\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nIELTS Essay Having Children at an Older Age\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Essay Financial Matters\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nIELTS Essay Financial Matters\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Essay Writing by Hand\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nIELTS Essay Writing by Hand\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIelts task 1 - Household work\n\n\n\n\n\n\nenglish\n\n\ntime\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nSEE technique in speaking\n\n\n\n\n\n\nenglish\n\n\nspeaking\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nZoos\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\nadvantages disadvantage\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nGrammar Present time\n\n\n\n\n\n\nenglish\n\n\ngrammar\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Services\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\nadvantages disadvantage\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\n5 basic structures of sentences in english\n\n\n\n\n\n\nenglish\n\n\ngrammar\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nFame\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\nadvantages disadvantage\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance-Enhancing Drugs in Sports\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post_template/index.html",
    "href": "posts/post_template/index.html",
    "title": "Post template",
    "section": "",
    "text": "Just write more markdown here!"
  },
  {
    "objectID": "posts/post_template/index.html#this-is-a-subsection",
    "href": "posts/post_template/index.html#this-is-a-subsection",
    "title": "Post template",
    "section": "This is a subsection",
    "text": "This is a subsection\nSubsection text! You get it now I assume\nThis is how you reference an image in your blog post\n\n\n\nThis is just a photo I took last time I was in Dundee.\n\n\nThat’s it! go for it!"
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html",
    "href": "posts/extreme-early-retirement/index.html",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Tim, a man from Regina, Saskatchewan, Canada, retired from engineering at the age of 39, after 11 years of planning and working towards his goal of early retirement, and his wife plans to retire from her childcare career in a few years (00:00:07).\nTim’s family achieved early retirement through a financial strategy known as FIRE, which involves drastically reducing spending, saving a large percentage of income, and investing the savings (00:00:20).\nTim first considered the idea of early retirement in 2006 and started saving money, cutting costs, and figuring out what matters to him and his family (00:00:53).\nWhen Tim and his wife started working towards early retirement, they had student loan debt and other financial issues, but they paid off their existing debt and then focused on reducing their spending (00:01:12).\n\n\n\n\n\nTo reduce their spending, Tim and his wife looked at their bills, such as their power bill, and found ways to lower them, such as changing to LED lightbulbs (00:01:48).\nThey also reduced their grocery bill by buying generic versions of products and making things from scratch, which often tasted better and saved them money (00:02:02).\nTim and his wife have a focused approach to spending, only eating out a couple of times a year, and making sure their spending is aligned with what matters to them (00:02:31).\nTim uses a phrase to help him avoid overspending, which is to ask himself if he really needs something and if he can solve a problem without spending money (00:02:53).\nTim and his wife have paid off their mortgage and car loans in advance, which has helped them in their retirement (00:03:23).\n\n\n\n\n\nThe individual has achieved extreme early retirement at the age of 39, with a significant amount of assets and no debt, and currently resides in Regina, Saskatchewan, Canada, where housing costs are relatively low, with an average housing cost of around $278,000 (00:03:50).\nThe majority of their investments are in straightforward strategies, including exchange-traded funds that mirror major indexes, allowing for a simple and low-maintenance approach to managing their finances (00:04:01).\nDespite the simplicity of their investment strategy, they still experience worries about the stock market, but have trained themselves not to check it frequently, focusing instead on long-term performance rather than short-term fluctuations (00:04:11).\nInitially, they found the wild gyrations of the stock market to be psychologically challenging, and having a larger cushion would have been helpful for peace of mind (00:04:27).\n\n\n\n\n\nOne of the downsides of early retirement is the unusual conversations with people who question their decision to retire at a young age, leading them to reframe their explanation to focus on pursuing a writing career (00:04:44).\nThey started a blog in 2006 to document their early retirement journey, which includes their doubts, learning experiences, and mistakes, and also wrote a book called “Three at Forty Five” to discuss the importance of happiness in early retirement (00:04:57).\nThe first year of early retirement was problematic, involving a major lifestyle change and a process of deprogramming themselves to realize they didn’t have to work as much, and instead focus on enjoyable activities (00:05:29).\nTo cope with the transition, they created to-do lists that included fun items, and have pursued various hobbies such as making their own wine and beer, playing Dungeons & Dragons, and creating terrain pieces (00:05:52).\n\n\n\n\n\nThey have been married for 19 years and have two sons, aged 14 and 11, and have taught them about money management, focusing on delayed gratification, saving, and responsible spending (00:06:23).\nThey gave their children an allowance from a young age and allowed them to spend it on whatever they wanted, teaching them valuable lessons about money management (00:06:47).\nA strategy was implemented to teach children the value of saving money by explaining that waiting to buy something now can lead to affording bigger things later, resulting in two kids who default to saving money without knowing what they’re saving for (00:07:17).\nA friend’s advice was taken to heart, which stated that kids are a constant drain on finances, but the expenses change as they age, and it’s essential to get used to it (00:07:43).\nA system was set up to take out cash every month for the kids and put it aside, which averaged out to the same amount of money over the course of the year, with the wife managing the cash flow to prepare for upcoming expenses like winter boots (00:07:49).\nRegistered education savings plans were saved for the kids’ college and university education, which was fully filled before leaving work, allowing the money to grow in interest (00:08:18).\n\n\n\n\n\nThe wife decided to continue working, which was factored into the plan, assuming she would work for a few more years to offset some of the spending and provide an extra cushion (00:08:50).\nA plan was written out in advance to prepare for potential stock market declines, including options like taking a loan or getting a part-time job, to help stay calm and rational during emotional times (00:09:28).\nHaving a written plan helped to prepare for a major stock market decline a year into retirement, which ultimately worked out well in the long run (00:09:08).\nIt’s recommended to write out an investment plan while calm and rational to prepare for potential future declines and have a clear plan of action (00:09:37).\n\n\n\n\n\nThe family’s current home is a daycare, which is quite spacious, and they plan to downsize in the long term, with any savings from lower taxes, water bills, and other expenses being used to offset potential losses (00:10:14).\nLiving in Canada provides benefits such as government-paid healthcare costs, which have been over-contributed to, covering basic healthcare costs, but not dental, prescription eyewear, or other expenses that are self-insured and paid out-of-pocket (00:10:40).\nGovernment benefits like the Canada Pension Plan and Old Age Security are factored into long-term plans, but with conservative assumptions, such as only one spouse collecting Old Age Security (00:11:05).\n\n\n\n\n\nA job at the local library was taken to work with great people, enjoy the work, and earn a supplemental income, which is used to fund extra activities, and has helped achieve a better work-life balance (00:11:30).\nReduced working hours have improved overall happiness, and retirement has provided extra time to focus on desired activities and spend quality time with family, particularly children (00:12:10).\n\n\n\n\n\nResources on early retirement, including Tim’s blog and book, are available in the video description for those interested (00:12:43)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#early-retirement-journey-and-financial-strategies",
    "href": "posts/extreme-early-retirement/index.html#early-retirement-journey-and-financial-strategies",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Tim, a man from Regina, Saskatchewan, Canada, retired from engineering at the age of 39, after 11 years of planning and working towards his goal of early retirement, and his wife plans to retire from her childcare career in a few years (00:00:07).\nTim’s family achieved early retirement through a financial strategy known as FIRE, which involves drastically reducing spending, saving a large percentage of income, and investing the savings (00:00:20).\nTim first considered the idea of early retirement in 2006 and started saving money, cutting costs, and figuring out what matters to him and his family (00:00:53).\nWhen Tim and his wife started working towards early retirement, they had student loan debt and other financial issues, but they paid off their existing debt and then focused on reducing their spending (00:01:12)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#spending-reduction-and-financial-management",
    "href": "posts/extreme-early-retirement/index.html#spending-reduction-and-financial-management",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "To reduce their spending, Tim and his wife looked at their bills, such as their power bill, and found ways to lower them, such as changing to LED lightbulbs (00:01:48).\nThey also reduced their grocery bill by buying generic versions of products and making things from scratch, which often tasted better and saved them money (00:02:02).\nTim and his wife have a focused approach to spending, only eating out a couple of times a year, and making sure their spending is aligned with what matters to them (00:02:31).\nTim uses a phrase to help him avoid overspending, which is to ask himself if he really needs something and if he can solve a problem without spending money (00:02:53).\nTim and his wife have paid off their mortgage and car loans in advance, which has helped them in their retirement (00:03:23)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#investment-strategy-and-market-volatility",
    "href": "posts/extreme-early-retirement/index.html#investment-strategy-and-market-volatility",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The individual has achieved extreme early retirement at the age of 39, with a significant amount of assets and no debt, and currently resides in Regina, Saskatchewan, Canada, where housing costs are relatively low, with an average housing cost of around $278,000 (00:03:50).\nThe majority of their investments are in straightforward strategies, including exchange-traded funds that mirror major indexes, allowing for a simple and low-maintenance approach to managing their finances (00:04:01).\nDespite the simplicity of their investment strategy, they still experience worries about the stock market, but have trained themselves not to check it frequently, focusing instead on long-term performance rather than short-term fluctuations (00:04:11).\nInitially, they found the wild gyrations of the stock market to be psychologically challenging, and having a larger cushion would have been helpful for peace of mind (00:04:27)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#challenges-and-adaptation-in-early-retirement",
    "href": "posts/extreme-early-retirement/index.html#challenges-and-adaptation-in-early-retirement",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "One of the downsides of early retirement is the unusual conversations with people who question their decision to retire at a young age, leading them to reframe their explanation to focus on pursuing a writing career (00:04:44).\nThey started a blog in 2006 to document their early retirement journey, which includes their doubts, learning experiences, and mistakes, and also wrote a book called “Three at Forty Five” to discuss the importance of happiness in early retirement (00:04:57).\nThe first year of early retirement was problematic, involving a major lifestyle change and a process of deprogramming themselves to realize they didn’t have to work as much, and instead focus on enjoyable activities (00:05:29).\nTo cope with the transition, they created to-do lists that included fun items, and have pursued various hobbies such as making their own wine and beer, playing Dungeons & Dragons, and creating terrain pieces (00:05:52)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#family-life-and-financial-education-for-children",
    "href": "posts/extreme-early-retirement/index.html#family-life-and-financial-education-for-children",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "They have been married for 19 years and have two sons, aged 14 and 11, and have taught them about money management, focusing on delayed gratification, saving, and responsible spending (00:06:23).\nThey gave their children an allowance from a young age and allowed them to spend it on whatever they wanted, teaching them valuable lessons about money management (00:06:47).\nA strategy was implemented to teach children the value of saving money by explaining that waiting to buy something now can lead to affording bigger things later, resulting in two kids who default to saving money without knowing what they’re saving for (00:07:17).\nA friend’s advice was taken to heart, which stated that kids are a constant drain on finances, but the expenses change as they age, and it’s essential to get used to it (00:07:43).\nA system was set up to take out cash every month for the kids and put it aside, which averaged out to the same amount of money over the course of the year, with the wife managing the cash flow to prepare for upcoming expenses like winter boots (00:07:49).\nRegistered education savings plans were saved for the kids’ college and university education, which was fully filled before leaving work, allowing the money to grow in interest (00:08:18)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#wifes-continued-work-and-financial-planning",
    "href": "posts/extreme-early-retirement/index.html#wifes-continued-work-and-financial-planning",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The wife decided to continue working, which was factored into the plan, assuming she would work for a few more years to offset some of the spending and provide an extra cushion (00:08:50).\nA plan was written out in advance to prepare for potential stock market declines, including options like taking a loan or getting a part-time job, to help stay calm and rational during emotional times (00:09:28).\nHaving a written plan helped to prepare for a major stock market decline a year into retirement, which ultimately worked out well in the long run (00:09:08).\nIt’s recommended to write out an investment plan while calm and rational to prepare for potential future declines and have a clear plan of action (00:09:37)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#housing-plans-and-government-benefits",
    "href": "posts/extreme-early-retirement/index.html#housing-plans-and-government-benefits",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The family’s current home is a daycare, which is quite spacious, and they plan to downsize in the long term, with any savings from lower taxes, water bills, and other expenses being used to offset potential losses (00:10:14).\nLiving in Canada provides benefits such as government-paid healthcare costs, which have been over-contributed to, covering basic healthcare costs, but not dental, prescription eyewear, or other expenses that are self-insured and paid out-of-pocket (00:10:40).\nGovernment benefits like the Canada Pension Plan and Old Age Security are factored into long-term plans, but with conservative assumptions, such as only one spouse collecting Old Age Security (00:11:05)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#part-time-work-and-enhanced-work-life-balance",
    "href": "posts/extreme-early-retirement/index.html#part-time-work-and-enhanced-work-life-balance",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "A job at the local library was taken to work with great people, enjoy the work, and earn a supplemental income, which is used to fund extra activities, and has helped achieve a better work-life balance (00:11:30).\nReduced working hours have improved overall happiness, and retirement has provided extra time to focus on desired activities and spend quality time with family, particularly children (00:12:10)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#resources-and-further-information",
    "href": "posts/extreme-early-retirement/index.html#resources-and-further-information",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Resources on early retirement, including Tim’s blog and book, are available in the video description for those interested (00:12:43)."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Write more markdown here!\nI’am Hung"
  },
  {
    "objectID": "programming/convolution/index.html",
    "href": "programming/convolution/index.html",
    "title": "Image classification",
    "section": "",
    "text": "This is the task of assigning an input image one label from a fixed categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, many other seemingly distinct CV tasks (such as object detection, segmentation) can be reduced to image classification.\nExample: In the image blow in image classification model takes a single image and assigns probabilities to 4 label {cat, dog, hat, mug}. Computer view a image as an one large 3-dimensional array of numbers. In this example, the cat image is 248 pixel wide, 400 pixel tall, and has three color channel Red, Green, Blue. Therefore, the image consists of \\(248 \\times 400 \\times 3\\) numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is turn this quarter of a million numbers into a single label, such as “cat”. \nChallenges: Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for human to perform, it is worth considering the challenges involved from the perspective of a CV algorithm. As we present list of challenges below.\n\nViewpoint variation: A single instance of an object can be oriented in many ways with respect to the camera.\nScale variation: Visual classes often exhibit variation of their size (size in the real world, not only in terms of their extent in the image)\nDeformation: Many objects of interest are not rigid bodies and can be deformed in extremes ways.\nOcclusion: The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.\nIllumination conditions: The effect of illumination are drastic on the pixel level.\nBackground clutter: The objects of interest may blend into their environment, making them hard to identify.\nIntra-class variation: The classes of interest can be often be relatively broad, such as chair. Their are many different types of these objects, each with their own appearance. \n\nA good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.\n\nCross product of variations This refers to the combination of all possible variations (e.g. viewpoint, scale, deformation, occlusion, illumination, etc.) that can occur within a class. The term “cross production” here is metaphorical, inpsired by the mathematical concept of a Cartesian product, which generate all possible combinations of elements from multiple sets.\n\nExample: A chair might appear:\n\nRotate (viewpoint variation)\nPartially hidden (occlusion)\nUnder bright sunlight (illumination)\nWhile being non-rigid (deformation) The model must recognize it as a “chair” despite this complex combination of variations Invariance Requirement: The model must be invariant to these variations, meaning its prediction for a class should not change even when factors alter the object’s appearance.\n\n\nInter-class variation There are difference between distinct classes (e.g. chairs vs tables). A model must retain sensitivity to these differences to avoid confusing classes, even when they share superficial similarities.\n\nExample: A “stool” (class: chair) and a “small table” (class: table) might both appear at similar scales or under similar lighting. The model must distinguish them based on defining feature (e.g. height, presence of a backrest).\nSensitivity Requirement: The model must preserver discriminate features that separate classes, even when intra-class variations (e.g. deformation in chairs) are extreme. Data-driven approach: How might we go about writing an algorithm that can classify images into distinct categories? Unlike writing an algorithm for, for example, sorting a list of numbers, it is not obvious how one might write an algorithm for identifying cats in images. Therefore, instead of trying to specify what every one of the categories of interest look like directly in code, the approach that we will take is not unlike one we would take with a child: we’re going to provide the computer with many examples of each class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. This approach is referred to as a data-driven approach, since it relies on first accumulating a training dataset of labeled images. Here is an example of what such a dataset might look like:\n\n\n\nThe image classification pipeline We’ve seen that the task in Image Classification is to taken an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows: - Input: Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the learning set. - Learning: Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model. - Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the ground truth)."
  },
  {
    "objectID": "programming/convolution/index.html#the-choice-of-distance",
    "href": "programming/convolution/index.html#the-choice-of-distance",
    "title": "Image classification",
    "section": "The choice of distance",
    "text": "The choice of distance"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html",
    "title": "Divide Array Into Equal Pairs",
    "section": "",
    "text": "Topic: array, hash table, bit manipulation, counting"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#count-array",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#count-array",
    "title": "Divide Array Into Equal Pairs",
    "section": "Count array",
    "text": "Count array\nnums that have 2 * n intenger\ndivide nums into n pairs\n1 element in 1 pair\nelements in pair is equal\nreturn true if can devide to n pair,\nSo, we can use count array\nif all even \\`return true\\`\n\nelse \\`return false\\`\nTC: O(n)\nclass Solution:\n    def divideArray(self, nums: List[int]) -&gt; bool:\n    ans = True\n    count_array = [0]*(500+1)\n    for num in nums:\n        count_array[num] += 1\n    print(count_array)\n    for num in count_array:\n        if num % 2 != 0:\n            return False\n    return ans"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#map",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#map",
    "title": "Divide Array Into Equal Pairs",
    "section": "Map",
    "text": "Map\nlike approach 1, we can use map for that (better code)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        frequency = Counter(nums)\n        # check consecutive pairs in sorted array\n        return all(count % 2 == 0 for count in frequency.values())"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#bool-array",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#bool-array",
    "title": "Divide Array Into Equal Pairs",
    "section": "Bool array",
    "text": "Bool array\nan improve, use boolean array\nO(n)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n\n        max_num = max(nums)\n\n        needs_pair = [False] * (max_num + 1)\n\n        for num in nums:\n            needs_pair[num] = not needs_pair[num]\n\n        return not any(needs_pair[num] for num in nums)"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#sorted",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#sorted",
    "title": "Divide Array Into Equal Pairs",
    "section": "Sorted",
    "text": "Sorted\nsorted that can have TC: O(nlogn)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        nums.sort()\n        # check consecutive pairs in sorted array\n        return all(nums[i] == nums[i+1] for i in range (0, len(nums), 2))"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#hash-set",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#hash-set",
    "title": "Divide Array Into Equal Pairs",
    "section": "Hash set",
    "text": "Hash set\nwe can store a element when first meet it, and even get it, we remote from set\nwhen retrieve all, if set have element.\nhash set have TC of lookup, addition, removal in constant time.\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        unpaired = set()\n\n        for num in nums:\n            if num in unpaired:\n                unpaired.remove(num)\n            else:\n                unpaired.add(num)\n        return not unpaired"
  },
  {
    "objectID": "programming/deep-learning-interview/index.html",
    "href": "programming/deep-learning-interview/index.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Logistic regression(Tolles and Meurer, 2016) is a model named after the logistic function, which plays a central role in the model.\nOriginally, the logistic function was created from typical statistical models of population growth. This function takes an S-shaped form and maps real values to a range in \\((0, L)\\). The general mathematical formula of the logistic function is:\n\\[\nf(x) = \\frac{L}{1 + e^{-k(x - x_0)}} \\tag{1}\n\\]\nwhere:\n\n\\(x_0\\) is the value at the midpoint of the logistic curve,\n\\(k\\) is the growth rate of the logistic function,\n\\(L\\) is the maximum value of the logistic function.\n\n\nThe logistic regression model is often used in classification tasks, especially binary classification, even though the term “regression” is included in its name. The upcoming sections will explain why this naming convention was adopted."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-1-increase-observation",
    "href": "programming/deep-learning-interview/index.html#problem-1-increase-observation",
    "title": "Logistic regression",
    "section": "2.1 Problem 1: Increase observation",
    "text": "2.1 Problem 1: Increase observation\nTrue or False: For a fixed number of observations in a data set, introducing more variables normally generates a model that has a better fit to the data. What may be the drawback of such a model fitting strategy?\nExample:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Create synthetic data\nnp.random.seed(0)\nn_samples = 30\nX = np.sort(5 * np.random.rand(n_samples, 1), axis=0)\ny = np.sin(X).ravel() + np.random.normal(0, 0.2, size=n_samples)\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Polynomial degrees to test\ndegrees = [1, 3, 9]\n\nplt.figure(figsize=(15, 4))\n\nfor i, degree in enumerate(degrees, 1):\n    # Generate polynomial features\n    poly = PolynomialFeatures(degree=degree)\n    X_train_poly = poly.fit_transform(X_train)\n    X_test_poly = poly.transform(X_test)\n\n    # Fit model\n    model = LinearRegression()\n    model.fit(X_train_poly, y_train)\n\n    # Predict\n    X_plot = np.linspace(0, 5, 100).reshape(-1, 1)\n    X_plot_poly = poly.transform(X_plot)\n    y_plot = model.predict(X_plot_poly)\n\n    # Calculate errors\n    train_mse = mean_squared_error(y_train, model.predict(X_train_poly))\n    test_mse = mean_squared_error(y_test, model.predict(X_test_poly))\n\n    # Plot\n    plt.subplot(1, 3, i)\n    plt.scatter(X_train, y_train, color='blue', label='Train data')\n    plt.scatter(X_test, y_test, color='green', label='Test data')\n    plt.plot(X_plot, y_plot, color='red', label=f'Degree {degree}')\n    plt.title(f\"Degree {degree}\\nTrain MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe see that: - Degree 1: (underfit): High bias, both train and test error are high - Degree 3 (good fit): Balanced bias-variance, good generalization. - Degree 9 (overfit): Train error very low, but test error high – model fits noise.\nTrue – Introducing more variables generally improves the model’s fit to the training data.\nHowever, there is a major drawback: it often leads to overfitting.\n\n\n2.1.1 Explanation\n\nWhy the Fit Improves\n\nIn regression or classification tasks, adding more features gives the model more flexibility to match the training data.\nThis allows the model to capture finer patterns, reduce residuals, and minimize training error.\n\nExample:\nIn polynomial regression, increasing the degree (i.e., adding more variables) can make the curve pass through all data points, resulting in nearly zero training error.\n\n\n\n\n2.1.2 Drawbacks of This Strategy\n\n1. Overfitting\n\nA model that fits the training data too well may learn noise or random fluctuations instead of the true underlying patterns.\nThis results in poor generalization to unseen or test data.\n\n\n\n2. Increased Variance\n\nMore variables increase the model’s sensitivity to small changes in data.\nA high-variance model may change dramatically with minor input changes.\n\n\n\n3. Curse of Dimensionality\n\nIn high-dimensional spaces, data becomes sparse.\nConcepts like distance, density, and similarity lose their meaning.\nMany algorithms (e.g., k-NN, clustering) perform poorly in high dimensions.\n\n\n\n4. Interpretability\n\nAdding more variables makes the model harder to interpret.\nThis is a problem in domains where transparency is important (e.g., medicine, finance).\n\n\n\n5. Computational Cost\n\nMore variables require more memory and longer training times.\nFeature selection or dimensionality reduction may be needed to manage complexity.\n\n\n\n\n\n2.1.3 Summary\nTrue – Adding more variables generally improves the fit on training data,\nbut it increases the risk of overfitting, poor generalization, and computational burden.\n\n\n\n2.1.4 Best Practice\nUse techniques like cross-validation and regularization (e.g., Lasso, Ridge, dropout)\nto balance model complexity and generalization performance."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-2-odds",
    "href": "programming/deep-learning-interview/index.html#problem-2-odds",
    "title": "Logistic regression",
    "section": "2.2 Problem 2: Odds",
    "text": "2.2 Problem 2: Odds\nDefine the term “odds of success” both qualitatively and formally. Give a numerical example that stresses the relation between probability and odds of an event occurring.\n\n2.2.1 Definition: Odds of Success\n\n\n2.2.2 Qualitative Definition\nThe odds of success express how much more likely an event is to occur than not occur. It is often used in statistics and logistic regression.\n\nIf an event is very likely, the odds are high.\nIf an event is unlikely, the odds are low.\nIf the event is equally likely to happen or not, the odds are 1 (or “even odds”).\n\n\n\n2.2.3 Formal Definition\nLet p be the probability of success (i.e., the event occurring). Then the odds of success are defined as:\n\\[\n\\text{Odds of success} = \\frac{p}{1 - p}\n\\]\nThis compares the chance the event does happen (p) to the chance it does not happen (1 - p).\n\n\n\n2.2.4 Numerical Example\nSuppose the probability of success is:\n\\[\np = 0.75\n\\]\nThen the odds of success are:\n\\[\n\\text{Odds} = \\frac{0.75}{1 - 0.75} = \\frac{0.75}{0.25} = 3\n\\]\nInterpretation:\nThe event is 3 times more likely to occur than not occur.\nIn other words, for every 3 successes, we expect 1 failure.\n\n\n\n2.2.5 Additional Comparison\n\n\n\nProbability (p)\nOdds = p / (1 - p)\n\n\n\n\n0.5\n1.0\n\n\n0.8\n4.0\n\n\n0.25\n0.33\n\n\n\nAs the probability increases toward 1, the odds increase toward infinity.\n\n\n\n2.2.6 Inverse: From Odds to Probability\nIf you are given the odds $ o $, you can convert back to probability:\n\\[\np = \\frac{o}{1 + o}\n\\]\nExample:\nIf odds = 4, then\n\\[\np = \\frac{4}{1 + 4} = \\frac{4}{5} = 0.8\n\\]"
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-3-interaction",
    "href": "programming/deep-learning-interview/index.html#problem-3-interaction",
    "title": "Logistic regression",
    "section": "2.3 Problem 3: Interaction",
    "text": "2.3 Problem 3: Interaction\n\nDefine what is meant by the term “interaction”, in the context of a logistic regression predictor variable.\nWhat is the simplest form of an interaction? Write its formulae.\nWhat statistical tests can be used to attest the significance of an interaction term?\n\n\n2.3.1 1. Definition of Interaction in Logistic Regression\nIn logistic regression, an interaction occurs when the effect of one predictor variable on the outcome depends on the level of another predictor variable.\nThis means the predictors do not act independently: the combined effect of two variables is not simply additive on the log-odds scale.\nExample:\nIf \\(X_1\\) is age and \\(X_2\\) is smoking status, an interaction term (\\(X_1 \\cdot X_2\\)) would capture how the effect of age on the probability of disease differs between smokers and non-smokers.\n\n\n\n2.3.2 2. Simplest Form of an Interaction\nThe simplest interaction involves two variables in a logistic regression model. The formula (on the log-odds scale) is:\n\\[\n\\log\\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\cdot X_2)\n\\]\nWhere: - \\(X_1\\) and \\(X_2\\) are predictors - \\(X_1 \\cdot X_2\\) is the interaction term - \\(\\beta_3\\) captures the change in effect of \\(X_1\\) at different levels of \\(X_2\\)\nIf \\(\\beta_3 \\ne 0\\), there is a statistically significant interaction.\n\n\n\n2.3.3 3. Statistical Tests for Interaction Terms\nTo test whether the interaction term significantly improves the model:\n\na. Wald Test\n\nTests if \\(\\beta_3 = 0\\)\nBased on the standard error and coefficient\nCommonly used in software output (e.g., summary() in R or LogitResults in statsmodels)\n\n\n\nb. Likelihood Ratio Test (LRT)\n\nCompares:\n\nModel 1: with interaction term\nModel 2: without interaction term\n\nNull hypothesis: interaction term does not improve the model\nLRT is more robust than the Wald test, especially in small samples\n\nSteps: 1. Fit both models (with and without interaction) 2. Compute:\n\\[\n\\chi^2 = -2(\\log L_{\\text{reduced}} - \\log L_{\\text{full}})\n\\] 3. Compare with chi-square distribution (df = 1 for one interaction term)\n\n\nc. ANOVA (Analysis of Deviance)\n\nAlternative approach to compare nested models in logistic regression.\nOften used in R with anova(model1, model2, test = \"Chisq\")\n\n\n\n\n2.3.4 Note: Interaction and Information Theory\nIn the context of information theory, interaction terms in a model can be interpreted as capturing mutual information between predictor variables and their combined influence on the target.\n\n\n\n2.3.5 Interaction as Additional Information\nWithout an interaction term, a model assumes additivity: each predictor affects the outcome independently. However, if two variables jointly influence the outcome, then their interaction carries additional information beyond their individual effects.\nThis added value can be viewed as:\n\nExtra bits of information (in the sense of entropy reduction) gained by knowing the joint effect of variables\nMutual information between variables that is relevant to the response, not captured in their marginal contributions\n\n\n\n\n2.3.6 Impact on Model Performance\n\n1. Improved Predictive Power\n\nCaptures complex relationships\nLeads to better fit and generalization, if the interaction is real and not noise\n\n\n\n2. Reduced Residual Uncertainty\n\nReduces unexplained variation in the outcome\nAnalogous to decreasing entropy in the output distribution by incorporating more structure\n\n\n\n3. Better Feature Representation\n\nInteraction terms effectively encode feature combinations that correlate strongly with the outcome\nSimilar to feature engineering guided by information gain\n\n\n\n\n\n2.3.7 Summary\nAdding interaction terms allows the model to capture dependency structures among variables that are meaningful to the target, thereby increasing the information the model has about the outcome. In information-theoretic terms, interactions reduce conditional entropy and increase mutual information between inputs and output."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-4",
    "href": "programming/deep-learning-interview/index.html#problem-4",
    "title": "Logistic regression",
    "section": "2.4 Problem 4:",
    "text": "2.4 Problem 4:\nTrue or False: In machine learning terminology, unsupervised learning refers to the mapping of input covariates to a target response variable that is attempted at being predicted when the labels are known.\nFalse\n\n2.4.1 Explanation\nIn machine learning, the statement describes supervised learning, not unsupervised learning.\n\n\n\n2.4.2 Definitions:\n\nSupervised Learning:\nThe algorithm learns to map input features (covariates) to a known target variable (labels).\nExamples: classification, regression.\nUnsupervised Learning:\nThe algorithm is used when labels are unknown. It finds patterns or structures in the data.\nExamples: clustering, dimensionality reduction.\n\n\n\n\n2.4.3 Why the Statement is False:\n\n“Unsupervised learning refers to the mapping of input covariates to a target response variable that is attempted at being predicted when the labels are known.”\n\n\nIt incorrectly claims unsupervised learning uses known labels, which is not true.\nThis description actually fits supervised learning.\n\n\n\n\n2.4.4 Corrected Version:\n\nSupervised learning refers to the mapping of input covariates to a target response variable, using known labels."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-5",
    "href": "programming/deep-learning-interview/index.html#problem-5",
    "title": "Logistic regression",
    "section": "2.5 Problem 5:",
    "text": "2.5 Problem 5:\nComplete the following sentence: In the case of logistic regression, the response variable is the log of the odds of being classified in […].\nComplete sentence:\nIn the case of logistic regression, the response variable is the log of the odds of being classified in the reference (or “positive”) category.\n\n\n2.5.1 Explanation\nLogistic regression models the probability of a binary outcome by applying the logit function to the response:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\n\\]\n\n\\(p\\) is the probability of the outcome being in the positive or reference class (e.g., “yes”, “success”, or class = 1).\nThe left-hand side is the log-odds of that outcome.\nThe model learns a linear relationship between the predictors and the log-odds of classification in the target category.\n\nSolution:\nIn the case of logistic regression, the response variable is the log of the odds of being classified in a group of binary or multi-class responses.\nThis definition essentially demonstrates that odds can take the form of a vector.\n\n\n\n2.5.2 Clarification:\n\nFor binary logistic regression, the model estimates: \\[\n\\log\\left(\\frac{p}{1 - p}\\right)\n\\] where \\(p\\) is the probability of being in the positive class.\nFor multinomial (multi-class) logistic regression, the model estimates a set of log-odds: \\[\n\\log\\left(\\frac{p_k}{p_{reference}}\\right)\n\\] for each class \\(k \\ne\\) reference, resulting in a vector of log-odds, one for each class.\n\nThus, in multiclass cases, the model output is not a single scalar log-odds but a vector of log-odds, supporting the idea that odds can be vector-valued."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-6",
    "href": "programming/deep-learning-interview/index.html#problem-6",
    "title": "Logistic regression",
    "section": "2.6 Problem 6:",
    "text": "2.6 Problem 6:\nDescribe how in a logistic regression model, a transformation to the response variable is applied to yield a probability distribution. Why is it considered a more informative representation of the response?\n\n2.6.1 Logistic Regression: Transformation of the Response Variable\nIn logistic regression, the response variable is categorical (often binary), but the model must output continuous values to fit it using linear predictors. This is done by applying a logit transformation, and then its inverse—the logistic (sigmoid) function—to map outputs to probabilities.\n\n\n\n2.6.2 Step-by-Step Transformation\n\nLinear combination of predictors: \\[\nz = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\n\\]\nLogit transformation (link function): \\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right) = z\n\\]\nInverse-logit (sigmoid) function to obtain probability: \\[\np = \\frac{1}{1 + e^{-z}}\n\\]\n\nThis maps any real-valued input \\(z \\in (-\\infty, \\infty)\\) into a valid probability \\(p \\in (0, 1)\\).\n\n\n\n2.6.3 Why This Is Informative\n\nProbabilistic output: Unlike hard class labels, logistic regression provides the estimated probability of belonging to a class, which gives more nuanced information.\nUncertainty awareness: Probabilities allow us to gauge confidence in predictions. For example, a prediction of 0.95 is more confident than 0.55.\nThreshold flexibility: You can choose decision thresholds based on the application (e.g., 0.5, 0.7) rather than being locked into fixed class predictions.\nSupports ranking and calibration: Probabilities are useful for ROC analysis, calibration, and expected loss minimization.\n\n\n\n\n2.6.4 Summary\nLogistic regression transforms the response variable through the logit link and uses its inverse to map model outputs to a valid probability distribution. This enables not only classification but also a more informative and interpretable representation of the predicted outcomes.\n\n\n2.6.5 Note: Pros and Cons of Output Transformations in Logistic Regression\nWhen transforming the response variable into a probability distribution, several methods can be used depending on the problem type. The most common are:\n\nSigmoid function — for binary classification\nSoftmax function — for multi-class classification\nClassic normalization — general scaling of outputs (less used in classification)\n\nBelow is a comparison of their pros and cons:\n\n\n\n2.6.6 1. Sigmoid Function\nDefinition: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nUse Case: Binary classification (2 classes)\nPros: - Simple and computationally efficient - Naturally maps real values to the interval (0, 1) - Interpretable as the probability of the positive class\nCons: - Only supports binary output - Cannot capture interactions among multiple classes - Not ideal for mutually exclusive multi-class problems\n\n\n\n2.6.7 2. Softmax Function\nDefinition: \\[\n\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n\\]\nUse Case: Multi-class classification (K &gt; 2, mutually exclusive classes)\nPros: - Generalizes sigmoid to multi-class setting - Produces a valid probability distribution over \\(K\\) classes - Probabilities sum to 1, suitable for cross-entropy loss\nCons: - Sensitive to extreme values (due to exponentiation) - Less robust to outliers in inputs - Computationally more expensive than sigmoid\n\n\n\n2.6.8 3. Classic Normalization\nDefinition: \\[\n\\text{normalized}(z_i) = \\frac{z_i}{\\sum_{j=1}^K z_j}\n\\]\nUse Case: Sometimes used as an approximation or in non-logistic models\nPros: - Simple and fast - Avoids exponentiation (numerically stable)\nCons: - Not guaranteed to produce valid probabilities unless all \\(z_i \\ge 0\\) - Can yield values outside [0, 1] if inputs are not positive - Lacks probabilistic interpretation unless additional constraints are applied\n\n\n\n2.6.9 Summary Table\n\n\n\n\n\n\n\n\n\n\n\nTransformation\nBest for\nOutput Range\nSums to 1\nInterpretable Probabilities\nKey Limitation\n\n\n\n\nSigmoid\nBinary classification\n(0, 1)\nNo\nYes\nNot suitable for &gt;2 classes\n\n\nSoftmax\nMulti-class classification\n(0, 1)\nYes\nYes\nSensitive to outliers\n\n\nNormalization\nHeuristic scaling\nVaries\nPossibly\nNot always\nMay not yield valid probs\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Raw model outputs (logits)\nlogits = np.array([2.0, 1.0, 0.1])\n\n# 1. Sigmoid function (binary case, apply to a single logit)\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nsigmoid_result = sigmoid(logits[0])  # Binary case example\n\n# 2. Softmax function (multi-class case)\ndef softmax(z):\n    exp_z = np.exp(z - np.max(z))  # stability improvement\n    return exp_z / np.sum(exp_z)\n\nsoftmax_result = softmax(logits)\n\n# 3. Classic normalization (not ideal for probabilities unless values are positive)\ndef normalize(z):\n    z_sum = np.sum(z)\n    return z / z_sum if z_sum != 0 else np.zeros_like(z)\n\nnormalize_result = normalize(logits)\n\n# Print results\nprint(\"Raw logits:       \", logits)\nprint(\"Sigmoid (z=2.0):  \", sigmoid_result)\nprint(\"Softmax:          \", softmax_result)\nprint(\"Normalization:    \", normalize_result)\n\n# Plot comparison\nlabels = ['Class 1', 'Class 2', 'Class 3']\nx = np.arange(len(labels))\nwidth = 0.25\n\nfig, ax = plt.subplots()\nax.bar(x - width, softmax_result, width, label='Softmax')\nax.bar(x, normalize_result, width, label='Normalization')\nax.bar(x + width, [sigmoid_result, 0, 0], width, label='Sigmoid (binary)')\n\nax.set_ylabel('Output Value')\nax.set_title('Output Transformations')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\nRaw logits:        [2.  1.  0.1]\nSigmoid (z=2.0):   0.8807970779778823\nSoftmax:           [0.65900114 0.24243297 0.09856589]\nNormalization:     [0.64516129 0.32258065 0.03225806]\n\n\n\n\n\n\n\n\n\nSummary: - Sigmoid applies to one logit for binary classification.\n\nSoftmax distributes probabilities across multiple classes.\nNormalization divides values by their sum but doesn’t always yield valid probabilities."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-7",
    "href": "programming/deep-learning-interview/index.html#problem-7",
    "title": "Logistic regression",
    "section": "2.7 Problem 7:",
    "text": "2.7 Problem 7:\nComplete the following sentence: Minimizing the negative log likelihood also means maximizing the […] of selecting the […] class.\nComplete sentence:\nMinimizing the negative log likelihood also means maximizing the likelihood of selecting the correct class.\n\n2.7.1 Explanation\nMinimizing the negative log likelihood (NLL) is equivalent to maximizing the likelihood of the model predicting the correct class.\n\nWhy?\nGiven: - A model that outputs predicted probabilities \\(p(y_i \\mid x_i)\\) for each observation - True class labels \\(y_i\\)\nThen the likelihood for the correct predictions is: \\[\nL = \\prod_{i=1}^{n} p(y_i \\mid x_i)\n\\]\nTaking the log-likelihood: \\[\n\\log L = \\sum_{i=1}^{n} \\log p(y_i \\mid x_i)\n\\]\nThe negative log-likelihood (NLL) is: \\[\n\\text{NLL} = -\\log L = -\\sum_{i=1}^{n} \\log p(y_i \\mid x_i)\n\\]\nSo minimizing NLL is mathematically the same as maximizing the log-likelihood, which increases the probability assigned to the correct class.\n\n\n\n\n2.7.2 Python Code Illustration\nBelow is an example comparing NLL for different predicted probabilities of the correct class:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulated predicted probabilities for the correct class\np_correct = np.linspace(0.01, 1.0, 100)\nnll = -np.log(p_correct)  # Negative log-likelihood\n\n# Plot\nplt.figure(figsize=(7, 4))\nplt.plot(p_correct, nll, label='NLL = -log(p)', color='darkblue')\nplt.title('Negative Log-Likelihood vs. Probability of Correct Class')\nplt.xlabel('Predicted Probability for Correct Class')\nplt.ylabel('Negative Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen the model assigns a high probability to the correct class, the NLL is low.\nAs the probability approaches 0, the NLL becomes very large.\nThus, minimizing NLL encourages the model to be more confident and accurate in predicting the correct class.\n\n\n2.7.3 Step-by-Step: Understanding Negative Log Likelihood (NLL)\nThis walkthrough will help you see how and why minimizing NLL means maximizing the probability of the correct class, both conceptually and numerically.\n\n\nStep 1: Define the task\nWe have a binary classification model, and it predicts a probability for the correct class.\n\n\n\nStep 2: Simulate model predictions\nWe simulate predicted probabilities for the true class (label = 1).\n\n\nCode:\n\n\nCode\npredicted_probs = [0.9, 0.7, 0.5, 0.3, 0.1]  # Predicted probability for the correct class\npredicted_probs\n\n\n[0.9, 0.7, 0.5, 0.3, 0.1]\n\n\n\n\nCode\nimport numpy as np\n\nprint(\"Predicted Probability → Negative Log-Likelihood\")\nfor p in predicted_probs:\n    nll = -np.log(p)\n    print(f\"{p:.1f} → {nll:.4f}\")\n\n\nPredicted Probability → Negative Log-Likelihood\n0.9 → 0.1054\n0.7 → 0.3567\n0.5 → 0.6931\n0.3 → 1.2040\n0.1 → 2.3026\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\np_vals = np.linspace(0.01, 1.0, 100)\nnll_vals = -np.log(p_vals)\n\nplt.plot(p_vals, nll_vals, label=\"NLL = -log(p)\", color=\"blue\")\nplt.xlabel(\"Predicted Probability for Correct Class\")\nplt.ylabel(\"Negative Log-Likelihood\")\nplt.title(\"NLL vs. Predicted Probability\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen the model is confident and right (e.g. \\(p = 0.9\\)), the NLL is low.\nWhen it’s unsure or wrong (e.g. \\(p = 0.1\\)), the NLL is high.\nTherefore, minimizing NLL encourages the model to assign high probability to the correct class."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-8",
    "href": "programming/deep-learning-interview/index.html#problem-8",
    "title": "Logistic regression",
    "section": "2.8 Problem 8:",
    "text": "2.8 Problem 8:\nAssume the probability of an event occurring is p = 0.1. 1. What are the odds of the event occurring?. 2. What are the log-odds of the event occurring?. 3. Construct the probability of the event as a ratio that equals 0.1.\n\n2.8.1 Step-by-Step: Probability, Odds, and Log-Odds\nAssume the probability of an event occurring is:\n\\[\np = 0.1\n\\]\n\n\n\n2.8.2 1. What are the odds of the event occurring?\nDefinition: &gt; Odds are the ratio of the probability of the event occurring to the probability of it not occurring.\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\n\n\n2.8.3 Calculation:\n\\[\n\\text{odds} = \\frac{0.1}{1 - 0.1} = \\frac{0.1}{0.9} \\approx 0.111\n\\]\n\n\n\n2.8.4 2. What are the log-odds (logit) of the event?\nDefinition: &gt; Log-odds are the logarithm of the odds (also known as the logit function):\n\\[\n\\text{log-odds} = \\log\\left(\\frac{p}{1 - p}\\right)\n\\]\n\n\n2.8.5 Calculation:\n\\[\n\\log\\left(\\frac{0.1}{0.9}\\right) = \\log(0.111...) \\approx -2.197\n\\]\n\n\n\n2.8.6 3. Construct the probability as a ratio that equals 0.1\nWe want to express:\n\\[\n\\frac{\\text{favorable outcomes}}{\\text{total outcomes}} = 0.1\n\\]\nOne example:\n\\[\n\\frac{1}{10} = 0.1\n\\]\nSo, this means 1 favorable case out of 10 total cases, or 9 unfavorable cases.\n\n\n\n2.8.7 Summary Table\n\n\n\nMetric\nValue\nFormula\n\n\n\n\nProbability\n0.1\ngiven\n\n\nOdds\n0.111\n\\(\\frac{0.1}{0.9}\\)\n\n\nLog-Odds\n-2.197\n\\(\\log\\left(\\frac{0.1}{0.9}\\right)\\)\n\n\nRatio Form\n1:9\n\\(1/10 = 0.1\\)\n\n\n\n\n\n2.8.8 Intuition Behind Probability, Odds, and Log-Odds\nUnderstanding why we use these representations helps clarify their role in models like logistic regression.\n\n\n\n2.8.9 Probability (p)\n\nIntuitive measure of likelihood: ranges between 0 and 1.\nEasy to interpret: “There is a 10% chance this will happen.”\n\nBut: Not ideal for modeling, because probabilities are bounded, and small changes near 0 or 1 can be disproportionate.\n\n\n\n2.8.10 Odds: \\(\\frac{p}{1 - p}\\)\n\nRepresent relative chances: how likely something is vs. not.\nExample: odds = 2 means “twice as likely to happen than not.”\n\nOdds are unbounded (0 to ∞), unlike probabilities. This makes them easier to model with linear functions.\n\n\n\n2.8.11 Log-Odds (Logit): \\(\\log\\left(\\frac{p}{1 - p}\\right)\\)\n\nTransforms probabilities to the entire real line: \\((-\\infty, +\\infty)\\).\nLinear in model parameters — makes logistic regression a linear model in log-odds space.\nSymmetric: log-odds of 0 means \\(p = 0.5\\).\n\n\nThis transformation enables optimization with gradient-based methods and maintains interpretability via the inverse sigmoid function.\n\n\n\n\n2.8.12 Summary: Why use log-odds?\n\n\n\nRepresentation\nRange\nGood For\n\n\n\n\nProbability\n[0, 1]\nIntuition, interpretability\n\n\nOdds\n[0, ∞)\nRelative comparison\n\n\nLog-Odds\n(−∞, ∞)\nLinear modeling, optimization\n\n\n\nLog-odds give models a mathematically stable and interpretable way to reason about binary outcomes — especially for logistic regression.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Probabilities from 0.01 to 0.99 (avoid 0 and 1 to prevent log(0))\np = np.linspace(0.01, 0.99, 500)\n\n# Compute odds and log-odds\nodds = p / (1 - p)\nlog_odds = np.log(odds)\n\n# Plotting\nfig, axs = plt.subplots(3, 1, figsize=(8, 10), sharex=True)\n\n# Probability\naxs[0].plot(p, p, color='blue')\naxs[0].set_ylabel(\"Probability (p)\")\naxs[0].set_title(\"Probability vs. Itself (Identity)\")\naxs[0].grid(True)\n\n# Odds\naxs[1].plot(p, odds, color='green')\naxs[1].set_ylabel(\"Odds (p / (1 - p))\")\naxs[1].set_title(\"Probability vs. Odds\")\naxs[1].grid(True)\n\n# Log-Odds\naxs[2].plot(p, log_odds, color='red')\naxs[2].set_ylabel(\"Log-Odds (log(p / (1 - p)))\")\naxs[2].set_xlabel(\"Probability (p)\")\naxs[2].set_title(\"Probability vs. Log-Odds\")\naxs[2].axhline(0, color='gray', linestyle='--')\naxs[2].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.8.13 Interpretation of the Plots\n\n\n2.8.14 Top Plot: Probability vs. Itself\n\nThis is the identity function, where the output equals the input.\nUseful to visualize the bounded linearity of probability values.\nRange is limited to [0, 1], which restricts direct use in linear models.\n\n\n\n\n2.8.15 Middle Plot: Probability vs. Odds\n\nOdds are computed as:\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nAs \\(p \\to 1\\), the odds grow rapidly (approaching ∞).\nAs \\(p \\to 0\\), the odds approach 0.\nNonlinear and asymmetric, making it difficult to model directly.\n\n\n\n\n2.8.16 Bottom Plot: Probability vs. Log-Odds (Logit)\n\nLog-odds are computed as:\n\\[\n\\text{log-odds} = \\log\\left(\\frac{p}{1 - p}\\right)\n\\]\nThe transformation is:\n\nSmooth\nSymmetric around \\(p = 0.5\\)\nLinear near \\(p = 0.5\\)\nMaps \\(p \\in (0, 1)\\) to \\((-\\infty, +\\infty)\\)\n\n\n\n\n\n2.8.17 Why Use Log-Odds?\nThe log-odds transformation allows:\n\nApplying linear models to binary classification.\nSmooth optimization using gradient descent.\nEasy interpretability: a one-unit increase in input causes a fixed increase in log-odds.\n\nThus, log-odds are the foundation of logistic regression, enabling a linear combination of inputs to model a probability through the sigmoid inverse."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-9.",
    "href": "programming/deep-learning-interview/index.html#problem-9.",
    "title": "Logistic regression",
    "section": "2.9 Problem 9.",
    "text": "2.9 Problem 9.\nTrue or False: If the odds of success in a binary response is 4, the corresponding probability of success is 0.8.\n\n\n2.9.1 Step-by-step Solution\nWe are given: \\[\n\\text{odds} = 4\n\\]\nRecall the relationship between odds and probability: \\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nSolve for \\(p\\): \\[\n\\frac{p}{1 - p} = 4\n\\Rightarrow p = 4(1 - p)\n\\Rightarrow p = 4 - 4p\n\\Rightarrow 5p = 4\n\\Rightarrow p = \\frac{4}{5} = 0.8\n\\]\n\n\n\n2.9.2 Final Answer\nTrue – If the odds are 4, the probability of success is 0.8."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-10",
    "href": "programming/deep-learning-interview/index.html#problem-10",
    "title": "Logistic regression",
    "section": "2.10 Problem 10:",
    "text": "2.10 Problem 10:\nDraw a graph of odds to probabilities, mapping the entire range of probabilities to their respective odds.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a range of probabilities from 0.01 to 0.99\np = np.linspace(0.01, 0.99, 500)\nodds = p / (1 - p)\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.plot(p, odds, color='blue')\nplt.xlabel(\"Probability (p)\")\nplt.ylabel(\"Odds (p / (1 - p))\")\nplt.title(\"Mapping: Probability to Odds\")\nplt.grid(True)\nplt.ylim(0, 20)  # limit to see behavior better near p=1\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.10.1 Graph: Probability vs. Odds\nThis plot shows how probability values map to odds:\n\nFormula:\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nAs the probability approaches 1, the odds grow rapidly toward infinity.\nAs the probability approaches 0, the odds approach 0.\nThe function is nonlinear and increasing, with a sharp curve as $ p $.\n\nThis graph helps visualize why odds are unbounded and why it’s useful to convert them to log-odds in modeling."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-11",
    "href": "programming/deep-learning-interview/index.html#problem-11",
    "title": "Logistic regression",
    "section": "2.11 Problem 11:",
    "text": "2.11 Problem 11:\nThe logistic regression model is a subset of a broader range of machine learning models known as generalized linear models (GLMs), which also include analysis of variance (ANOVA), vanilla linear regression, etc. There are three components to a GLM; identify these three components for binary logistic regression.\n\n2.11.1 Components of a Generalized Linear Model (GLM) in Binary Logistic Regression\nA Generalized Linear Model (GLM) has three main components. For binary logistic regression, they are:\n\n\n\n2.11.2 1. Random Component\nSpecifies the distribution of the response variable.\n\nIn binary logistic regression, the response ( Y {0, 1} ) is assumed to follow a Bernoulli distribution: \\[\nY \\sim \\text{Bernoulli}(p)\n\\]\n\n\n\n\n2.11.3 2. Systematic Component\nRepresents the linear predictor, which is a linear combination of input features:\n\nLet ( x = (x_1, x_2, , x_n) ), then: \\[\n\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n\n\\]\nThis is often written compactly as: \\[\n\\eta = \\mathbf{x}^\\top \\boldsymbol{\\beta}\n\\]\n\n\n\n\n2.11.4 3. Link Function\nConnects the expected value of the response to the linear predictor.\n\nIn logistic regression, the link function is the logit function: \\[\n\\text{logit}(p) = \\log\\left( \\frac{p}{1 - p} \\right) = \\eta\n\\]\nThe inverse of the logit gives the sigmoid function to recover probabilities: \\[\np = \\frac{1}{1 + e^{-\\eta}}\n\\]\n\n\n\n\n2.11.5 Summary Table\n\n\n\n\n\n\n\nGLM Component\nLogistic Regression Specification\n\n\n\n\nRandom Component\n( Y (p) )\n\n\nSystematic Component\n( = ^ )\n\n\nLink Function\n( (p) = ( ) )\n\n\n\n\n\n2.11.6 Adjusted GLM Components for Voice Activity Detection (VAD)\nAssume the binary outcome: - \\(Y = 1\\): voice activity detected - \\(Y = 0\\): no voice activity detected\nWe define the GLM components for a logistic regression model as follows:\n\n\n\n2.11.7 Random Component\nThe response variable \\(Y\\) is binary:\n\\[\nY \\sim \\text{Bernoulli}(p)\n\\]\nwhere \\(p = \\mathbb{P}(Y = 1 \\mid \\text{features})\\) represents the probability that voice activity is present in a given time frame.\n\n\n\n2.11.8 Systematic Components\nWe propose two alternative linear predictors using different input features:\n\nSystematic Component A:\nUse energy and zero-crossing rate: \\[\n\\eta = \\beta_0 + \\beta_1 \\cdot \\text{Energy} + \\beta_2 \\cdot \\text{ZCR}\n\\]\n\nEnergy: overall signal power in the frame\n\nZCR (Zero Crossing Rate): frequency of sign changes in waveform\n\n\n\nSystematic Component B:\nUse MFCC coefficients (common in speech processing): \\[\n\\eta = \\beta_0 + \\beta_1 \\cdot \\text{MFCC}_1 + \\beta_2 \\cdot \\text{MFCC}_2 + \\cdots + \\beta_{13} \\cdot \\text{MFCC}_{13}\n\\]\n\nMFCCs: Mel-Frequency Cepstral Coefficients — compact representation of spectral shape\n\n\n\n\n\n2.11.9 Link Function\nUse the logit link function to relate the probability to the linear predictor: \\[\n\\text{logit}(p) = \\log\\left( \\frac{p}{1 - p} \\right) = \\eta\n\\]\nor equivalently: \\[\np = \\frac{1}{1 + e^{-\\eta}}\n\\]\n\n\n\n2.11.10 Summary\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nRandom Component\n\\(Y \\sim \\text{Bernoulli}(p)\\)\n\n\nSystematic A\n\\(\\eta = \\beta_0 + \\beta_1 \\cdot \\text{Energy} + \\beta_2 \\cdot \\text{ZCR}\\)\n\n\nSystematic B\n\\(eta = \\beta_0 + \\sum_{i=1}^{13} \\beta_i \\cdot \\text{MFCC}_i\\)\n\n\nLink Function\n\\(\\text{logit}(p) = \\log\\left( \\frac{p}{1 - p} \\right)\\)\n\n\n\nThis setup applies logistic regression to real-world audio-based classification.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Simulate data for two systematic components\nnp.random.seed(0)\nn_samples = 200\n\n# Simulated features for Systematic A: Energy & ZCR\nenergy = np.random.normal(loc=0.5, scale=0.1, size=n_samples)\nzcr = np.random.normal(loc=0.3, scale=0.05, size=n_samples)\nX_A = np.column_stack((energy, zcr))\n\n# Simulated features for Systematic B: 13 MFCCs\nmfcc = np.random.normal(loc=0, scale=1, size=(n_samples, 13))\nX_B = mfcc\n\n# Simulated binary labels based on true linear model\ndef simulate_labels(X, true_coef):\n    logits = X @ true_coef[1:] + true_coef[0]\n    probs = 1 / (1 + np.exp(-logits))\n    return (np.random.rand(len(probs)) &lt; probs).astype(int), probs\n\n# True coefficients for Systematic A and B\ntrue_coef_A = np.array([-2, 5, 8])  # Intercept, Energy, ZCR\ntrue_coef_B = np.array([-0.5] + [0.3]*13)  # Intercept + MFCCs\n\n# Generate labels\ny_A, p_A = simulate_labels(X_A, true_coef_A)\ny_B, p_B = simulate_labels(X_B, true_coef_B)\n\n# Fit logistic regression models\nscaler_A = StandardScaler().fit(X_A)\nX_A_std = scaler_A.transform(X_A)\nmodel_A = LogisticRegression().fit(X_A_std, y_A)\n\nscaler_B = StandardScaler().fit(X_B)\nX_B_std = scaler_B.transform(X_B)\nmodel_B = LogisticRegression().fit(X_B_std, y_B)\n\n# Predict probabilities\np_pred_A = model_A.predict_proba(X_A_std)[:, 1]\np_pred_B = model_B.predict_proba(X_B_std)[:, 1]\n\n# Plot probability distributions\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(p_pred_A[y_A == 0], bins=20, alpha=0.6, label='No Voice')\nplt.hist(p_pred_A[y_A == 1], bins=20, alpha=0.6, label='Voice')\nplt.title(\"Predicted Probabilities (Energy & ZCR)\")\nplt.xlabel(\"Probability of Voice Activity\")\nplt.ylabel(\"Count\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.hist(p_pred_B[y_B == 0], bins=20, alpha=0.6, label='No Voice')\nplt.hist(p_pred_B[y_B == 1], bins=20, alpha=0.6, label='Voice')\nplt.title(\"Predicted Probabilities (MFCC Features)\")\nplt.xlabel(\"Probability of Voice Activity\")\nplt.ylabel(\"Count\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat this illustrates: Logistic regression maps the linear combination of audio features (systematic component) to a probability of voice activity.\nThe predicted probability distributions show how well the features separate the two classes (voice vs. no voice).\nThis supports the GLM formulation where:\nInputs are combined linearly,\nThe output is transformed via the logit (sigmoid) link,\nAnd the response is modeled as a Bernoulli random variable."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-12",
    "href": "programming/deep-learning-interview/index.html#problem-12",
    "title": "Logistic regression",
    "section": "2.12 Problem 12:",
    "text": "2.12 Problem 12:\nLet us consider the logit transformation, i.e., log-odds. Assume a scenario in which the logit forms the linear decision boundary:\n\\[\n\\log \\left( \\frac{\\Pr(Y = 1 \\mid \\mathbf{X})}{\\Pr(Y = 0 \\mid \\mathbf{X})} \\right) = \\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} \\tag{2.1}\n\\]\nwhere: - \\(\\mathbf{X}\\) is a vector of systematic components (input features), - \\(\\boldsymbol{\\theta}\\) is a vector of predictor coefficients, - \\(\\theta_0\\) is the intercept.\nTask:\nWrite the mathematical expression for the hyperplane that describes the decision boundary for this logistic regression model.\n\n2.12.1 Logistic Regression Decision Boundary (Using Logit)\nGiven the logit model:\n\\[\n\\log \\left( \\frac{\\Pr(Y = 1 \\mid \\mathbf{X})}{\\Pr(Y = 0 \\mid \\mathbf{X})} \\right) = \\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X}\n\\]\nThis expression defines the log-odds as a linear function of input features \\(\\mathbf{X}\\).\n\n\n\n2.12.2 Step 1: Set the Decision Threshold\nIn binary classification, the decision boundary occurs when both classes are equally likely:\n\\[\n\\Pr(Y = 1 \\mid \\mathbf{X}) = \\Pr(Y = 0 \\mid \\mathbf{X}) = 0.5\n\\]\nThus, the odds ratio becomes:\n\\[\n\\frac{\\Pr(Y = 1 \\mid \\mathbf{X})}{\\Pr(Y = 0 \\mid \\mathbf{X})} = 1\n\\]\nTaking the logarithm:\n\\[\n\\log \\left( \\frac{\\Pr(Y = 1 \\mid \\mathbf{X})}{\\Pr(Y = 0 \\mid \\mathbf{X})} \\right) = 0\n\\]\n\n\n\n2.12.3 Step 2: Solve for the Boundary\nSet the log-odds to zero in the original equation:\n\\[\n\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} = 0\n\\]\n\n\n\n2.12.4 Final Result: Hyperplane Equation\nThis is the equation of the decision boundary — a hyperplane that separates the feature space:\n\\[\n\\boxed{\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} = 0}\n\\]\n\nIf \\(\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} &gt; 0\\), then \\(\\Pr(Y=1) &gt; 0.5\\)\nIf \\(\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} &lt; 0\\), then \\(\\Pr(Y=1) &lt; 0.5\\)\n\nThis linear boundary is fundamental in logistic regression for classification tasks.\n\n\n2.12.5 Solution\nThe hyperplane that defines the decision boundary in a logistic regression model is:\n\\[\n\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} = 0 \\tag{2.15}\n\\]\n\n\n\n2.12.6 Derivation from Logit Function\nWe start from the logit model:\n\\[\n\\log \\left( \\frac{\\Pr(Y = 1 \\mid \\mathbf{X})}{\\Pr(Y = 0 \\mid \\mathbf{X})} \\right) = \\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X}\n\\]\nThis expression defines the log-odds of the response variable \\(Y\\) being 1 as a linear function of the input vector \\(\\mathbf{X}\\).\nAt the decision boundary, we are equally likely to classify the outcome as either class (i.e., \\(\\Pr(Y=1) = \\Pr(Y=0) = 0.5\\)). This implies:\n\\[\n\\frac{\\Pr(Y = 1 \\mid \\mathbf{X})}{\\Pr(Y = 0 \\mid \\mathbf{X})} = 1\n\\]\nTaking the logarithm:\n\\[\n\\log \\left( \\frac{\\Pr(Y = 1 \\mid \\mathbf{X})}{\\Pr(Y = 0 \\mid \\mathbf{X})} \\right) = 0\n\\]\nNow set the left-hand side of the model equal to 0:\n\\[\n\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} = 0\n\\]\n\n\n\n2.12.7 Conclusion\nThe equation\n\\[\n\\boxed{\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} = 0}\n\\]\nis the mathematical expression of the hyperplane that separates the classes. It forms the decision boundary in logistic regression, where the model predicts:\n\nClass 1 if \\(\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} &gt; 0\\)\nClass 0 if \\(\\theta_0 + \\boldsymbol{\\theta}^T \\mathbf{X} &lt; 0\\)"
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#problem-13-logit-and-sigmoid",
    "href": "programming/deep-learning-interview/index.html#problem-13-logit-and-sigmoid",
    "title": "Logistic regression",
    "section": "2.13 Problem 13: Logit and Sigmoid",
    "text": "2.13 Problem 13: Logit and Sigmoid\nTrue or False:\nStatement:\nThe logit function and the natural logistic (sigmoid) function are inverses of each other.\nAnswer:\nTrue\n\n\n2.13.1 Explanation:\n\nThe sigmoid function (also known as the logistic function) is defined as:\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nThe logit function is the inverse of the sigmoid and is defined as:\n\\[\n\\text{logit}(p) = \\log \\left( \\frac{p}{1 - p} \\right)\n\\]\nThese two functions are mathematical inverses:\n\nApplying the logit to the output of a sigmoid returns the original input.\nApplying the sigmoid to the output of a logit returns the original probability.\n\n\n\n\n\n2.13.2 Additional Note:\nThe sigmoid function is widely used: - In binary classification to map a linear model’s output to a probability in [0, 1]. - As an activation function in artificial neural networks (although less common now compared to ReLU).\nThus, the statement is True."
  },
  {
    "objectID": "programming/deep-learning-interview/index.html#derivative-of-the-natural-sigmoid-function",
    "href": "programming/deep-learning-interview/index.html#derivative-of-the-natural-sigmoid-function",
    "title": "Logistic regression",
    "section": "2.14 Derivative of the Natural Sigmoid Function",
    "text": "2.14 Derivative of the Natural Sigmoid Function\nLet the sigmoid function be defined as:\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\nThis maps real values \\(x \\in \\mathbb{R}\\) to a range in \\((0, 1)\\).\n\n\n2.14.1 Step 1: Compute the Derivative\nWe differentiate \\(\\sigma(x)\\) with respect to \\(x\\):\nLet:\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}} = f(x)\n\\]\nThen:\n\\[\n\\frac{d}{dx} \\sigma(x) = \\frac{d}{dx} \\left( \\frac{1}{1 + e^{-x}} \\right)\n\\]\nApply the quotient rule or chain rule:\nLet \\(u(x) = 1 + e^{-x}\\), then:\n\\[\n\\frac{d}{dx} \\left( \\frac{1}{u(x)} \\right) = -\\frac{1}{u(x)^2} \\cdot \\frac{d}{dx} u(x)\n\\]\nWe have:\n\\[\n\\frac{d}{dx} u(x) = \\frac{d}{dx} (1 + e^{-x}) = -e^{-x}\n\\]\nSo:\n\\[\n\\frac{d}{dx} \\sigma(x) = -\\frac{1}{(1 + e^{-x})^2} \\cdot (-e^{-x}) = \\frac{e^{-x}}{(1 + e^{-x})^2}\n\\]\n\n\n\n2.14.2 Step 2: Express in Terms of \\(\\sigma(x)\\)\nSince:\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}, \\quad \\text{then} \\quad 1 - \\sigma(x) = \\frac{e^{-x}}{1 + e^{-x}}\n\\]\nSo the derivative becomes:\n\\[\n\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n\\]\n\n\n\n2.14.3 Final Answer:\n\\[\n\\boxed{\\frac{d}{dx} \\sigma(x) = \\sigma(x) \\cdot (1 - \\sigma(x))}\n\\]\nThis elegant result is widely used in training neural networks via backpropagation."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html",
    "href": "programming/disjoint-set-union/index.html",
    "title": "Disjoint Set Union",
    "section": "",
    "text": "This data structure provides the following capabilities. We are given several elements, each of which is a separate set. A DSU will have an operation to combine any two sets, and it will be able to tell in which set a specific element is. The classical version also introduces a third operation, it can create a set from a new element.\nThus the basic interface of this data structure consists of only three operations: - make_set() - create a new set consisting of the new element v. - union_sets(a, b) - merges the two specified sets (the set in which the element a is located, and the set in which the element b is located). - find_set(v) - return the representative (also called leader) of the set that contains the element v. This representative is an element of its corresponding set. It is selected in each set by the data structure itself (and can change over time, namely after union_sets call). This representative can be used to check if two elements are part of the same set or not. a and b are exactly in the same set, if find_set(a) == find_set(b). Otherwise they are in different sets.\nAs described on more detail later, the data structure allows us to do each of these operations in almost \\(O(1)\\) time on average.\nAlso in one of the subsections an alternative structure of a DSU is explained, which achieves a slower average complexity of \\(O(\\log n)\\), but can be more powerful than the regular DSU structure."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#naive-implementation",
    "href": "programming/disjoint-set-union/index.html#naive-implementation",
    "title": "Disjoint Set Union",
    "section": "Naive implementation",
    "text": "Naive implementation\nWe can already write the first implementation of the Disjoint Set Union data structure. It will be pretty inefficient at first, but later we can improve it using two optimizations, so that it will take nearly constant time for each function call.\nAs we said, all the information about the sets of elements will be kept in an array parent.\nTo create a new set (operation make_set(v)), we simply create a tree with root in the vertex x, meaning that it is its own ancestor.\nTo combine two sets (operation union_sets(a, b)), we first find the representative of the set in which a is located, and the representative of the set in which b is located. If the representatives are identical, that we have nothing to do, the sets are already merged. Otherwise, we can simply specify that one of the representatives is the parent of the other representative - thereby combining the two trees.\nFinally the implementation of the find representative function (operation find_set(v)): we simply climb the ancestors of the vertex v until we reach the root, i.e. a vertex such that the reference to the ancestor leads to itself. This operation is easily implemented recursively.\nvoid make_set(int v) {\n    parent[v] = v;\n}\n\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    return find_set(parent[v]);\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b)\n        parent[b] = a;\n}\nHowever this implementation is inefficient. It is easy to construct an example, so that the trees degenerate into long chains. In that case each call find_set(v) can take \\(O(n)\\) time.\nThis is far away from the complexity that we want to have (nearly constant time). Therefore we will consider two optimizations that will allow to significantly accelerate the work."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#path-compression-optimization",
    "href": "programming/disjoint-set-union/index.html#path-compression-optimization",
    "title": "Disjoint Set Union",
    "section": "Path compression optimization",
    "text": "Path compression optimization\nThis optimization is designed for speeding up find_set.\nIf we call find_set() for some vertex x, we actually find the representative p for all vertices that we visit on the path between v and the actually representative p. The trick is to make the paths for all those nodes shorter, by setting the parent of each visited vertex directly to p.\nWe can see the operation in the following image. On the left there is a tree, and on the right side there is the compressed tree after calling find_set(7), which shortens the paths for the visited node 7, 5, 3 and 2.\n![[DSU_path_compression.png]]\nThe new implementation of find_set is as follows:\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    return parent[v] = find_set(parent[v]);\n}\nThe simple implementation does what was intended: first find the representative of the set (root vertex), and then in the process of stack unwinding the visited nodes are attached directly to the representative.\nThis simple modification of the operation already achieves the time complexity \\(O(\\log n)\\) per call on average (here without proof). There is a second modification, that will make it even faster."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#union-by-size-rank",
    "href": "programming/disjoint-set-union/index.html#union-by-size-rank",
    "title": "Disjoint Set Union",
    "section": "Union by size / rank",
    "text": "Union by size / rank\nIn this optimization we will change the union_set operation. To be precise, we will change which tree gets attached to the other one. In the naive implementation the second tree always got attached to the first one. In practice that can lead to trees containing chains of length \\(O(n)\\). With this optimization we will avoid this by choosing very carefully which tree gets attached.\nThere are many possible heuristic that can be used. Most popular are the following two approaches: In the first approach we use the size of the trees as rank, and in the second one we use the depth of the tree (more precisely, the upper bound on the tree depth, because the depth will get smaller when applying path compression).\nIn both approaches the essence of the optimization is the same: we attach the tree with the lower rank to the one with the bigger rank.\nHere is the implementation of union by size:\nvoid make_set(int v) {\n    parent[v] = v;\n    size[v] = 1;\n}\n\nvoid union_sets(int a, int b){\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (size[a] &lt; size[b])\n            swap(a, b);\n        parent[b] = a;\n        size[a] += size[b];\n    }\n}\nand here is the implementation of union by rank based on the depth of the trees:\nvoid make_set(int v) {\n    parent[v] = v;\n    rank[v] = 0;\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (rank[a] &lt; rank[b])\n            swap(a, b);\n        parent[b] = a;\n        if (rank[a] == rank[b])\n            rank[a]++;\n    }\n}\nBoth optimizations are equivalent in terms of time and space complexity. So in practice we can use any of them."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#time-complexity",
    "href": "programming/disjoint-set-union/index.html#time-complexity",
    "title": "Disjoint Set Union",
    "section": "Time complexity",
    "text": "Time complexity\nAs mentioned before, if we combine both optimizations - path compression with union by size / rank - we will reach nearly constant time queries. It turns out, that the final amortized complexity is \\(O(\\alpha(n))\\), where \\(\\alpha (n)\\) is the inverse Ackermann function, which grows very slowly. In fact it grows so lowly, that it doesn’t exceed 4 for all reasonable \\(n\\) (approximately \\(n&lt;10^{600}\\)).\nAmortized complexity is the total time per operation, evaluated over a sequence of multiple operations. The idea is to guarantee the total time of the entire sequence, while allowing single operations to be much slower than the amortized time. E.g. in our case a single call might take \\(O(\\log n)\\) in the worst case, but if we do \\(m\\) such calls back to back we will end up with an average time of \\(O(\\alpha (n))\\).\nWe will also not present a proof for this time complexity, since it is quite long and complicated.\nAlso, it’s worth mentioning that DSU with union by size / rank, but without path compression works in \\(O(\\log n)\\) time per query."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#linking-by-index-coin-flip-linking",
    "href": "programming/disjoint-set-union/index.html#linking-by-index-coin-flip-linking",
    "title": "Disjoint Set Union",
    "section": "Linking by index / coin-flip linking",
    "text": "Linking by index / coin-flip linking\nBoth union by rank and union by size require that we store additional data for each set, and maintain these values during each union operation. There exist also a randomized algorithm, that simplifies the union operation a little bit: linking by index.\nWe assign each set a random value called the index, and we attach the set with the smaller index to the one with the larger one. It is likely that a bigger set will have a bigger than the smaller set, therefore this operation is closely related to union by size. In fact it can be proven, that this operation has the same time complexity as union by size. However in practice it is slightly slower than union by size."
  },
  {
    "objectID": "programming/maximum-subarray-problem/index.html",
    "href": "programming/maximum-subarray-problem/index.html",
    "title": "Maximum subarray problem and Kadane’s algorithm",
    "section": "",
    "text": "In computer science, the maximum sum subarray problem, also known as the maximum segment sum problem, is the task of finding a contiguous subarray with the largest sum, within a given one-dimensional array A[1...n] of numbers. It can be solved in \\(O(n)\\) time and \\(O(1)\\) space.\nFormally, the task is to find indices \\(i\\) and \\(j\\) with \\(1\\leq i \\leq j \\leq n\\) such that the sum \\[\n\\Sigma^j_{x=i}A[x]\n\\] is as large as possible"
  },
  {
    "objectID": "programming/maximum-subarray-problem/index.html#brute-force-thoughts",
    "href": "programming/maximum-subarray-problem/index.html#brute-force-thoughts",
    "title": "Maximum subarray problem and Kadane’s algorithm",
    "section": "Brute-Force Thoughts",
    "text": "Brute-Force Thoughts\nA naive way to find the maximum subarray is:\n\nEnumerate all pairs of indices (i, j) with 0 ≤ i ≤ j &lt; n.\nCompute the sum of array[i..j].\nTrack the largest sum seen.\n\nThat requires \\(O(n^2)\\) subarrays and, if we sum each subarray from scratch, up to \\(O(n)\\) per sum, yielding \\(O(n^3)\\) time. We can improve one factor to \\(O(n^2)\\) by keeping a running sum when extending the end index, but that’s still too slow when n is large.\nWe want something like \\(O(n)\\) time."
  },
  {
    "objectID": "programming/maximum-subarray-problem/index.html#key-observation-intuition",
    "href": "programming/maximum-subarray-problem/index.html#key-observation-intuition",
    "title": "Maximum subarray problem and Kadane’s algorithm",
    "section": "Key Observation (Intuition)",
    "text": "Key Observation (Intuition)\n\nObservation: Suppose you want to know “What is the maximum-sum subarray that ends exactly at index i?” Once you know that, you could check all i and pick the best among them.\n\n\nDefine\n\\[dp[i] = \\text{the maximum subarray sum among all subarrays that end at index }i.\\] Our ultimate answer (global max) will be\n\\[\n  \\max_{0 \\le k &lt; n} dp[k].\n\\]\nHow do we compute dp[i] if we already know dp[i–1]? Consider any subarray that ends at i. It either:\n\nIs just the single element array[i] (i.e., we “start fresh” at i), or\n\nIs some subarray that ended at i−1 plus array[i] (i.e., we extend the best ending at i−1).\n\nIn other words: \\[\n  dp[i] \\;=\\; \\max\\bigl(\\; array[i],\\; dp[i-1] + array[i] \\bigr).\n\\]\n\nIf dp[i-1] (the best ending at i−1) is negative, then we’re better off “dropping” it and taking array[i] alone.\nIf dp[i-1] is positive (or zero), then extending it by adding array[i] only makes the sum larger.\n\n\nThat recurrence is exactly Kadane’s idea."
  },
  {
    "objectID": "programming/maximum-subarray-problem/index.html#deriving-kadanes-recurrence",
    "href": "programming/maximum-subarray-problem/index.html#deriving-kadanes-recurrence",
    "title": "Maximum subarray problem and Kadane’s algorithm",
    "section": "Deriving Kadane’s Recurrence",
    "text": "Deriving Kadane’s Recurrence\n\nDefine\n\\[dp[i] = \\text{max subarray sum ending exactly at index }i.\\]\nBase case:\n\\[dp[0] = array[0].\\]\nTransition: For each (i ), consider two possibilities for the subarray that ends at (i):\n\nStart a new subarray at (i). Its sum is (array[i]).\n\nExtend the best subarray ending at (i-1) by including the element at (i). Its sum is (dp[i-1] + array[i]).\n\nTherefore: \\[\n  dp[i] = \\max\\bigl(array[i],\\, dp[i-1] + array[i]\\bigr).\n\\]\nGlobal answer: As you fill these in from (i = 0) up to (n-1), keep track of \\[\n  \\text{global\\_max} = \\max_{0 \\le k &lt; n} dp[k].\n\\] That is the maximum sum among all possible ending-at-k subarrays, which necessarily includes the overall best subarray.\n\nBecause computing each dp[i] takes O(1) time, the entire process is O(n)."
  },
  {
    "objectID": "programming/maximum-subarray-problem/index.html#intuitive-explanation",
    "href": "programming/maximum-subarray-problem/index.html#intuitive-explanation",
    "title": "Maximum subarray problem and Kadane’s algorithm",
    "section": "Intuitive Explanation",
    "text": "Intuitive Explanation\n\nAs you sweep from left to right, maintain two values:\n\ncurrent_max = “best subarray sum ending exactly at the current position.”\nglobal_max = “best subarray sum seen so far anywhere.”\n\nWhen you arrive at a new element x = array[i], ask yourself:\n&gt; “If I want to pick a subarray that ends at i, is it better to (a) start fresh at i (just take x), or (b) stick with the best contiguous sum I had ending at i−1 and add x to it?”\n\nIf the best sum ending at i−1 was negative, adding x would only make it worse than just taking x alone.\n\nIf the best sum ending at i−1 was positive, adding x can only help (or at least not make it smaller than x).\n\nConcretely:\ncurrent_max = max(x, current_max + x)\nglobal_max  = max(global_max, current_max)\n\nand then move on to i+1.\nThink of it like this: whenever the running sum (best-ending-here) dips below zero, you toss it away and start over at the next index, because any prefix with negative sum would only drag down whatever comes after.\nfunction Kadane(array):\n    if array is empty:\n        return 0   // or some convention (e.g. negative infinity) depending on the problem\n\n    current_max = array[0]\n    global_max  = array[0]\n\n    for i from 1 to (n - 1):\n        x = array[i]\n        // Either extend the previous best subarray, or start new at i\n        current_max = max(x, current_max + x)\n\n        // Update global answer if needed\n        global_max = max(global_max, current_max)\n\n    return global_max\nInitialization: We set both current_max and global_max to array[0]. That handles the case where all numbers might be negative: the answer is the single largest element.\nLoop: At each step, update current_max using the recurrence. Then, if the new current_max is higher than any global_max we’ve seen so far, update global_max.\nReturn: By the end, global_max holds the largest sum of any contiguous subarray in the whole array."
  },
  {
    "objectID": "programming/maximum-subarray-problem/index.html#worked-example",
    "href": "programming/maximum-subarray-problem/index.html#worked-example",
    "title": "Maximum subarray problem and Kadane’s algorithm",
    "section": "Worked Example",
    "text": "Worked Example\nTake the array [-2, 1, -3, 4, -1, 2, 1, -5, 4]. Walk through Kadane’s steps:\n\n\n\n\n\n\n\n\n\n\ni\narray[i]\ncurrent_max (before)\ncurrent_max (after) = max(array[i], current_max₍i−1₎ + array[i])\nglobal_max\n\n\n\n\n0\n−2\n(init) −2\nmax(−2, —) = −2\n−2\n\n\n1\n+1\n−2\nmax( 1, (−2 + 1) = −1 ) = 1\n1\n\n\n2\n−3\n1\nmax(−3, (1 + (−3)) = −2 ) = −2\n1\n\n\n3\n+4\n−2\nmax( 4, (−2 + 4) = 2 ) = 4\n4\n\n\n4\n−1\n4\nmax(−1, (4 + (−1)) = 3 ) = 3\n4\n\n\n5\n+2\n3\nmax( 2, (3 + 2) = 5 ) = 5\n5\n\n\n6\n+1\n5\nmax( 1, (5 + 1) = 6 ) = 6\n6\n\n\n7\n−5\n6\nmax(−5, (6 + (−5)) = 1 ) = 1\n6\n\n\n8\n+4\n1\nmax( 4, (1 + 4) = 5 ) = 5\n6\n\n\n\nAt index 6, current_max becomes 6, and that’s the largest sum observed. Indeed, subarray [4, −1, 2, 1] ends at i=6 and has sum 6.\nBy index 7, adding −5 would drop the running sum to 1, but since 1 is still ≥ −5 itself, Kadane chooses 1 (i.e., “extend” rather than “start new”).\nBy index 8, it’s better to start new at 8 or to extend the 1? We compare:\n\n“Start new” at 8: sum = 4\n“Extend previous” (which was 1): sum = 1 + 4 = 5\n\n→ so we pick 5. However, 5 &lt; global_max = 6, so global_max remains 6."
  },
  {
    "objectID": "programming/maximum-subarray-problem/index.html#handling-all-negative-arrays",
    "href": "programming/maximum-subarray-problem/index.html#handling-all-negative-arrays",
    "title": "Maximum subarray problem and Kadane’s algorithm",
    "section": "Handling All-Negative Arrays",
    "text": "Handling All-Negative Arrays\nA common question: “What if the array is entirely negative, e.g. [-5, -3, -8]?”\nKadane’s initialization of\ncurrent_max = global_max = array[0]\nautomatically handles that. We never zero out a running sum unless it’s worse than starting fresh. So if all numbers are negative, every time you consider a new element x, you compute\ncurrent_max = max(x, current_max + x)\nSince current_max + x is even more negative than x alone, the recurrence forces current_max to be the largest single negative element encountered so far. Thus the global max ends up being the least-negative (i.e., the “largest”) element of the array."
  },
  {
    "objectID": "programming/maximum-subarray-problem/index.html#final-remarks",
    "href": "programming/maximum-subarray-problem/index.html#final-remarks",
    "title": "Maximum subarray problem and Kadane’s algorithm",
    "section": "Final Remarks",
    "text": "Final Remarks\n\nTime complexity: O(n), since each element is processed exactly once with O(1) work per element.\nSpace complexity: O(1) extra space if you only keep two scalars (current_max, global_max). (If you store the entire dp[] array, it’s O(n), but you don’t need to; you only ever use the “previous” value.)\n\nKadane’s algorithm is essentially a specialized form of dynamic programming that keeps track of a “running best suffix sum” and resets whenever that suffix sum would be negative. Once you grasp that the maximum-sum subarray ending at i is either “continue from i−1” or “start anew at i,” the rest follows naturally."
  },
  {
    "objectID": "programming/leetcode-house-robber/house-robber.html",
    "href": "programming/leetcode-house-robber/house-robber.html",
    "title": "House robber",
    "section": "",
    "text": "Topic: array, dymanic programming\n\nQuestion\nYou are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and it will automatically contact the police if two adjacent houses were broken into on the same night.\nGiven an integer array nums representing the amount of money of each house, return the maximum amount of money you can rob tonight without alerting the police.\nExample 1:\n\nInput: nums = [1,2,3,1]\nOutput: 4\nExplanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4.\n\nExample 2:\n\nInput: nums = [2,7,9,3,1]\nOutput: 12\nExplanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1). Total amount you can rob = 2 + 9 + 1 = 12.\n\nConstraints:\n1 &lt;= nums.length &lt;= 100\n0 &lt;= nums[i] &lt;= 400\n\n\nApproaches"
  },
  {
    "objectID": "programming/leetcode-minimum-cost-walk-in-weight-graph/index.html",
    "href": "programming/leetcode-minimum-cost-walk-in-weight-graph/index.html",
    "title": "Minimum Cost Walk in Weighted Graph",
    "section": "",
    "text": "Question\nThere is an undirected weighted graph with n vertices labeled from 0 to n - 1.\nYou are given the integer n and an array edges, where edges[i] = [ui, vi, wi] indicates that there is an edge between vertices ui and vi with a weight of wi.\nA walk on a graph is a sequence of vertices and edges. The walk starts and ends with a vertex, and each edge connects the vertex that comes before it and the vertex that comes after it. It’s important to note that a walk may visit the same edge or vertex more than once.\nThe cost of a walk starting at node u and ending at node v is defined as the bitwise AND of the weights of the edges traversed during the walk. In other words, if the sequence of edge weights encountered during the walk is w0, w1, w2, …, wk, then the cost is calculated as w0 & w1 & w2 & … & wk, where & denotes the bitwise AND operator.\nYou are also given a 2D array query, where query[i] = [si, ti]. For each query, you need to find the minimum cost of the walk starting at vertex si and ending at vertex ti. If there exists no such walk, the answer is -1.\nReturn the array answer, where answer[i] denotes the minimum cost of a walk for query i.\nExample 1:\nInput: n = 5, edges = [[0,1,7],[1,3,7],[1,2,1]], query = [[0,3],[3,4]]\nOutput: [1,-1]\nExplanation:\nTo achieve the cost of 1 in the first query, we need to move on the following edges: 0-&gt;1 (weight 7), 1-&gt;2 (weight 1), 2-&gt;1 (weight 1), 1-&gt;3 (weight 7).\nIn the second query, there is no walk between nodes 3 and 4, so the answer is -1.\nExample 2:\nInput: n = 3, edges = [[0,2,7],[0,1,15],[1,2,6],[1,2,1]], query = [[1,2]]\nOutput: [0]\nExplanation:\nTo achieve the cost of 0 in the first query, we need to move on the following edges: 1-&gt;2 (weight 1), 2-&gt;1 (weight 6), 1-&gt;2 (weight 1).\nConstraints:\n2 &lt;= n &lt;= 105\n0 &lt;= edges.length &lt;= 105\nedges[i].length == 3\n0 &lt;= ui, vi &lt;= n - 1\nui != vi\n0 &lt;= wi &lt;= 105\n1 &lt;= query.length &lt;= 105\nquery[i].length == 2\n0 &lt;= si, ti &lt;= n - 1\nsi != ti\n\n\nAnalysis\nLet’s determine when the answer to a query is -1. This happens when no walk exists between the two nodes, meaning they belong to different connected components.\nNow, suppose the two nodes belong to the same connected component. What is the minimum cost of a walk connecting them? As mentioned, the optimal walk includes as many edges as possible. Since revisiting an edge does not affect the total score, we can freely traverse the edges of the component, meaning that we can move back and forth to reach all of them. Therefore, the best way to achieve the lowest cost is to visit every edge in the component.\nTo efficiently find and process the connected components of the graph, we use the Disjoint Set (Union-Find) data structure. When we Union two nodes, we merge their entire groups, as now a path exists between every node in one group and every node in the other. To maintain efficiency, the root of the larger group is chosen as the representative of the merge group. This minimizes the time needed for future Find operations by reducing the number of steps required to reach the current representative."
  },
  {
    "objectID": "programming/leetcode-count-good-triplets/index.html",
    "href": "programming/leetcode-count-good-triplets/index.html",
    "title": "Count good triplets",
    "section": "",
    "text": "Given an array of integers arr, and three integers a, b and c. You need to find the number of good triplets.\nA triplet (arr[i], arr[j], arr[k]) is good if the following conditions are true:\n\n0 &lt;= i &lt; j &lt; k &lt; arr.length\n|arr[i] - arr[j]| &lt;= a\n|arr[j] - arr[k]| &lt;= b\n|arr[i] - arr[k]| &lt;= c\n\nWhere |x| denotes the absolute value of x.\nReturn the number of good triplets.\nExample 1:\nInput: arr = [3,0,1,1,9,7], a = 7, b = 2, c = 3 Output: 4 Explanation: There are 4 good triplets: [(3,0,1), (3,0,1), (3,1,1), (0,1,1)].\nExample 2:\nInput: arr = [1,1,2,2,3], a = 0, b = 0, c = 1 Output: 0 Explanation: No triplet satisfies all conditions.\nConstraints:\n\n3 &lt;= arr.length &lt;= 100\n0 &lt;= arr[i] &lt;= 1000\n0 &lt;= a, b, c &lt;= 1000"
  },
  {
    "objectID": "programming/leetcode-count-good-triplets/index.html#approach-2-optimized-enumeration",
    "href": "programming/leetcode-count-good-triplets/index.html#approach-2-optimized-enumeration",
    "title": "Count good triplets",
    "section": "Approach 2: Optimized enumeration",
    "text": "Approach 2: Optimized enumeration\nWe need to eliminate one loop or make one of them constant-time.\nWe could try fixing (j,k) and then counting valid i &lt; j. That \\(O(n^2)\\) pairs, and if we can count the \\(i\\)s in \\(O(1)\\) each, we down to \\(O(n^2)\\) total.\n\nTranslate the \\(i\\)-constraints into an interval\nFor a given \\((j,k)\\), the condition on \\(i\\) are \\[|arr[i] - arr[j]| \\leq a\\] so, \\[arr[i] \\in [arr[j]-a, arr[j]+a]\\]\nand \\[|arr[i]-arr[k]|\\leq c\\] so, \\[arr[i]\\in[arr[k]-c, arr[k]+c]\\]\nTheir intersection is a single interval \\([l, r]\\). So we just need to count how many prior \\(i&lt;j\\) have \\(arr[i] \\in [l, r]\\).\n\n\nChoose a data structure for last range-count\nSince arr[i] is small-range, maintain a frequency array freq[0..M] for all seen indices \\(&lt;j\\), and its prefix sums sum[v] =\\(\\Sigma_{u=0}^v freq[u]\\)\nThen the count in \\([l, r]\\) is simply sum[r]-sum[l-1] in \\(O(1)\\).\n\n\nEnsure the \\(i&lt;j\\) ordering\nWe iterate \\(j\\) from 1 to \\(n\\). Before handling any \\((j, k)\\) pairs, our freq/sum reflects exact indices 1 through \\(j-1\\).\nFor each \\(k&gt;j\\), check \\(|arr[j] - arr[k]|\\leq b\\). If it passes, compute \\([l, r]\\) and do the \\(O(1)\\) range-sum query.\nAfter finishing all \\(k\\) for this \\(j\\), we insert \\(arr[j]\\) into freq (and update sum), before moving onto \\(j + 1\\).\nclass Solution:\n    def countGoodTriplets(self, arr: List[int], a: int, b: int, c: int) -&gt; int:\n        # ans: total count of valid (i,j,k)\n        ans = 0\n        \n        n = len(arr)\n        # total[v] will hold the prefix-sum of frequencies:\n        #   total[v] = number of i&lt;j with arr[i] ≤ v\n        # we assume arr[i] ∈ [0..1000], so we size total to 1001\n        total = [0] * 1001\n\n        # Move j from 0 to n-1\n        for j in range(n):\n            # Pair j with every k&gt;j\n            for k in range(j + 1, n):\n                # First check the |arr[j] - arr[k]| ≤ b constraint\n                if abs(arr[j] - arr[k]) &lt;= b:\n                    # Compute the interval of allowed arr[i] from j’s constraint\n                    lj, rj = arr[j] - a, arr[j] + a\n                    # Compute the interval of allowed arr[i] from k’s constraint\n                    lk, rk = arr[k] - c, arr[k] + c\n\n                    # Intersection [l..r] of the two intervals\n                    # also clamp to [0..1000] to stay in array bounds\n                    l = max(0, lj, lk)\n                    r = min(1000, rj, rk)\n\n                    # If the intersection is non-empty, count how many\n                    # prior i&lt;j have arr[i] in [l..r] via prefix sums\n                    if l &lt;= r:\n                        if l == 0:\n                            ans += total[r]\n                        else:\n                            ans += total[r] - total[l - 1]\n\n            # After processing all k for this j, we “add” arr[j] into our\n            # prefix-sum structure so that future iterations see it.\n            # We do this by incrementing total[v] for all v ≥ arr[j].\n            # That way total[v] remains = # of arr[i] ≤ v for i&lt;next j.\n            for v in range(arr[j], 1001):\n                total[v] += 1\n\n        return ans"
  },
  {
    "objectID": "programming/leetcode-find-closest-node-given-2-nodes/index.html",
    "href": "programming/leetcode-find-closest-node-given-2-nodes/index.html",
    "title": "2359. Find Closest Node to Given Two Nodes",
    "section": "",
    "text": "Topic: Graph, Deep-First Search\n\nQuestion\nYou are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and it will automatically contact the police if two adjacent houses were broken into on the same night.\nGiven an integer array nums representing the amount of money of each house, return the maximum amount of money you can rob tonight without alerting the police.\nExample 1:\n\nInput: nums = [1,2,3,1]\nOutput: 4\nExplanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4.\n\nExample 2:\n\nInput: nums = [2,7,9,3,1]\nOutput: 12\nExplanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1). Total amount you can rob = 2 + 9 + 1 = 12.\n\nConstraints:\n1 &lt;= nums.length &lt;= 100\n0 &lt;= nums[i] &lt;= 400\n\n\nApproaches"
  },
  {
    "objectID": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html",
    "href": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html",
    "title": "How to approach most DP problems",
    "section": "",
    "text": "To solve a dp problem: https://leetcode.com/problems/house-robber/solutions/156523/from-good-to-great-how-to-approach-most-of-dp-problems/\nThis particular problem can be approached using the following sequence:"
  },
  {
    "objectID": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html#step-5-iterative-2-variables-bottom-up",
    "href": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html#step-5-iterative-2-variables-bottom-up",
    "title": "How to approach most DP problems",
    "section": "Step 5: Iterative + 2 variables (bottom-up)",
    "text": "Step 5: Iterative + 2 variables (bottom-up)\nIn the previous step, we use only memo[i] and memo[i-1], so going just 2 step back. We can hold them in 2 variables instead. This optimization is met in Fibonacci sequence creation and some other problems [[Optimize Fibonacci]]\n    def rob(self, nums: List[int]) -&gt; int:\n        if len(nums) == 0:\n            return 0\n        prev1, prev2 = 0, 0\n        for num in nums:\n            temp = prev1\n            prev1 = max(prev2 + num, prev1)\n            prev2 = temp\n        return prev1"
  },
  {
    "objectID": "carried over.html",
    "href": "carried over.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "to use or do something at a later time than planned:\nThe performance has had to be carried over to/till next week because the repairs to the theatre aren’t finished yet."
  },
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "Programming",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 12, 2025\n\n\nLogistic regression\n\n\nYour name\n\n\n\n\nJun 2, 2025\n\n\nMaximum subarray problem and Kadane’s algorithm\n\n\nHung \n\n\n\n\nMay 30, 2025\n\n\nFinetuning\n\n\nHung \n\n\n\n\nMay 30, 2025\n\n\n2359. Find Closest Node to Given Two Nodes\n\n\nHung \n\n\n\n\nMay 30, 2025\n\n\n127. Word Ladder\n\n\nHung \n\n\n\n\nMay 7, 2025\n\n\nRole of Parallel Computing\n\n\nHung \n\n\n\n\nApr 15, 2025\n\n\nCount good triplets\n\n\nHung \n\n\n\n\nApr 4, 2025\n\n\nMaximum subarray\n\n\nHung \n\n\n\n\nMar 21, 2025\n\n\nImage classification\n\n\n \n\n\n\n\nMar 21, 2025\n\n\nDocker basics\n\n\nKim Hung Bui\n\n\n\n\nMar 21, 2025\n\n\nDisjoint Set Union\n\n\nKim Hung Bui\n\n\n\n\nMar 21, 2025\n\n\nMinimum Cost Walk in Weighted Graph\n\n\nKim Hung Bui\n\n\n\n\nMar 20, 2025\n\n\n3191. Minimum Operations to Make Binary Array Elements Equal to One I\n\n\nHung \n\n\n\n\nMar 20, 2025\n\n\nParity invariance\n\n\nHung \n\n\n\n\nMar 18, 2025\n\n\nleetcode-Longest-Nice-Subarray\n\n\nHung \n\n\n\n\nMar 17, 2025\n\n\nDivide Array Into Equal Pairs\n\n\nHung \n\n\n\n\nMar 17, 2025\n\n\nHouse robber\n\n\nHung \n\n\n\n\nMar 17, 2025\n\n\nHow to approach most DP problems\n\n\nKim Hung Bui\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "ANTI-CAPITALIST SOFTWARE LICENSE (v 1.4)\nCopyright © 2024 Erick Ratamero\nThis is anti-capitalist software, released for free use by individuals and organizations that do not operate by capitalist principles.\nPermission is hereby granted, free of charge, to any person or organization (the “User”) obtaining a copy of this software and associated documentation files (the “Software”), to use, copy, modify, merge, distribute, and/or sell copies of the Software, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or modified versions of the Software.\nThe User is one of the following:\n\n\nAn individual person, laboring for themselves\nA non-profit organization\nAn educational institution\nAn organization that seeks shared profit for all of its members, and allows non-members to set the cost of their labor\n\n\nIf the User is an organization with owners, then all owners are workers and all workers are owners with equal equity and/or equal vote.\nIf the User is an organization, then the User is not law enforcement or military, or working for or under either.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "english/english-farm-animals-vegetables/index.html",
    "href": "english/english-farm-animals-vegetables/index.html",
    "title": "Farm animals and Vegetables",
    "section": "",
    "text": "It has been suggested that primary children should learn how to grow vegetables and keep animals.\nDo the advantages of this outweigh the disadvantages?\nDavid Lang’s essay:\nSome today have argued that it would be a sensible policy to teach children how to farm and raise animals at school. In my opinion, though this would require enormous resources, it would be beneficial in terms of both physical and intellectual development.\nThose arguing against this proposal point out the resources needed. In order to implement such an ambitious programs, schools would firstly need land nearby where animals and crops be raised. This might be possible within a school’s pre-existing grounds, but there would still need to be significant investment in infrastructure. Schools would therefore have to reallocate a significant portion of their budget, potentially reducing the money for crucial facilities, staff, and teaching resources.\nHowever, the advantages for the holistic development of children is overwhelming. Students today spend more time than ever before passively using technological devices. By spending more time outdoors learning about animals and crops, students will better understand their position within the natural food chain. Overtime, this may lead them to adopt environmentally friendlier, compassionate lifestyle choices such as becoming vegetarians or shopping locally rather than buying from large chains. Moreover, the work outdoors would necessarily involve physical exercise. Since obesity is a growing concern in most countries, this time outside could help to improve standards of physical fitness and establish healthy lifelong habits.\nIn conclusion, despite the money this reform would require, schools should adopt it when possible so as to achieve a more complete education for students. Governments should consider the long-term effects when deciding on such policies.\n\nChat gpt\nThe proposal to incorporate agricultural and animal husbandry education into primary school curricula has garnered attention. In my view, while such initiatives offer certain benefits, the potential drawbacks, particularly concerning resource allocation and health risks, are more significant.\nAdvocates highlight that engaging children in growing vegetables and caring for animals can foster responsibility and a connection to nature. These activities may enhance students’ understanding of food sources and promote environmental stewardship. However, implementing such programs requires substantial resources. Schools would need to invest in appropriate land, infrastructure, and trained personnel to oversee these activities. This financial commitment could divert funds from essential academic subjects, potentially compromising the quality of core education.\nMoreover, there are health and safety concerns associated with young children handling soil and animals. Exposure to allergens, zoonotic diseases, and physical injuries are risks that cannot be overlooked. Ensuring proper hygiene and safety measures would necessitate additional oversight and training, further straining school resources.\nIn conclusion, while teaching primary children to cultivate plants and care for animals has educational merits, the associated disadvantages, particularly in terms of resource allocation and health risks, outweigh the benefits. Schools should prioritize core academic subjects and consider integrating environmental education through less resource-intensive methods.\n💡 Why David Lang’s Essay Feels Simple but Scores High\n\nClear structure\n\nIntroduction: Directly paraphrases the question + gives a clear opinion.\nBody 1: Discusses the opposite view.\nBody 2: Presents his own view (which he supports).\nConclusion: Restates the opinion and wraps up.\n👉 Why it works: Examiners love clarity. They’re not grading you on how “fancy” your structure is — just that it’s logical and easy to follow.\n\nPrecise vocabulary, not fancy vocabulary\n\nHe uses natural, topic-specific language, like:\n“holistic development”\n“environmentally friendlier”\n“passively using technological devices”\n👉 Why it works: Band 9 vocabulary is about being accurate and appropriate, not sounding like a thesaurus.\n\nBalanced ideas + developed examples\n\nHe acknowledges the disadvantages (cost, resources) and then shows why advantages outweigh them (fitness, habits, awareness). He explains his points — not just states them.\n👉 Why it works: IELTS cares about depth more than how many ideas you list. One strong idea with support = better than five weak ones.\n\nGrammar is natural and controlled\n\nHe uses complex sentences when needed, not to show off.\n👉 Example:\n“Since obesity is a growing concern in most countries, this time outside could help…”\nThat’s complex, but clear. No over-complicated structures.\n✅ How YOU Can Write Like David Lang\n\nFollow a clean structure\n\nIntro: Paraphrase + opinion\nBody 1: Opposing view\nBody 2: Your view (with examples)\nConclusion: Restate opinion\n\nUse precise, topic-relevant words\n\nLearn how to talk naturally about big topics (education, health, environment) with words that fit.\n\nDon’t list ideas — explain them\n\nPick 1–2 strong points per paragraph and expand with why, how, and an example.\n\nWrite like you’re explaining, not performing\n\nPretend you’re talking to a smart friend, not an examiner. Make it natural and reasoned, not robotic or flowery.\n✏️ Step 1: Write the Introduction\n✅ What we need:\nParaphrase the question\nGive a clear opinion\n🧠 Example:\nSome believe that it would be beneficial for young children to be taught farming skills at school. However, I believe that the drawbacks of such a policy outweigh the potential benefits.\n🔍 Why this works:\nIt’s short.\nClear opinion.\nNo fancy words. Just focused.\nWould you like to try writing your own version of the introduction before we move on?\nOr should we go ahead to Body Paragraph 1?"
  },
  {
    "objectID": "english/household-work/index.html",
    "href": "english/household-work/index.html",
    "title": "Ielts task 1 - Household work",
    "section": "",
    "text": "The bar chart below shows the average duration of household women did (unemployed, part-time employed and full-time) when compared to men who had full-time work in the UK between 1985 and 2005.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\nThe bar chart details hours of housework completed in the UK from 1985 to 2005 by fully employed males and females with various states of employment. Looking from an overall perspective, it is readily apparent that time spent on housework rose for all groups with the exception of unemployed women, which was stable overall. However, the latter category undertook by far the most housework, while men with full-time jobs did comparatively little.\nUnemployed women led throughout the period in terms of hours, at a peak of 7 hours in 1985 and 2005 and a slight dip to 6.5 hours in between. In contrast, men with full-time work were unchanged at just 1 hour in 1985 and 1995, though there was a slight rise to 1.5 hours in 2005.\nWomen with part-time employment were slightly lower than their unemployed counterparts throughout the period, beginning at 5.3 hours in 1985, before declining to 5 hours over the next 5 years, and then finishing the period with a surge to 6 hours. The figures for women with full-time employment resembled those of men, though they were marginally higher at 2, 1.5, and 2.8 hours in 1985, 1995, and 2005, respectively."
  },
  {
    "objectID": "english/household-work/index.html#part-3",
    "href": "english/household-work/index.html#part-3",
    "title": "Ielts task 1 - Household work",
    "section": "Part 3",
    "text": "Part 3\n\nDo you think tidiness is important in the workplace? Absolutely. A tidy workplace not only creates a more professional image but also boosts efficiency. When things are organized, it’s easier to find what you need, which saves time and reduce stress. Moreover, a clean environment can promote better focus and even improve team morale.\nWhy do some people struggle to stay tidy? There could be a few reasons. Some people simply lead very busy lives and don’t prioritize tidiness, while others may not have developed habits of organization from a young age. In some cases, people might not even notice the mess around them - they’ve grown accustomed to it. For a few, it could also be tied to mental health challenges, like anxiety or ADHD, which can effect one’s ability to maintain order.\nHow can parents teach children to be tidy? The most effective way is by setting a good example. If children see their parents consistently keeping things organized, they’re more likely to adopt those habits themselves. It also helps to make tidying part of a daily routine and turn it into something fun or rewarding - for instance, though games or praise. Teaching them to take pride in their space can go a long way.\nDo you think technology helps people stay more organized and tidy? Definitely. There are so many apps and digital tools now that can help people manages their time, declutter digitally, and keep track of tasks or schedules. Even simple things like calendar reminder or to-do list apps can encourage tidier habits. That said, technology can also contribute to digital clutter if not used mindfully.\nIs there a difference in tidiness between younger and older generations? In some ways, yes. Older generations often place a high value on physical tidiness, perhaps because they grew up in a time when everything had to be done manually. Younger people, on the other hand, might prioritize digital organization - like keeping their phones or laptops neatly arranged. However, i think personal habits play a bigger role than age."
  },
  {
    "objectID": "english/two-question-essay/index.html",
    "href": "english/two-question-essay/index.html",
    "title": "How to Structure a ‘Two Question’ Essay",
    "section": "",
    "text": "Let’s look at an example of this kind of question:\n*Nowadays governments are investing more in public transport such as buses and trains rather than in building new roads.*\n*What are the reasons for this?*\n*Is this a positive or negative development?*\n\nClear position\nFor this kind of task we need to discuss both questions. In the first question, we are given a situation and asked to explain why. The second question requires a clear opinion so we must choose a side. This is very important for our task response score. If we don’t choose a side and support that ‘position’ clearly, we will lose marks for our task response. We will get a maximum of 5 for task achievement if we do not choose a side.\nIt’s not important which side we choose as long as we choose one. Here are the options for the second questions:\n\nAgree: We think it’s a positive development that governments are spending more money on public transport. We support this opinion.\nDisagree: We think it’s a negative development that governments are spending more money on public transport. We support this opinion.\n\n\n\nClear essay structure\nDavid Lang recommend that we should give our opinion in the introduction as well as the conclusion (for all IELTS Task 2 questions). It’s OK to write a very general introduction and leave our opinion to the end. But it’s not as clear and we might run out of time before we’ve hot to our conclusion, so overall it’s a risky strategy. If we don’t choose a side and support that ‘position’ clearly, we will lose marks for our task response.\nTherefore, him suggested essay structure is four paragraphs. The opinion is presented in the introduction and re-stated in the conclusion. Ideally each body paragraph in the introduction and re-stated in the conclusion.\n\n\nPossible Essay structures\n1. Agree - we feel it’s a positive development that governments are spending more money on public transport.\nWe need at least one good reason why it’s happening and one good reason why it’s a positive development.\n\n\n\n\n\n\n\nParagraph 1 (intro)\nParaphrase both questions and state your overall opinion (you feel it’s a positive development)\n\n\nParagraph 2 (body 1)\nExplain and support your reason why governments are spending more money on public transport.\n\n\nParagraph 3 (body 2)\nExplain and support your reason why this is a positive development.\n\n\nParagraph 4 (conclusion)\nRe-state your overall opinion (you feel it’s a positive development) and  summarise your main reasons.\n\n\n\n2. Disagree: - we feel it’s a negative development that governments are spending more money on public transport.\nWe need at least one good reason why it’s happening and one good reason why it’s a negative development. Only the parts highlighted below are different from the first essay\n\n\n\n\n\n\n\n#### Paragraph 1 (intro)\n#### Paraphrase both questions and state your overall opinion (you feel it’s a negative development)\n\n\n#### Paragraph 2 (body 1)\n#### Explain and support your reason why governments are spending more money on public transport.\n\n\n#### Paragraph 3 (body 2)\n#### Explain and support your reason why this is a negative development.\n\n\n#### Paragraph 4 (conclusion)\n#### Re-state your overall opinion (you feel it’s a negative development) and  summarise your main reasons."
  },
  {
    "objectID": "english/having-children-at-older-age/index.html",
    "href": "english/having-children-at-older-age/index.html",
    "title": "IELTS Essay Having Children at an Older Age",
    "section": "",
    "text": "Question:\nSome people choose to have their first child at an older age. What are the reasons? Do the advantages outweigh the disadvantages?\nDavid Lang’s sample In recent years, an increasing number of individuals have opted to start their families later in life. This trend raises the question of why more people are choosing to have their first child at an older age and whether the benefits of doing so justify the drawbacks.\nOne of the primary reasons for delaying parenthood is the pursuit of career stability in a worrying economic climate. In today’s competitive job market, many individuals, particularly women, prefer to establish themselves professionally before starting a family. By focusing on their careers in their twenties and early thirties, they can achieve financial security and attain higher positions, which often translates to job satisfaction and greater resources for their future children. This growing trend is reflected in how mainstream society now places significant emphasis on personal development and achieving personal goals before embarking on parenthood. For that reason, many young adults prioritize their own personal and financial welfare, a situation that engenders an atmosphere less conductive to having children early in life.\nThere are legitimate disadvantages connected with having children later in life. One of the primary concerns is the potential health risks for both the mother and the child. Advanced maternal age is often associated with a higher risk of complications such as gestational diabetes, hypertension, and chromosomal abnormalities. However, despite these drawbacks, the advantages of having children later in life due to greater maturity overshadow the hypothetical drawbacks. The emotional maturity and life experience that older parents bring to child-rearing can result in a more stable and nurturing environment. Older parents are likely to be more financially secure and better prepared to provide for their children’s education and well-being. The ability to make well-considered decisions and the patience developed over time can contribute to a more thoughtful and deliberate approach to parenting.\nIn conclusion, while there are valid concerns regarding the health risks associated with having children later in life, the benefits, such as financial stability, emotional maturity, and better preparation for parenthood, outweigh these disadvantages. As societal norms and medical advancements continue to evolve, the rend of delaying parenthood is likely to remain prevalent, reflecting the changing priorities and values of modern individuals.\nSample, disadvantages over advantages.\nAn increasing number of individuals are choosing to have their first child at an older age. This trend is driven by factors such as career prioritization, financial security, and a desire for greater personal maturity. While some argue that the benefits of delayed parenthood - like improved financial stability and enhanced life experience - outweigh the disadvantages, I strongly disagree. The potential health risks, fertility issues, and other practical challenges mean that the disadvantages ultimately outweigh any advantages.\nModern society has shifted priorities, and many couples now postpone parenthood for several reasons. Primarily, individuals strive to establish their careers and achieve financial stability before starting a family. This approach allows prospective parents to create a secure environment for their child, free from economic uncertainty. Furthermore, many believe that greater life experience and emotional maturity can lead to more thoughtful parenting decisions. As a result, the decision to delay having a first child is often seen as a strategic move to optimize one’s personal and professional circumstances.\nDespite these reasons, the drawbacks of having a first child at an older age are significant. Health concerns are paramount; older mothers face increased risks of pregnancy complications, such as gestational diabetes and hypertension, which can jeopardize both maternal and infant health. Additionally, advancing age is closely linked to decreased fertility, often necessitating expensive and emotionally draining fertility treatments. Beyond health, older parents might experience reduced physical stamina, which is crucial for the demanding nature of childcare, and a larger generational gap that can affect the parent-child relationship. These challenges make the prospect of delayed parenthood less attractive, as the risks and long-term implications overshadow the benefits of waiting.\nIn conclusion, although delaying parenthood may provide some advantages in terms of financial and emotional preparedness, the significant health risks, fertility challenges, and practical issues associated with having a first child later in life outweigh these benefit. Therefore, i contend that the disadvantages of postponing parenthood ultimately make it a less favorable option compared to earlier family planning.\nSample, advantages over disadvantages.\nIn recent years, an increasing number of individuals have chosen to have their first child at an older age. This trend is largely influenced by factors such as the pursuit of career goals, the desire for financial stability, and the accumulation of life experience before entering parenthood. I firmly believe that these advantages not only lead to more secure and thoughtful family planning but also contribute to improved parenting outcomes, ultimately outweighing the potential disadvantages.\nModern societal shifts have prompted many prospective parents to postpone starting a family. Primarily, individuals are keen to establish a stable career and secure a robust financial foundation before welcoming a child. This approach allows them to create a nurturing environment where the child’s needs are met without the stress of economic uncertainty. Furthermore, waiting until later in life often results in enhanced emotional maturity and life experience, enabling parents to make more considered decisions and effectively manage the challenges of raising a child. These reasons underscore why many see value in delaying parenthood as a strategic, long-term investment in both personal and familial well-being.\nThe advantages of choosing to have a child at an older age are manifold. With a well-established career and financial security, older parents are better equipped to provide for their child’s educational and extracurricular needs, contributing to the child’s overall development. In addition, greater maturity and life experience often translate into more patient, empathetic, and balanced parenting. Such parents typically possess enhanced problem-solving skills and resilience, which can be invaluable when navigating the inevitable challenges of childcare. Moreover, studies have shown that older parents tend to invest more time and resources in their children’s upbringing, resulting in a more stable and nurturing home environment. Collectively, these advantages not only promote healthier family dynamics but also pave the way for the child’s long-term success and well-being.\nIn conclusion, while there are inherent challenges associated with delayed parenthood, the benefits- ranging from financial stability and career establishment to increased emotional maturity and effective parenting- clearly outweigh the drawbacks. By choosing to have a child at an older age, many individuals are better prepared to offer a secure, supportive, and enriching environment for their children, ultimately leading to more positive outcomes for families."
  },
  {
    "objectID": "english/driverless-vehicles/index.html",
    "href": "english/driverless-vehicles/index.html",
    "title": "IELTS Cambridge 16 Essay - Driverless Vehicles",
    "section": "",
    "text": "In the future all cars, buses and trucks will be driverless. The only people travelling inside these vehicles will be passengers.\nDo you think the advantages of driverless vehicles outweigh the disadvantages?\nA future filled with driverless vehicles is an inevitability. In my opinion, though there are concomitant moral risks, the practical import for health such a seismic shift will be positive on the whole.\nCritics of this trend suggest that machines should not be responsible for the potential loss of human life. This argument rests on the fact when there is an accident involving humans, it is possible to either assess blame and assign punishment or accept that human error played a role. In contrast, an individual severely injured as the result of a collision caused by a machine may justly feel that all human autonomy and free will is absent from their fate. This is analogous to a natural disaster with the crucial distinction being that humanity has engineered the situation. It is human nature to prefer to control a situation rather than surrender the outcome to an autonomous machine.\nProponents of automated vehicles, on the other hand, argue the tangible ramifications concerning public safety. The evidence overwhelmingly suggests that driverless vehicles cause fewer accidents than humans. The reason for this is that humans are prone to errors related to fatigue, distractions such as smartphones, and, in some cases, altered states. Machines might experience the occasional technical error, however, they are far more consistent by comparison. In fact, the few accidents that have occurred with automated vehicles were the fault of human drivers. If all cars were driverless, this would enabled a greater\n\nIn the coming years, it is predicted that all vehicles, including cars, buses, and trucks, will operate autonomously without human drivers. While this development may raise some concerns, I firmly believe that the advantages of driverless transportation far outweigh the disadvantages.\nOne major advantage of autonomous vehicles is improved road safety. The majority of traffic accidents today are caused by human error, including distracted driving, fatigue, and impaired judgment. Driverless vehicles, on the other hand, rely on advanced sensors, algorithms, and machine learning, which allow them to make precise decisions and maintain constant awareness of their surroundings, can process vast amount of data in real time and react faster than any human driver. This can greatly reduce accidents, save lives, and lower the burden on healthcare systems. In addition, driverless technology could improve traffic flow and reduce congestion by enabling vehicles to communicate and coordinate more efficiency.\nHowever, this transformation is not without drawbacks. A major concern is job loss among professional drivers, including taxi drivers, truckers, and bus operators. The widespread adoption of autonomous vehicles could lead to large-scale unemployment in these sectors. Moreover, ethical challenges arise in situations where a vehicle must choose between two harmful outcomes, raising questions about how machines should be programmed to make such decisions.\nDespite these issues, I believe that the long-term benefits are more compelling. The reduction in traffic accidents, the potential for greater mobility for elderly and disabled individuals, and the increase in productivity for commuters are all powerful advantages. With proactive government policies, including retraining programs and strong ethical guidelines for AI systems, the negative impacts can be mitigated.\nIn conclusion, although there are valid concerns about the rise of driverless vehicles, particularly regarding employment and ethics, the positive impacts on safety, efficiency, and accessibility make this a highly promising development. Therefore, I believe the advantages clearly outweight the disadvantages."
  },
  {
    "objectID": "english/essay-agriculture/index.html",
    "href": "english/essay-agriculture/index.html",
    "title": "Agriculture",
    "section": "",
    "text": "In spite of the advances made in agriculture, many people around the world still go hungry. Why is this the case? What can be done about this problem? Give reasons for your answer and include any relevant examples from your own experience or knowledge. Write at least 250 words.\n\nRecent years have witnessed tremendous developments in agricultural science yet every day people all over the world continue to suffer and even die from hunger. In my opinion, this is because innovations rarely impact the developing world and the best solution for it is to direct humanitarian funding towards these advances.\nThe main cause of more advanced agricultural methods not reaching the people who need it the most is that the developing world has trouble implementing these methods. There are a number of reasons for this ranging from limited financial resources to poor existing infrastructure to political instability, depending on the country in question. One example of this would be in many African nations, where malnourishment has historically been highest. Countries like the Congo have seen revolution after revolution over the last several decades, which has effectively destabilized the entire country. The universities where students would learn about changes in agriculture are frequently shut down or destroyed. The young people who would become agricultural scientists end up drawn into the conflict as soldiers or victims. Long-term economic neglect means that the government has very limited ability to subsidies farming. These problems are present to varying degrees in countries around the world and offer one possible explanation for the continued prevalence of hunger.\nThe solution that I believe would be most effective is directing humanitarian funds previously focused on food aid towards education and agriculture. Food aid is a notoriously poor solution because it only offers an immediate solution and warlords often exploit it to support their continued mistreatment of their people. It hurts more than it helps. However, there would be better long-term effects if international organizations and governments redirects that money into helping build better farms, provide more modern equipment, and sending qualified professions to train people in need. There is a TedTalk by a young man in Kenya who built his own windmill out of old bicycle parts and reading a book in the local library. It is large enough to power irrigation channels for the entire village. If more people like him can be found or trained when this will have an impact that lasts for decades and is relatively impervious to the factors preventing agricultural advances from taking root in developing countries.\nIn conclusion, developing companies face myriad problems that hinder their ability to take advantage of newer agricultural models and we should direct more funding to helping these countries learn more about new farming methods. This is likely to be an important issue in the world as the gap between rich and poor, developed and undeveloped widens while technological progress continues its indifferent march forward.\n\n1 Key Academic Vocabulary\nNouns/nominalisations: developments, innovations, humanitarian funding, infrastructure, instability, malnourishment, subsidies, exploitation, irrigation, prevalence, technological progress\nVerbs: suffer, destabilize, implement, exploit, redirect, power, widen\nAdjectives/adverbs: tremendous, advanced, limited, poor, political, historical(ly), immediate, long-term, modern, qualified, relative(ly), impervious, indifferent\n\n\n2 Linking & Signposting Phrases\nContrast: yet, however, but\nCause/Reason: because, depending on, due to\nExample: one example (of this) would be\nCondition: if…, then…\nAddition/Sequence: and, also\nConclusion: in conclusion\n\n\n3 Grammar Structures\nComplex sentences with multiple clauses:\n“There are a number of reasons for this ranging from X to Y…”\n“The universities where students would learn… are frequently shut down.”\nRelative clauses: “countries where…”, “…methods that…”\nConditionals (first): “If more people like him can be found…, this will have an impact.”\nPassive voice: “are frequently shut down or destroyed.”\nNominalisation to pack information: “the continued prevalence of hunger.”\n\n\n4 Sentence-building Techniques\nTopic sentence → elaboration → example in each body paragraph\nBalance of simple/simple-compound sentences (“However, there would be better long-term effects…” vs. “It is large enough to power irrigation channels for the entire village.”)\nAbstract–Concrete pairing: stating a general problem then giving a vivid, specific case (Congo revolutions; Kenyan windmill)."
  },
  {
    "objectID": "english/present-time/index.html",
    "href": "english/present-time/index.html",
    "title": "Grammar Present time",
    "section": "",
    "text": "Present Simple\n\n\n\n\n\n\n\nUse to\nExample\n\n\n\n\nGeneral truths\nThe left-hand side of the brain controls the right-hand side of the body.\n\n\nCurrent habits\nI don’t always go to lectures that are early in the morning.\n\n\nPermanent situations and states\nAngie teaches French at a local adult education centre."
  },
  {
    "objectID": "english/english-health-services/index.html",
    "href": "english/english-health-services/index.html",
    "title": "Health Services",
    "section": "",
    "text": "Health services are a basic necessity. However, private companies have made them quite costly for ordinary individuals Do the advantages of private health care outweigh its disadvantage"
  },
  {
    "objectID": "english/english-health-services/index.html#analysis",
    "href": "english/english-health-services/index.html#analysis",
    "title": "Health Services",
    "section": "Analysis",
    "text": "Analysis\nAdvantages of Private Health Care: - Efficiency and shorter Waiting Times: Private hospitals offer quicker services compare to public hospitals, where long waiting lists are common. - Better facilities and advanced technology: Many private health institution provide cutting-edge medical technology, specialized treatments, and higher standards of care. - Greater Patient Choice: Individuals can select their preferred doctors, hospitals, and treatment plans, leading to more personalized care. Disadvantages of Private Health Care: - High costs and Limited Accessibility: Private health care is expensive, making it inaccessible for low-income individuals who cannot afford quality treatment. - Profit-driven System: The focus on profit can sometimes lead to unnecessary medical procedures, overcharging, or prioritizing financial gain over patient well-being. - Health inequality: A reliance on private health care can widen the gap between the rich and the poor, as only those who can afford it receive top-tier medical services. ## Sample essay Health care is an essential service, yet private medical facilities often make it unaffordable for many individuals. While private health care offers significant advantages such as efficiency, superior technology, and greater patient choice, it also has considerable drawbacks, particularly its high costs and contribution to social inequality. In my view, although private health care enhances medical standards, its disadvantages outweigh its benefits due to its exclusive and financial burden on ordinary citizens.\nOne of the key benefits of private health care is it efficiency. Unlike public hospitals, which often suffer from overcrowding and long waiting times, private institutions provide quicker access to medical attention. This is particularly crucial for emergency treatments or specialized procedures that require immediate intervention. Additionally, private hospitals invest heavily in advanced medical technology and modern infrastructure, ensuring high-quality treatment and specialized care. For instance, many private institutions offer robotic surgeries and personalized cancer treatments, which are not always available in public hospitals. Furthermore, private health care allows patients to choose their doctors and treatment plans, leading to a more personalized and comfortable experience.\nHowever, the drawbacks of private health care are significant. The most pressing concern is its exorbitant cost, making it unaffordable for a large portion of the population. Many life-saving procedures and medications are out of reach for lower-income individuals, forcing them to rely on overstretched public health systems. Furthermore, because private hospitals overate on a profit-driven model, there is a risk of unnecessary procedures being performed to maximize revenue, sometimes at the expensive of patient welfare. Lastly, the presence of an expensive private health care sector creates a divide between the wealthy and the poor, exacerbating health inequalities. In countries without strong public health care, the underprivileged often suffer from preventable diseases simply due to financial constraints.\nIn conclusion, while private health care provides high-quality services and efficiency, its high costs and potential for exploitation make it a less viable option for the general population. A well-functioning public health system remains crucial to ensuring that medical care is accessible to all, regardless of financial status. Governments should regulate private health care costs and prioritize investment in public health services to balance quality with accessibility."
  },
  {
    "objectID": "english/english-health-services/index.html#structure",
    "href": "english/english-health-services/index.html#structure",
    "title": "Health Services",
    "section": "Structure",
    "text": "Structure\n1. Health care is an essential service, yet private medical facilities often make it unaffordable for many individuals. 2. While private health care offers significant advantages such as efficiency, superior technology, and greater patient choice, it also has considerable drawbacks, particularly its high costs and contribution to social inequality. 3. In my view, although private health care enhances medical standards, its disadvantages outweigh its benefits due to its exclusive and financial burden on ordinary citizens. 1.Paraphrase the question. 2.Mention both advantages and disadvantages. 3.Clearly state your opinion.* 1. One of the key benefits of private health care is its efficiency, as it ensures faster access to treatment, advanced medical technology, and personalized care. 2. Unlike public hospitals, which often suffer from overcrowding and long waiting times, private institutions provide quicker access to medical attention. 3. This is particularly crucial for emergency treatments or specialized procedures that require immediate intervention. Additionally, private hospitals invest heavily in advanced medical technology and modern infrastructure, ensuring high-quality treatment and specialized care. 4. For instance, many private institutions offer robotic surgeries and personalized cancer treatments, which are not always available in public hospitals. 5. Furthermore, private health care allows patients to choose their doctors and treatment plans, leading to a more personalized and comfortable experience. 1.Write a topic sentence with a clear main idea at the end. 2.Explain our main idea 3.Develop it fully 4.Develop it with specific or hypothetical examples 5.Better to have more detail\n1. However, the drawbacks of private health care are significant. 2. The most pressing concern is its exorbitant cost, making it unaffordable for a large portion of the population. 3. Many life-saving procedures and medications are out of reach for lower-income individuals, forcing them to rely on overstretched public health systems. 4. Furthermore, because private hospitals overate on a profit-driven model, there is a risk of unnecessary procedures being performed to maximize revenue, sometimes at the expensive of patient welfare. Lastly, the presence of an expensive private health care sector creates a divide between the wealthy and the poor, exacerbating health inequalities. 5. In countries without strong public health care, the underprivileged often suffer from preventable diseases simply due to financial constraints.\n1.Write a new topic sentence with a new main idea at the end 2.Explain our new main idea. 3.Include specific details and examples. 4.Add as much information as we can and make sure it links logically. 5.This country is a bit long - aim for about 275 words.\n1. In conclusion, while private health care provides high-quality services and efficiency, its high costs and potential for exploitation make it a less viable option for the general population. 2. A well-functioning public health system remains crucial to ensuring that medical care is accessible to all, regardless of financial status. Governments should regulate private health care costs and prioritize investment in public health services to balance quality with accessibility.\n1.Summaries our main idea. 2.Include a final thought."
  },
  {
    "objectID": "english/fame/index.html",
    "href": "english/fame/index.html",
    "title": "Fame",
    "section": "",
    "text": "Due to television and the internet it is easier today for a large number of people to become famous. Is this a positive or negative development?\nDavid Lang;\nIt is easier than ever today for people to become famous because of increased channels for distribution such as television and the internet. This is clearly a positive trend as it opens up the entertainment industry to greater diversity.\nThose who decry this trend typically to talentless celebrities. In the past, a comedian or musician might work for years honing their craft in order to get a slot on one of the few talkshows and then become famous overnight. Nowadays, many celebrities start on reality TV or a YouTube channel and do not have any discernible talents. The socialite Paris Hilton was one of the first to take advantage of her wealth who through dating celebrities and canny use of social media and television was able to become a celebrity in her own right.\nDespite the aforementioned cases, this is on a whole a positive trend because of the more diverse range of famous people today. Fame used to be the prerogative of white men and women, with the occasional exception from an ethnic group. One of the reasons for this is that whites typically come from more privileged backgrounds and have greater opportunities, thereby making up a disproportionate share of success stories. The internet in particular has leveled the playing field to an extent. Anyone with a smartphone today has the ability to broadcast their talents and cultivate a loyal fanbase, regardless of gender or ethnicity and without the barriers that existed in the past such as ingrained reluctance from authority figures to give certain ethnicities an honest chance.\nIn conclusion, there are clear cultural drawbacks to the ease with which many achieve fame but today it is nonetheless more inclusive. A greater concern should be the age at which people become famous as it can have developmental side effects.\nChatGPT:\nIn today’s digital age, becoming famous is easier than ever due to the explosion of online platforms and television opportunities. While some see this as a democratization of fame, it is, in reality, a negative development that has diminished the value of talent, effort, and cultural depth.\nFame today is often disconnected from genuine ability. In the past, achieving celebrity status typically required years of dedication to a particular craft. A musician, for instance, might have spent decades perfecting their skills before getting a rare shot on a major talk show. This long road filtered out the mediocre and ensured that fame reflected a certain level of excellence. By contrast, many modern celebrities rise to prominence through reality TV or social media, sometimes without any identifiable talent. Paris hilton is prime example, became a household name not because of any specific talent, but because of her social status and media presence. Similarly, Kim Kardashian built a brand through strategic use of television and social media, not through traditional artistic or professional success.\nThis shift also sends a harmful message to younger generations: that fame can be achieved without effort or substance. As a result, more people may value visibility over personal development or hard work. In previous decades, fame was generally linked to expertise or originality - actors, musicians, writers, and athletes were admired for what they did, not just who they were. Today, however, social media influencers can gain millions of followers by simply being entertaining or attractive online. This can create unrealistic aspirations and encourage superficiality. Furthermore, the psychological impact of sudden fame, especially among young people, cannot be ignored. Many find themselves under intense pressure to maintain their online image, which can lead to mental health challenges such as anxiety, stress, and loss of identity.\nIn conclusion, although more people now have the opportunity to become famous, the way fame is achieved today often undermines the importance of talent, discipline, and meaningful contribution. This is a negative development that not only lowers cultural standards but also risks harming individuals who are unprepared for the realities of public life. Rather than celebrating fame for its own sake, society should place more value on substance and long-term achievement.\n\n\nIn today’s digital age, becoming famous is easier than ever due to the explosion of online platforms and television opportunities. While some see this as a democratization of fame, it is, in reality, a negative development that has diminished the value of talent, effort, and cultural depth.\nFame today is often disconnected from genuine ability. In the past, achieving celebrity status typically required years of dedication to a particular craft. A musician, for instance, might have spent decades perfecting their skills before getting a rare shot on a major talk show. This long road filtered out the mediocre and ensured that fame reflected a certain level of excellence. By contrast, many modern celebrities rise to prominence through reality TV or social media, sometimes without any identifiable talent. Paris Hilton is a prime example, becoming a household name not because of any specific talent, but because of her social status and media presence. Similarly, Kim Kardashian built a brand through strategic use of television and social media, not through traditional artistic or professional success.\nThis shift also sends a harmful message to younger generations: that fame can be achieved without effort or substance. As a result, more people may value visibility over personal development or hard work. In previous decades, fame was generally linked to expertise or originality — actors, musicians, writers, and athletes were admired for what they did, not just who they were. Today, however, social media influencers can gain millions of followers by simply being entertaining or attractive online. This can create unrealistic aspirations and encourage superficiality. Furthermore, the psychological impact of sudden fame, especially among young people, cannot be ignored. Many find themselves under intense pressure to maintain their online image, which can lead to mental health challenges such as anxiety, stress, and loss of identity.\nIn conclusion, although more people now have the opportunity to become famous, the way fame is achieved today often undermines the importance of talent, discipline, and meaningful contribution. This is a negative development that not only lowers cultural standards but also risks harming individuals who are unprepared for the realities of public life. Rather than celebrating fame for its own sake, society should place more value on substance and long-term achievement.\n\n\n\n\n\n\n\n\n\n\n\nPhrase / Word\nMeaning\nExample\n\n\n\n\nDemocratization of fame\nMaking fame accessible to more people, not just elites\nSocial media has led to the democratization of fame.\n\n\nNegative development\nA harmful trend or change\nMany believe this is a negative development for society.\n\n\nDiminished the value\nMade something less meaningful\nEasy access to fame has diminished the value of hard work.\n\n\nDisconnected from genuine ability\nNot related to real skill\nTheir popularity seems disconnected from genuine ability.\n\n\nDedication to a particular craft\nCommitted effort to improve in one area\nIt takes years of dedication to a particular craft.\n\n\nPerfecting their skills\nImproving abilities to an excellent level\nShe spent hours perfecting her skills.\n\n\nFiltered out the mediocre\nRemoved those who are not very good\nThe old system filtered out the mediocre.\n\n\nRise to prominence\nBecome well-known\nThe actor rose to prominence quickly.\n\n\nIdentifiable talent\nClear or noticeable ability\nMany influencers lack identifiable talent.\n\n\nPrime example\nA clear or typical example\nHe is a prime example of overnight fame.\n\n\nSocial status and media presence\nReputation and online visibility\nHer social status and media presence made her famous.\n\n\nBuilt a brand\nCreated a personal identity for marketing\nShe built a brand around lifestyle content.\n\n\nSends a harmful message\nCommunicates a negative idea\nThis show sends a harmful message to teens.\n\n\nValue visibility over personal development\nCare more about being seen than growing personally\nInfluencers often value visibility over development.\n\n\nExpertise or originality\nDeep knowledge or unique ideas\nFame used to reflect expertise or originality.\n\n\nUnrealistic aspirations\nGoals that are not practical or likely\nReality TV encourages unrealistic aspirations.\n\n\nSuperficiality\nLack of depth or real value\nThe influencer culture promotes superficiality.\n\n\nPsychological impact of sudden fame\nMental and emotional effects of fast fame\nSudden fame can have serious psychological impacts.\n\n\nIntense pressure\nStrong stress or demands\nShe felt intense pressure to stay relevant.\n\n\nLoss of identity\nLosing sense of who you are\nMany stars suffer from loss of identity.\n\n\nUndermines the importance of…\nMakes something seem less valuable\nThis trend undermines the importance of effort.\n\n\nLowers cultural standards\nReduces quality in society\nThe trend lowers cultural standards over time.\n\n\nRisks harming individuals\nMight hurt people\nIt risks harming individuals mentally and emotionally.\n\n\nCelebrating fame for its own sake\nValuing fame without cause or substance\nSociety celebrates fame for its own sake too often.\n\n\nSubstance and long-term achievement\nMeaningful success that lasts\nWe should focus on substance and long-term achievement."
  },
  {
    "objectID": "english/fame/index.html#vocabulary",
    "href": "english/fame/index.html#vocabulary",
    "title": "Fame",
    "section": "",
    "text": "In today’s digital age, becoming famous is easier than ever due to the explosion of online platforms and television opportunities. While some see this as a democratization of fame, it is, in reality, a negative development that has diminished the value of talent, effort, and cultural depth.\nFame today is often disconnected from genuine ability. In the past, achieving celebrity status typically required years of dedication to a particular craft. A musician, for instance, might have spent decades perfecting their skills before getting a rare shot on a major talk show. This long road filtered out the mediocre and ensured that fame reflected a certain level of excellence. By contrast, many modern celebrities rise to prominence through reality TV or social media, sometimes without any identifiable talent. Paris Hilton is a prime example, becoming a household name not because of any specific talent, but because of her social status and media presence. Similarly, Kim Kardashian built a brand through strategic use of television and social media, not through traditional artistic or professional success.\nThis shift also sends a harmful message to younger generations: that fame can be achieved without effort or substance. As a result, more people may value visibility over personal development or hard work. In previous decades, fame was generally linked to expertise or originality — actors, musicians, writers, and athletes were admired for what they did, not just who they were. Today, however, social media influencers can gain millions of followers by simply being entertaining or attractive online. This can create unrealistic aspirations and encourage superficiality. Furthermore, the psychological impact of sudden fame, especially among young people, cannot be ignored. Many find themselves under intense pressure to maintain their online image, which can lead to mental health challenges such as anxiety, stress, and loss of identity.\nIn conclusion, although more people now have the opportunity to become famous, the way fame is achieved today often undermines the importance of talent, discipline, and meaningful contribution. This is a negative development that not only lowers cultural standards but also risks harming individuals who are unprepared for the realities of public life. Rather than celebrating fame for its own sake, society should place more value on substance and long-term achievement."
  },
  {
    "objectID": "english/fame/index.html#vocabulary-list",
    "href": "english/fame/index.html#vocabulary-list",
    "title": "Fame",
    "section": "",
    "text": "Phrase / Word\nMeaning\nExample\n\n\n\n\nDemocratization of fame\nMaking fame accessible to more people, not just elites\nSocial media has led to the democratization of fame.\n\n\nNegative development\nA harmful trend or change\nMany believe this is a negative development for society.\n\n\nDiminished the value\nMade something less meaningful\nEasy access to fame has diminished the value of hard work.\n\n\nDisconnected from genuine ability\nNot related to real skill\nTheir popularity seems disconnected from genuine ability.\n\n\nDedication to a particular craft\nCommitted effort to improve in one area\nIt takes years of dedication to a particular craft.\n\n\nPerfecting their skills\nImproving abilities to an excellent level\nShe spent hours perfecting her skills.\n\n\nFiltered out the mediocre\nRemoved those who are not very good\nThe old system filtered out the mediocre.\n\n\nRise to prominence\nBecome well-known\nThe actor rose to prominence quickly.\n\n\nIdentifiable talent\nClear or noticeable ability\nMany influencers lack identifiable talent.\n\n\nPrime example\nA clear or typical example\nHe is a prime example of overnight fame.\n\n\nSocial status and media presence\nReputation and online visibility\nHer social status and media presence made her famous.\n\n\nBuilt a brand\nCreated a personal identity for marketing\nShe built a brand around lifestyle content.\n\n\nSends a harmful message\nCommunicates a negative idea\nThis show sends a harmful message to teens.\n\n\nValue visibility over personal development\nCare more about being seen than growing personally\nInfluencers often value visibility over development.\n\n\nExpertise or originality\nDeep knowledge or unique ideas\nFame used to reflect expertise or originality.\n\n\nUnrealistic aspirations\nGoals that are not practical or likely\nReality TV encourages unrealistic aspirations.\n\n\nSuperficiality\nLack of depth or real value\nThe influencer culture promotes superficiality.\n\n\nPsychological impact of sudden fame\nMental and emotional effects of fast fame\nSudden fame can have serious psychological impacts.\n\n\nIntense pressure\nStrong stress or demands\nShe felt intense pressure to stay relevant.\n\n\nLoss of identity\nLosing sense of who you are\nMany stars suffer from loss of identity.\n\n\nUndermines the importance of…\nMakes something seem less valuable\nThis trend undermines the importance of effort.\n\n\nLowers cultural standards\nReduces quality in society\nThe trend lowers cultural standards over time.\n\n\nRisks harming individuals\nMight hurt people\nIt risks harming individuals mentally and emotionally.\n\n\nCelebrating fame for its own sake\nValuing fame without cause or substance\nSociety celebrates fame for its own sake too often.\n\n\nSubstance and long-term achievement\nMeaningful success that lasts\nWe should focus on substance and long-term achievement."
  },
  {
    "objectID": "english/ielts-essay-template/index.html",
    "href": "english/ielts-essay-template/index.html",
    "title": "IELTS essay template",
    "section": "",
    "text": "Goal: Introduce the topic, paraphrase the question, clearly state your opinion, and outline the essay.\n\n\nIn recent years, the issue of [main idea from the prompt] has sparked considerable debate. ### Specific issue or controversy: Some people argue that [first side of the argument], while others believe that [second side]. ### Clear thesis statement (your opinion): Personally, I [completely/somewhat] agree/disagree with the idea that [restate the statement using your view]. ### Outline what the essay will cover: This essay will outline the reasons for my opinion, supported by relevant examples."
  },
  {
    "objectID": "english/ielts-essay-template/index.html#paragraph-1---introduction",
    "href": "english/ielts-essay-template/index.html#paragraph-1---introduction",
    "title": "IELTS essay template",
    "section": "",
    "text": "Goal: Introduce the topic, paraphrase the question, clearly state your opinion, and outline the essay.\n\n\nIn recent years, the issue of [main idea from the prompt] has sparked considerable debate. ### Specific issue or controversy: Some people argue that [first side of the argument], while others believe that [second side]. ### Clear thesis statement (your opinion): Personally, I [completely/somewhat] agree/disagree with the idea that [restate the statement using your view]. ### Outline what the essay will cover: This essay will outline the reasons for my opinion, supported by relevant examples."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#i.-overview-of-statistical-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#i.-overview-of-statistical-inference",
    "title": "Statistical Inference and Learning",
    "section": "I. Overview of Statistical Inference",
    "text": "I. Overview of Statistical Inference\n\nDefinition:\nStatistical inference (often called “learning” in computer science) is the process of using data to deduce the underlying distribution \\(F\\) that generated the data. This may involve estimating the entire distribution or specific features (such as the mean).\nApplications:\n\nExtracting meaningful information from data\n\nMaking informed decisions and predictions\n\nServing as the foundation for more advanced topics in statistics and machine learning"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#ii.-modeling-approaches",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#ii.-modeling-approaches",
    "title": "Statistical Inference and Learning",
    "section": "II. Modeling Approaches",
    "text": "II. Modeling Approaches\n\nA. Parametric Models\n\nDefinition:\nA model defined by a finite number of parameters.\n\nExample (Normal Distribution):\n\\[\nf(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nCharacteristics:\n\nSimpler to analyze and interpret\n\nMore efficient when the assumptions hold true\n\n\n\n\nB. Nonparametric Models\n\nDefinition:\nModels that do not restrict the distribution to a finite-dimensional parameter space.\nExamples:\n\nEstimating the entire cumulative distribution function (cdf)\\(F\\)\n\nEstimating a probability density function (pdf) with smoothness assumptions (e.g., assuming the pdf belongs to a [[Sobolev space]])\n\nCharacteristics:\n\nGreater flexibility to model complex data\n\nFewer assumptions about the form of the distribution"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.1-one-dimensional-parametric-estimation--",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.1-one-dimensional-parametric-estimation--",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.1: One-Dimensional Parametric Estimation -",
    "text": "Example 6.1: One-Dimensional Parametric Estimation -\nScenario: We observe independent Bernoulli(\\(p\\)) random variables \\(X_1, X_2, \\dots, X_n\\).\nGoal: Estimate the unknown parameter \\(p\\) (the probability of success).\nEstimator: The natural estimator is the sample mean: \\[ \\hat{p}_n = \\frac{1}{n}\\sum_{i=1}^n X_i. \\] Key Points: - Unbiasedness: \\[E(\\hat{p}_n) = p.\\] Thus, the estimator is unbiased.\nVariance: Since \\[\\operatorname{Var}(X_i) = p(1-p)\\], the variance of the estimator is\n\\[ \\operatorname{Var}(\\hat{p}_n) = \\frac{p(1-p)}{n}. \\] - Consistency: As \\(n\\) increases, the variance shrinks, making \\(\\hat{p}_n\\) a consistent estimator of \\(p\\).\n\n# Example 6.1: One-Dimensional Parametric Estimation (Bernoulli)\nimport numpy as np\n\n# True parameter for Bernoulli distribution\np_true = 0.7\nn = 1000  # number of observations\n\n# Generate n independent Bernoulli(p) observations (0 or 1)\nX = np.random.binomial(1, p_true, n)\n\n# Estimator: sample mean is the natural estimator for p\np_hat = np.mean(X)\n\nprint(\"Example 6.1: Bernoulli Parameter Estimation\")\nprint(\"True p:\", p_true)\nprint(\"Estimated p:\", p_hat)"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.2-two-dimensional-parametric-estimation--",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.2-two-dimensional-parametric-estimation--",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.2: Two-Dimensional Parametric Estimation -",
    "text": "Example 6.2: Two-Dimensional Parametric Estimation -\nScenario: Suppose \\[X_1, X_2, \\dots, X_n\\] are independent observations from a distribution \\[F\\] whose probability density function is given by a parametric family: \\[ f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right). \\] - Goal: Estimate the two parameters: the mean \\[\\mu\\] and the standard deviation \\[\\sigma\\]. - Nuisance Parameter: If we are primarily interested in \\[\\mu\\], then \\[\\sigma\\] becomes a nuisance parameter—an additional parameter that must be estimated but is not of direct interest. - Key Points: - Multidimensionality: The estimation problem involves simultaneous estimation of \\[\\mu\\] and \\[\\sigma\\]. - Methods: Techniques such as maximum likelihood estimation (MLE) are commonly used, sometimes incorporating methods (like profile likelihood) to eliminate the effect of nuisance parameters when focusing on \\[\\mu\\]. . # Analytical Explanation of Two Nonparametric Estimation Examples\nBelow, we analyze and explain two examples that illustrate nonparametric estimation techniques: one for estimating the cumulative distribution function (CDF) and another for estimating the probability density function (PDF)."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.3-nonparametric-estimation-of-the-cdf",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.3-nonparametric-estimation-of-the-cdf",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.3: Nonparametric Estimation of the CDF",
    "text": "Example 6.3: Nonparametric Estimation of the CDF\n\nProblem Statement\n\nData:\nWe have independent observations \\[X_1, X_2, \\dots, X_n\\] drawn from an unknown distribution with CDF \\[F\\].\nObjective:\nEstimate the entire cumulative distribution function \\[F\\], assuming minimal assumptions—namely, that \\[F\\] is any valid CDF (denoted by \\[\\mathcal{F}_{\\text{ALL}}\\]).\n\n\n\nApproach\n\nEstimator:\nThe natural nonparametric estimator for the CDF is the empirical distribution function (EDF) defined as: \\[\n\\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\},\n\\] where \\[\\mathbf{1}\\{X_i \\le x\\}\\] is an indicator function that is 1 if \\[X_i \\le x\\] and 0 otherwise.\n\n\n\nWhy This Works\n\nMinimal Assumptions:\nNo specific parametric form for \\[F\\] is assumed; all that is required is that \\[F\\] is a valid CDF. This makes the method very general.\nConvergence Properties:\nThe Glivenko–Cantelli theorem guarantees that the empirical CDF converges uniformly to the true CDF: \\[\n\\sup_x \\left| \\hat{F}_n(x) - F(x) \\right| \\to 0 \\quad \\text{as} \\quad n \\to \\infty.\n\\] This property ensures that the estimator is consistent.\nIntuitive Interpretation:\nThe EDF simply calculates the proportion of observations less than or equal to a given value, which is the natural way to “build” the CDF from data."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.4-nonparametric-density-estimation",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.4-nonparametric-density-estimation",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.4: Nonparametric Density Estimation",
    "text": "Example 6.4: Nonparametric Density Estimation\n\nProblem Statement\n\nData:\nAgain, we have independent observations \\[X_1, X_2, \\dots, X_n\\] from a distribution with CDF \\[F\\]. Let the associated PDF be \\[f = F'\\].\nObjective:\nEstimate the PDF \\[f\\]. However, unlike the CDF, estimating the density function nonparametrically is not possible under the sole assumption that \\[F\\] is any CDF.\n\n\n\nNeed for Additional Assumptions\n\nIll-Posed Without Smoothness:\nThe space of all CDFs (denoted by \\[\\mathcal{F}_{\\text{ALL}}\\]) is too vast; a generic CDF need not be differentiable. Even if a density exists, it can be highly irregular, making consistent estimation difficult or impossible.\nIntroducing Smoothness via Sobolev Spaces:\nTo estimate \\[f\\] reliably, we assume that \\[f\\] belongs to a more restricted function class. One common assumption is that \\[f\\] lies in a Sobolev space (denoted by \\[\\mathcal{F}_{\\text{SOB}}\\]).\nFor instance, one might assume: \\[\n\\mathcal{F}_{\\text{SOB}} = \\left\\{ f \\in \\mathcal{F}_{\\text{DENS}} : \\int \\left(f^{(s)}(x)\\right)^2 dx &lt; \\infty \\right\\},\n\\] where:\n\n\\[\\mathcal{F}_{\\text{DENS}}\\] is the set of all probability density functions.\n\\[f^{(s)}(x)\\] denotes the \\[s\\]-th derivative of \\[f\\].\nThe condition \\[\\int \\left(f^{(s)}(x)\\right)^2 dx &lt; \\infty\\] ensures that \\[f\\] is not “too wiggly” or irregular.\n\n\n\n\nEstimation Methods\n\nKernel Density Estimation (KDE):\nWith the smoothness assumption in place, methods such as kernel density estimation can be employed. A kernel density estimator has the form: \\[\n\\hat{f}_n(x) = \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right),\n\\] where:\n\n\\[K(\\cdot)\\] is a smooth kernel function (e.g., Gaussian).\n\\[h\\] is a bandwidth parameter that controls the smoothness of the estimate.\n\n\n\n\nWhy These Assumptions are Necessary\n\nRegularization:\nThe smoothness condition imposed by the Sobolev space helps regularize the estimation problem. It restricts the set of possible densities to those that have bounded variation or a controlled number of oscillations.\nImproved Convergence:\nSmoothness assumptions lead to better convergence properties of the density estimator, allowing for rates of convergence that can be rigorously analyzed.\nPractical Feasibility:\nIn many real-world scenarios, the underlying density is indeed smooth (e.g., physical phenomena, economic variables), making this assumption both realistic and useful."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#summary",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#summary",
    "title": "Statistical Inference and Learning",
    "section": "Summary",
    "text": "Summary\n\nExample 6.3:\n\nTask: Estimate the CDF \\[F\\] from data with minimal assumptions.\nMethod: Use the empirical CDF \\[\\hat{F}_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\}\\].\nKey Property: Convergence guaranteed by the Glivenko–Cantelli theorem.\n\nExample 6.4:\n\nTask: Estimate the density \\[f\\] from data.\nChallenge: Estimation is ill-posed without additional assumptions.\nSolution: Assume that \\[f\\] is smooth by requiring it to belong to a Sobolev space (e.g., \\[\\mathcal{F}_{\\text{DENS}} \\cap \\mathcal{F}_{\\text{SOB}}\\]), then use methods like kernel density estimation.\nBenefit: Smoothness constraints make the problem well-posed and lead to estimators with favorable convergence properties.\n\n\nThese examples highlight the progression from estimating a distribution function under minimal assumptions to needing extra regularity conditions when estimating derivatives like the density."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iii.-core-concepts-in-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iii.-core-concepts-in-inference",
    "title": "Statistical Inference and Learning",
    "section": "III. Core Concepts in Inference",
    "text": "III. Core Concepts in Inference\n\n1. Point Estimation\n\nConcept:\nA point estimator is a function of the data, denoted as\n\\[\n\\hat{\\theta}_n = g(X_1, X_2, \\dots, X_n)\n\\]\nused to provide a single “best guess” for the unknown parameter \\[\\theta\\].\nKey Properties:\n\nBias:\n\\[\n\\text{bias}(\\hat{\\theta}_n) = E(\\hat{\\theta}_n) - \\theta\n\\]\nVariance and Standard Error (se):\n\\[\n\\text{se} = \\sqrt{Var(\\hat{\\theta}_n)}\n\\]\nMean Squared Error (MSE):\n\\[\n\\text{mse} = E\\left[(\\hat{\\theta}_n - \\theta)^2\\right] = \\text{bias}^2(\\hat{\\theta}_n) + Var(\\hat{\\theta}_n)\n\\]\nConsistency:\nAn estimator is consistent if\n\\[\n\\hat{\\theta}_n \\xrightarrow{P} \\theta \\quad \\text{as } n \\to \\infty\n\\]\nAsymptotic Normality:\nMany estimators satisfy\n\\[\n\\frac{\\hat{\\theta}_n - \\theta}{\\text{se}} \\approx N(0, 1)\n\\]\nfor large samples, which facilitates the construction of confidence intervals.\n\n\n\n\n2. Confidence Sets\n\nConcept:\nA confidence interval (or set) is a range constructed from the data that, over many repetitions of the experiment, contains the true parameter \\[\\theta\\] with a specified probability (coverage).\nExample (Normal-Based Interval):\nWhen \\[\\hat{\\theta}_n\\] is approximately normally distributed, an approximate \\[1-\\alpha\\] confidence interval is: \\[\nC_n = \\left( \\hat{\\theta}_n - z_{\\alpha/2}\\,\\text{se}, \\quad \\hat{\\theta}_n + z_{\\alpha/2}\\,\\text{se} \\right)\n\\] where \\[z_{\\alpha/2}\\] is the quantile of the standard Normal distribution such that \\[\nP(Z &gt; z_{\\alpha/2}) = \\frac{\\alpha}{2}.\n\\]\n\n\n\n3. Hypothesis Testing\n\nConcept:\nHypothesis testing involves formulating a null hypothesis \\[H_0\\] (a default statement, such as a coin being fair) and an alternative hypothesis \\[H_1\\], then using the data to decide whether there is sufficient evidence to reject \\[H_0\\].\nExample (Testing Coin Fairness):\n\\[\nH_0: p = 0.5 \\quad \\text{versus} \\quad H_1: p \\neq 0.5\n\\]\nProcess:\n\nDefine an appropriate test statistic (e.g., \\[T = |\\hat{p}_n - 0.5|\\])\n\nSet a significance level \\[\\alpha\\]\n\nDetermine the rejection region based on \\[\\alpha\\] or compute a p-value\n\nReject \\[H_0\\] if the test statistic falls into the rejection region"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iv.-frequentist-vs.-bayesian-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iv.-frequentist-vs.-bayesian-inference",
    "title": "Statistical Inference and Learning",
    "section": "IV. Frequentist vs. Bayesian Inference",
    "text": "IV. Frequentist vs. Bayesian Inference\n\nFrequentist Inference:\n\nTreats parameters as fixed but unknown\n\nFocuses on the properties of estimators over repeated sampling (e.g., confidence intervals, hypothesis tests)\n\nBayesian Inference:\n\nTreats parameters as random variables with prior distributions\n\nUses Bayes’ theorem to update beliefs in light of new data, allowing direct probability statements about parameters\n\nComparison:\n\nFrequentist methods emphasize long-run frequency properties.\n\nBayesian methods provide a framework for incorporating prior knowledge and making probabilistic statements about parameters."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#v.-additional-information",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#v.-additional-information",
    "title": "Statistical Inference and Learning",
    "section": "V. Additional Information",
    "text": "V. Additional Information\n\nBibliographic References\n\nElementary Level:\n\nDeGroot and Schervish (2002)\n\nLarsen and Marx (1986)\n\nIntermediate Level:\n\nCasella and Berger (2002)\n\nBickel and Doksum (2000)\n\nRice (1995)\n\nAdvanced Level:\n\nCox and Hinkley (2000)\n\nLehmann and Casella (1998)\n\nLehmann (1986)\n\nvan der Vaart (1998)\n\n\n\n\nExercises\n\nPoisson Estimation:\nFor \\[X_1, X_2, \\dots, X_n \\sim \\text{Poisson}(\\lambda)\\] with the estimator\n\\[\n\\hat{\\lambda} = \\frac{1}{n}\\sum_{i=1}^n X_i,\n\\]\ndetermine the bias, standard error, and mean squared error.\nUniform Distribution Estimation (Method 1):\nFor \\[X_1, X_2, \\dots, X_n \\sim \\text{Uniform}(0, \\theta)\\] and the estimator\n\\[\n\\hat{\\theta} = \\max\\{X_1, X_2, \\dots, X_n\\},\n\\]\ncalculate the bias, standard error, and mse.\nUniform Distribution Estimation (Method 2):\nFor the same model with the estimator\n\\[\n\\hat{\\theta} = 2X_{(n)},\n\\]\nwhere \\[X_{(n)}\\] is the maximum, compute the bias, standard error, and mse."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#vi.-key-takeaways",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#vi.-key-takeaways",
    "title": "Statistical Inference and Learning",
    "section": "VI. Key Takeaways",
    "text": "VI. Key Takeaways\n\nInference Fundamentals:\nLearning how to deduce properties of a population from a sample is central to statistics and machine learning.\nModel Choice:\n\nParametric models are simpler but rely on strong assumptions.\n\nNonparametric models offer flexibility with fewer assumptions.\n\nEstimator Evaluation:\nProperties such as bias, variance (or standard error), and mean squared error are essential in assessing the quality of estimators.\nConfidence and Testing:\n\nConfidence intervals quantify the uncertainty in estimates.\n\nHypothesis testing provides a formal framework for decision-making.\n\nPhilosophical Approaches:\nThe frequentist and Bayesian paradigms provide different perspectives on probability and inference, influencing how uncertainty is quantified and interpreted."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "The Bernoulli distribution is a discrete probability distribution that models the outcome of a single trial with two possible outcomes: success (1) and failure (0).\n\n\n\nA Bernoulli random variable ( X ) takes the value: - ( X = 1 ) with probability ( p ) (success), - ( X = 0 ) with probability ( 1 - p ) (failure).\nMathematically, the probability mass function (PMF) is given by: \\[\nP(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}, \\ 0 \\leq p \\leq 1.\n\\]\n\n\n\n\n\n\nThe mean represents the expected outcome of the random variable: \\[\nE[X] = p.\n\\]\nWe expected value \\(E(x)\\) of a random variable \\(X\\) is given by: \\[\nE(X) = \\sigma x \\dot P(X = x)\n\\] For a Bernoulli random variable: \\[\nE(X) = 1 \\dot p + 0 \\dot( 1 - p) = p\n\\]\n\n\n\nThe variance measures how much the outcomes deviate from the mean: \\[\n\\operatorname{Var}(X) = p(1 - p).\n\\] The variance of a random variable \\(X\\) measures how much the values of \\(X\\) deviate from its mean: \\[\nVar(X) = E[(X-E(X))^2]\n\\] expand this: \\[\nVar(X) = E(X^2 - 2pX + p^2)\n\\] Since \\(p^2\\) is constant and \\(E(X)= p\\), we have: \\[\nVar(X) = E(X^2) - 2pE(X) + p^2\n\\]\nFor a Bernoulli variable, \\(X^2 = X\\) (because \\(1^2 = 1\\) and \\(0^2=0\\)): \\[\nE(X^2) = E(X) = p\n\\] Substituting back, \\[\nVar(X) = p - 2p^2 + p = p - p^2 = p(1)\n\\] ### 📝 Standard Deviation The standard deviation is the square root of the variance: \\[\n\\sigma = \\sqrt{p(1 - p)}.\n\\]\n\n\n\nSkewness measures the asymmetry of the distribution: \\[\n\\gamma_1 = \\frac{1 - 2p}{\\sqrt{p(1 - p)}}.\n\\]\n\n\n\nThe kurtosis of the Bernoulli distribution is: \\[\n\\gamma_2 = \\frac{1 - 6p(1 - p)}{p(1 - p)}.\n\\]\n\n\n\nThe estimator for \\(p\\) based on \\(n\\) independent observations \\(X_1, X_2, \\dots, X_n\\) is the sample mean:\n\\[\n\\hat{p}_n = \\frac{1}{n}\\sigma^{n}_{i=1}X_i.\n\\]\n\n\n\n\n\nDomain: ( x {0, 1} ).\nParameter: Single parameter ( p ), where ( 0 p ).\nSupport: The distribution is defined on two points: 0 and 1.\nMemoryless: The Bernoulli distribution is not memoryless.\nSpecial Case:\n\nIf ( p = 0.5 ), the distribution is symmetric.\nIf ( p ), the distribution is skewed.\n\n\n\n\n\n\n\nBinomial Distribution:\nThe Bernoulli distribution is a special case of the Binomial distribution with ( n = 1 ): \\[\n\\text{Bernoulli}(p) = \\text{Binomial}(n=1, p).\n\\]\nGeometric Distribution:\nA geometric random variable models the number of Bernoulli trials until the first success.\nBeta Distribution (Conjugate Prior):\nIn Bayesian statistics, the Beta distribution is the conjugate prior for the Bernoulli likelihood.\n\n\n\n\n\n\nModeling Binary Outcomes:\n\nCoin flips (Heads/Tails)\nPass/Fail tests\nYes/No survey responses\nOn/Off states in systems\n\nMachine Learning:\n\nLogistic regression for binary classification.\nBernoulli Naive Bayes classifiers.\n\nStatistical Inference:\n\nEstimating proportions (e.g., percentage of people supporting a policy).\n\n\n\n\n\n\nimport numpy as np\n\n# Parameters\np = 0.7  # Probability of success\nn = 1000  # Number of trials\n\n# Simulate Bernoulli trials\ndata = np.random.binomial(n=1, p=p, size=n)\n\n# Estimating p\np_estimate = np.mean(data)\n\nprint(f\"True probability p: {p}\")\nprint(f\"Estimated probability p̂: {p_estimate:.4f}\")"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#definition",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#definition",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "A Bernoulli random variable ( X ) takes the value: - ( X = 1 ) with probability ( p ) (success), - ( X = 0 ) with probability ( 1 - p ) (failure).\nMathematically, the probability mass function (PMF) is given by: \\[\nP(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}, \\ 0 \\leq p \\leq 1.\n\\]"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#properties-of-the-bernoulli-distribution",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#properties-of-the-bernoulli-distribution",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "The mean represents the expected outcome of the random variable: \\[\nE[X] = p.\n\\]\nWe expected value \\(E(x)\\) of a random variable \\(X\\) is given by: \\[\nE(X) = \\sigma x \\dot P(X = x)\n\\] For a Bernoulli random variable: \\[\nE(X) = 1 \\dot p + 0 \\dot( 1 - p) = p\n\\]\n\n\n\nThe variance measures how much the outcomes deviate from the mean: \\[\n\\operatorname{Var}(X) = p(1 - p).\n\\] The variance of a random variable \\(X\\) measures how much the values of \\(X\\) deviate from its mean: \\[\nVar(X) = E[(X-E(X))^2]\n\\] expand this: \\[\nVar(X) = E(X^2 - 2pX + p^2)\n\\] Since \\(p^2\\) is constant and \\(E(X)= p\\), we have: \\[\nVar(X) = E(X^2) - 2pE(X) + p^2\n\\]\nFor a Bernoulli variable, \\(X^2 = X\\) (because \\(1^2 = 1\\) and \\(0^2=0\\)): \\[\nE(X^2) = E(X) = p\n\\] Substituting back, \\[\nVar(X) = p - 2p^2 + p = p - p^2 = p(1)\n\\] ### 📝 Standard Deviation The standard deviation is the square root of the variance: \\[\n\\sigma = \\sqrt{p(1 - p)}.\n\\]\n\n\n\nSkewness measures the asymmetry of the distribution: \\[\n\\gamma_1 = \\frac{1 - 2p}{\\sqrt{p(1 - p)}}.\n\\]\n\n\n\nThe kurtosis of the Bernoulli distribution is: \\[\n\\gamma_2 = \\frac{1 - 6p(1 - p)}{p(1 - p)}.\n\\]\n\n\n\nThe estimator for \\(p\\) based on \\(n\\) independent observations \\(X_1, X_2, \\dots, X_n\\) is the sample mean:\n\\[\n\\hat{p}_n = \\frac{1}{n}\\sigma^{n}_{i=1}X_i.\n\\]"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#key-characteristics",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#key-characteristics",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Domain: ( x {0, 1} ).\nParameter: Single parameter ( p ), where ( 0 p ).\nSupport: The distribution is defined on two points: 0 and 1.\nMemoryless: The Bernoulli distribution is not memoryless.\nSpecial Case:\n\nIf ( p = 0.5 ), the distribution is symmetric.\nIf ( p ), the distribution is skewed."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#relationship-to-other-distributions",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#relationship-to-other-distributions",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Binomial Distribution:\nThe Bernoulli distribution is a special case of the Binomial distribution with ( n = 1 ): \\[\n\\text{Bernoulli}(p) = \\text{Binomial}(n=1, p).\n\\]\nGeometric Distribution:\nA geometric random variable models the number of Bernoulli trials until the first success.\nBeta Distribution (Conjugate Prior):\nIn Bayesian statistics, the Beta distribution is the conjugate prior for the Bernoulli likelihood."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#applications-of-the-bernoulli-distribution",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#applications-of-the-bernoulli-distribution",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Modeling Binary Outcomes:\n\nCoin flips (Heads/Tails)\nPass/Fail tests\nYes/No survey responses\nOn/Off states in systems\n\nMachine Learning:\n\nLogistic regression for binary classification.\nBernoulli Naive Bayes classifiers.\n\nStatistical Inference:\n\nEstimating proportions (e.g., percentage of people supporting a policy)."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#python-example-simulating-a-bernoulli-random-variable",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#python-example-simulating-a-bernoulli-random-variable",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "import numpy as np\n\n# Parameters\np = 0.7  # Probability of success\nn = 1000  # Number of trials\n\n# Simulate Bernoulli trials\ndata = np.random.binomial(n=1, p=p, size=n)\n\n# Estimating p\np_estimate = np.mean(data)\n\nprint(f\"True probability p: {p}\")\nprint(f\"Estimated probability p̂: {p_estimate:.4f}\")"
  },
  {
    "objectID": "math/convex-optimization/index.html",
    "href": "math/convex-optimization/index.html",
    "title": "Convex optimization",
    "section": "",
    "text": "1 Define\nA convex optimization problem has the form\n\\[\n\\begin{aligned}\n& \\underset{x}{\\text{minimize}} & & f_0(x) \\\\\n& \\text{subject to} & & f_i(x) \\le 0,\\quad i=1,\\dots,m,\\\\\n& & & A x = b.\n\\end{aligned}\n\\]\n\nVariable: \\(x\\in\\mathbb R^n\\)\nEquality constraints: linear (\\(A x = b\\))\n\nInequality constraints: \\(f_1,\\dots,f_m\\) are convex functions\n\nA function \\(f\\) is convex if, for all \\(x,y\\) and \\(\\theta\\in[0,1]\\), \\[\nf\\bigl(\\theta x+(1-\\theta)y\\bigr)\\;\\le\\;\\theta\\,f(x)\\;+\\;(1-\\theta)\\,f(y),\n\\] i.e. it has non-negative (upward) curvature.```mermaid\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# convex function\nf = lambda t: t**2\n\n# pick x, y, theta\nx, y, theta = -1.0, 2.0, 0.4\nz = theta*x + (1-theta)*y\n\n# sample for curve\nt = np.linspace(x-0.5, y+0.5, 300)\nplt.plot(t, f(t), label=\"f(t) (convex curve)\")\n\n# points\nplt.scatter([x, y, z], [f(x), f(y), f(z)], zorder=5)\nplt.text(x, f(x), \"  (x, f(x))\", va=\"bottom\")\nplt.text(y, f(y), \"  (y, f(y))\", va=\"bottom\")\nplt.text(z, f(z), \"  (z, f(z))\", va=\"bottom\")\n\n# chord\nfx, fy = f(x), f(y)\nchord = lambda t: fx + (fy-fx)/(y-x)*(t-x)\nplt.plot([x, y], [fx, fy], '--', label=\"chord\")\n\nplt.legend()\nplt.xlabel(\"t\")\nplt.ylabel(\"f(t)\")\nplt.title(\"Convexity: f(z) ≤ θ f(x) + (1–θ) f(y)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen is an optimization problem hard to solve?\nClassically, people thought:\nLinear problems (zero curvature) are easy, and\nNonlinear problems (nonzero curvature) are hard.\nHowever, this view is too simplistic. The key distinction is actually convexity:\nConvex problems (nonnegative curvature) can be solved reliably in polynomial time using interior-point methods, gradient-based schemes with global convergence guarantees, and other efficient algorithms. The feasible set and objective have no ‘valleys’ that trap algorithms in suboptimal points.\nNonconvex problems (regions of negative curvature) are generally NP-hard. They can exhibit multiple local minima, saddle points, and complex landscape features, so finding the global minimum is computationally intractable in the worst case (unless P=NP).\nThus, the presence of negative curvature (nonconvexity) — not merely ‘nonlinearity’ — is what makes an optimization problem hard to solve.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define convex and nonconvex functions\nconvex = lambda x: x**2\nnonconvex = lambda x: x**2 + 5*np.sin(3*x)\n\n# Domain\nx = np.linspace(-3, 3, 500)\n\n# Plot both functions\nplt.figure()\nplt.plot(x, convex(x), label='Convex: $f(x)=x^2$')\nplt.plot(x, nonconvex(x), label='Nonconvex: $f(x)=x^2+5\\sin(3x)$')\n\n# Mark minima\n# For convex, minimum at x=0\nplt.scatter(0, convex(0), color='black', label='Convex minimum')\n# For nonconvex, approximate local minima\nd = nonconvex(x)\nidx = (np.diff(np.sign(np.diff(d))) &gt; 0).nonzero()[0] + 1\nplt.scatter(x[idx], d[idx], color='red', s=20, label='Nonconvex local minima')\n\nplt.title('Convex vs Nonconvex Landscapes')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nnegative curvature (nonconvexity) — not merely ‘nonlinearity’ — is what makes an optimization problem hard to solve."
  }
]