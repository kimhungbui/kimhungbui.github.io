[
  {
    "objectID": "english.html",
    "href": "english.html",
    "title": "English",
    "section": "",
    "text": "How to Structure a ‘Two Question’ Essay\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nHow to Structure a ‘Two Question’ Essay\n\n\n\n\n\nMar 25, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance-Enhancing Drugs in Sports\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\ndouble question\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Services\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\nadvantages disadvantage\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nIELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\n\n\n\n\n\n\nZoos\n\n\n\n\n\n\nenglish\n\n\nwriting\n\n\nadvantages disadvantage\n\n\n\nYour post description\n\n\n\n\n\nMar 20, 2025\n\n\nHung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html",
    "href": "posts/extreme-early-retirement/index.html",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Tim, a man from Regina, Saskatchewan, Canada, retired from engineering at the age of 39, after 11 years of planning and working towards his goal of early retirement, and his wife plans to retire from her childcare career in a few years (00:00:07).\nTim’s family achieved early retirement through a financial strategy known as FIRE, which involves drastically reducing spending, saving a large percentage of income, and investing the savings (00:00:20).\nTim first considered the idea of early retirement in 2006 and started saving money, cutting costs, and figuring out what matters to him and his family (00:00:53).\nWhen Tim and his wife started working towards early retirement, they had student loan debt and other financial issues, but they paid off their existing debt and then focused on reducing their spending (00:01:12).\n\n\n\n\n\nTo reduce their spending, Tim and his wife looked at their bills, such as their power bill, and found ways to lower them, such as changing to LED lightbulbs (00:01:48).\nThey also reduced their grocery bill by buying generic versions of products and making things from scratch, which often tasted better and saved them money (00:02:02).\nTim and his wife have a focused approach to spending, only eating out a couple of times a year, and making sure their spending is aligned with what matters to them (00:02:31).\nTim uses a phrase to help him avoid overspending, which is to ask himself if he really needs something and if he can solve a problem without spending money (00:02:53).\nTim and his wife have paid off their mortgage and car loans in advance, which has helped them in their retirement (00:03:23).\n\n\n\n\n\nThe individual has achieved extreme early retirement at the age of 39, with a significant amount of assets and no debt, and currently resides in Regina, Saskatchewan, Canada, where housing costs are relatively low, with an average housing cost of around $278,000 (00:03:50).\nThe majority of their investments are in straightforward strategies, including exchange-traded funds that mirror major indexes, allowing for a simple and low-maintenance approach to managing their finances (00:04:01).\nDespite the simplicity of their investment strategy, they still experience worries about the stock market, but have trained themselves not to check it frequently, focusing instead on long-term performance rather than short-term fluctuations (00:04:11).\nInitially, they found the wild gyrations of the stock market to be psychologically challenging, and having a larger cushion would have been helpful for peace of mind (00:04:27).\n\n\n\n\n\nOne of the downsides of early retirement is the unusual conversations with people who question their decision to retire at a young age, leading them to reframe their explanation to focus on pursuing a writing career (00:04:44).\nThey started a blog in 2006 to document their early retirement journey, which includes their doubts, learning experiences, and mistakes, and also wrote a book called “Three at Forty Five” to discuss the importance of happiness in early retirement (00:04:57).\nThe first year of early retirement was problematic, involving a major lifestyle change and a process of deprogramming themselves to realize they didn’t have to work as much, and instead focus on enjoyable activities (00:05:29).\nTo cope with the transition, they created to-do lists that included fun items, and have pursued various hobbies such as making their own wine and beer, playing Dungeons & Dragons, and creating terrain pieces (00:05:52).\n\n\n\n\n\nThey have been married for 19 years and have two sons, aged 14 and 11, and have taught them about money management, focusing on delayed gratification, saving, and responsible spending (00:06:23).\nThey gave their children an allowance from a young age and allowed them to spend it on whatever they wanted, teaching them valuable lessons about money management (00:06:47).\nA strategy was implemented to teach children the value of saving money by explaining that waiting to buy something now can lead to affording bigger things later, resulting in two kids who default to saving money without knowing what they’re saving for (00:07:17).\nA friend’s advice was taken to heart, which stated that kids are a constant drain on finances, but the expenses change as they age, and it’s essential to get used to it (00:07:43).\nA system was set up to take out cash every month for the kids and put it aside, which averaged out to the same amount of money over the course of the year, with the wife managing the cash flow to prepare for upcoming expenses like winter boots (00:07:49).\nRegistered education savings plans were saved for the kids’ college and university education, which was fully filled before leaving work, allowing the money to grow in interest (00:08:18).\n\n\n\n\n\nThe wife decided to continue working, which was factored into the plan, assuming she would work for a few more years to offset some of the spending and provide an extra cushion (00:08:50).\nA plan was written out in advance to prepare for potential stock market declines, including options like taking a loan or getting a part-time job, to help stay calm and rational during emotional times (00:09:28).\nHaving a written plan helped to prepare for a major stock market decline a year into retirement, which ultimately worked out well in the long run (00:09:08).\nIt’s recommended to write out an investment plan while calm and rational to prepare for potential future declines and have a clear plan of action (00:09:37).\n\n\n\n\n\nThe family’s current home is a daycare, which is quite spacious, and they plan to downsize in the long term, with any savings from lower taxes, water bills, and other expenses being used to offset potential losses (00:10:14).\nLiving in Canada provides benefits such as government-paid healthcare costs, which have been over-contributed to, covering basic healthcare costs, but not dental, prescription eyewear, or other expenses that are self-insured and paid out-of-pocket (00:10:40).\nGovernment benefits like the Canada Pension Plan and Old Age Security are factored into long-term plans, but with conservative assumptions, such as only one spouse collecting Old Age Security (00:11:05).\n\n\n\n\n\nA job at the local library was taken to work with great people, enjoy the work, and earn a supplemental income, which is used to fund extra activities, and has helped achieve a better work-life balance (00:11:30).\nReduced working hours have improved overall happiness, and retirement has provided extra time to focus on desired activities and spend quality time with family, particularly children (00:12:10).\n\n\n\n\n\nResources on early retirement, including Tim’s blog and book, are available in the video description for those interested (00:12:43)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#early-retirement-journey-and-financial-strategies",
    "href": "posts/extreme-early-retirement/index.html#early-retirement-journey-and-financial-strategies",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Tim, a man from Regina, Saskatchewan, Canada, retired from engineering at the age of 39, after 11 years of planning and working towards his goal of early retirement, and his wife plans to retire from her childcare career in a few years (00:00:07).\nTim’s family achieved early retirement through a financial strategy known as FIRE, which involves drastically reducing spending, saving a large percentage of income, and investing the savings (00:00:20).\nTim first considered the idea of early retirement in 2006 and started saving money, cutting costs, and figuring out what matters to him and his family (00:00:53).\nWhen Tim and his wife started working towards early retirement, they had student loan debt and other financial issues, but they paid off their existing debt and then focused on reducing their spending (00:01:12)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#spending-reduction-and-financial-management",
    "href": "posts/extreme-early-retirement/index.html#spending-reduction-and-financial-management",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "To reduce their spending, Tim and his wife looked at their bills, such as their power bill, and found ways to lower them, such as changing to LED lightbulbs (00:01:48).\nThey also reduced their grocery bill by buying generic versions of products and making things from scratch, which often tasted better and saved them money (00:02:02).\nTim and his wife have a focused approach to spending, only eating out a couple of times a year, and making sure their spending is aligned with what matters to them (00:02:31).\nTim uses a phrase to help him avoid overspending, which is to ask himself if he really needs something and if he can solve a problem without spending money (00:02:53).\nTim and his wife have paid off their mortgage and car loans in advance, which has helped them in their retirement (00:03:23)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#investment-strategy-and-market-volatility",
    "href": "posts/extreme-early-retirement/index.html#investment-strategy-and-market-volatility",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The individual has achieved extreme early retirement at the age of 39, with a significant amount of assets and no debt, and currently resides in Regina, Saskatchewan, Canada, where housing costs are relatively low, with an average housing cost of around $278,000 (00:03:50).\nThe majority of their investments are in straightforward strategies, including exchange-traded funds that mirror major indexes, allowing for a simple and low-maintenance approach to managing their finances (00:04:01).\nDespite the simplicity of their investment strategy, they still experience worries about the stock market, but have trained themselves not to check it frequently, focusing instead on long-term performance rather than short-term fluctuations (00:04:11).\nInitially, they found the wild gyrations of the stock market to be psychologically challenging, and having a larger cushion would have been helpful for peace of mind (00:04:27)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#challenges-and-adaptation-in-early-retirement",
    "href": "posts/extreme-early-retirement/index.html#challenges-and-adaptation-in-early-retirement",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "One of the downsides of early retirement is the unusual conversations with people who question their decision to retire at a young age, leading them to reframe their explanation to focus on pursuing a writing career (00:04:44).\nThey started a blog in 2006 to document their early retirement journey, which includes their doubts, learning experiences, and mistakes, and also wrote a book called “Three at Forty Five” to discuss the importance of happiness in early retirement (00:04:57).\nThe first year of early retirement was problematic, involving a major lifestyle change and a process of deprogramming themselves to realize they didn’t have to work as much, and instead focus on enjoyable activities (00:05:29).\nTo cope with the transition, they created to-do lists that included fun items, and have pursued various hobbies such as making their own wine and beer, playing Dungeons & Dragons, and creating terrain pieces (00:05:52)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#family-life-and-financial-education-for-children",
    "href": "posts/extreme-early-retirement/index.html#family-life-and-financial-education-for-children",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "They have been married for 19 years and have two sons, aged 14 and 11, and have taught them about money management, focusing on delayed gratification, saving, and responsible spending (00:06:23).\nThey gave their children an allowance from a young age and allowed them to spend it on whatever they wanted, teaching them valuable lessons about money management (00:06:47).\nA strategy was implemented to teach children the value of saving money by explaining that waiting to buy something now can lead to affording bigger things later, resulting in two kids who default to saving money without knowing what they’re saving for (00:07:17).\nA friend’s advice was taken to heart, which stated that kids are a constant drain on finances, but the expenses change as they age, and it’s essential to get used to it (00:07:43).\nA system was set up to take out cash every month for the kids and put it aside, which averaged out to the same amount of money over the course of the year, with the wife managing the cash flow to prepare for upcoming expenses like winter boots (00:07:49).\nRegistered education savings plans were saved for the kids’ college and university education, which was fully filled before leaving work, allowing the money to grow in interest (00:08:18)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#wifes-continued-work-and-financial-planning",
    "href": "posts/extreme-early-retirement/index.html#wifes-continued-work-and-financial-planning",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The wife decided to continue working, which was factored into the plan, assuming she would work for a few more years to offset some of the spending and provide an extra cushion (00:08:50).\nA plan was written out in advance to prepare for potential stock market declines, including options like taking a loan or getting a part-time job, to help stay calm and rational during emotional times (00:09:28).\nHaving a written plan helped to prepare for a major stock market decline a year into retirement, which ultimately worked out well in the long run (00:09:08).\nIt’s recommended to write out an investment plan while calm and rational to prepare for potential future declines and have a clear plan of action (00:09:37)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#housing-plans-and-government-benefits",
    "href": "posts/extreme-early-retirement/index.html#housing-plans-and-government-benefits",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "The family’s current home is a daycare, which is quite spacious, and they plan to downsize in the long term, with any savings from lower taxes, water bills, and other expenses being used to offset potential losses (00:10:14).\nLiving in Canada provides benefits such as government-paid healthcare costs, which have been over-contributed to, covering basic healthcare costs, but not dental, prescription eyewear, or other expenses that are self-insured and paid out-of-pocket (00:10:40).\nGovernment benefits like the Canada Pension Plan and Old Age Security are factored into long-term plans, but with conservative assumptions, such as only one spouse collecting Old Age Security (00:11:05)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#part-time-work-and-enhanced-work-life-balance",
    "href": "posts/extreme-early-retirement/index.html#part-time-work-and-enhanced-work-life-balance",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "A job at the local library was taken to work with great people, enjoy the work, and earn a supplemental income, which is used to fund extra activities, and has helped achieve a better work-life balance (00:11:30).\nReduced working hours have improved overall happiness, and retirement has provided extra time to focus on desired activities and spend quality time with family, particularly children (00:12:10)."
  },
  {
    "objectID": "posts/extreme-early-retirement/index.html#resources-and-further-information",
    "href": "posts/extreme-early-retirement/index.html#resources-and-further-information",
    "title": "Family Man Retires at 39 – Extreme Early Retirement | FIRE",
    "section": "",
    "text": "Resources on early retirement, including Tim’s blog and book, are available in the video description for those interested (00:12:43)."
  },
  {
    "objectID": "mathematic.html",
    "href": "mathematic.html",
    "title": "Mathematic",
    "section": "",
    "text": "Statistical Inference and Learning\n\n\n\n\n\n\nStatistical\n\n\n\nYour post description\n\n\n\n\n\nFeb 26, 2025\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Randome Variable\n\n\n\n\n\n\nmath\n\n\nstatistic\n\n\n\nBernoulli Randome Variable\n\n\n\n\n\nSep 11, 2024\n\n\nKim Hung Bui\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we have normal distribution formular\n\n\n\n\n\n\nmath\n\n\nstatistic\n\n\n\nYour post description\n\n\n\n\n\nSep 11, 2024\n\n\nYour name\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "english/english-topic-sentence/index.html",
    "href": "english/english-topic-sentence/index.html",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "",
    "text": "The standard essay structure for a IELTS Task 2 essay is four paragraphs - an introduction, two body paragraphs and a conclusion.\nThe topic sentence is the first sentence of each body paragraph. It tells the examiner the main idea of the paragraph so it functions like a mini introduction."
  },
  {
    "objectID": "english/english-topic-sentence/index.html#analyze-sample-essays",
    "href": "english/english-topic-sentence/index.html#analyze-sample-essays",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "Analyze sample essays",
    "text": "Analyze sample essays\n\nLook at some task 2 sample essays.\nHighlight the topic sentence(s) - what is the topic? What is the main idea?\nRead the body paragraph(s) - do all the supporting ideas relate to that idea?"
  },
  {
    "objectID": "english/english-topic-sentence/index.html#practice-writing-topic-sentences-for-sample-essays",
    "href": "english/english-topic-sentence/index.html#practice-writing-topic-sentences-for-sample-essays",
    "title": "IELTS Writing Task 2 - Topic Sentences – The Fastest Way to Improve your Score",
    "section": "Practice writing topic sentences for sample essays",
    "text": "Practice writing topic sentences for sample essays\n\nLook at some task 2 sample essay.\nCover up the first sentence of the body paragraph(s).\nRead the body again - what is the topic and main idea ?\nWrite a topic sentence for that paragraph, then compare it with the original one. ## Analyze our own writing\nLook back through our old task 2 essays.\nAre the topic sentences good? If not, write new ones. ## Practice writing topic sentences for new question\nLook at some sample task 2 question (there is one below)\nAnalyze the question and brainstorm main ideas.\nChoose the main topic and main idea for each body paragraph.\nWrite our topic sentences."
  },
  {
    "objectID": "english/english-health-services/index.html",
    "href": "english/english-health-services/index.html",
    "title": "Health Services",
    "section": "",
    "text": "Health services are a basic necessity. However, private companies have made them quite costly for ordinary individuals Do the advantages of private health care outweigh its disadvantage"
  },
  {
    "objectID": "english/english-health-services/index.html#analysis",
    "href": "english/english-health-services/index.html#analysis",
    "title": "Health Services",
    "section": "Analysis",
    "text": "Analysis\nAdvantages of Private Health Care: - Efficiency and shorter Waiting Times: Private hospitals offer quicker services compare to public hospitals, where long waiting lists are common. - Better facilities and advanced technology: Many private health institution provide cutting-edge medical technology, specialized treatments, and higher standards of care. - Greater Patient Choice: Individuals can select their preferred doctors, hospitals, and treatment plans, leading to more personalized care. Disadvantages of Private Health Care: - High costs and Limited Accessibility: Private health care is expensive, making it inaccessible for low-income individuals who cannot afford quality treatment. - Profit-driven System: The focus on profit can sometimes lead to unnecessary medical procedures, overcharging, or prioritizing financial gain over patient well-being. - Health inequality: A reliance on private health care can widen the gap between the rich and the poor, as only those who can afford it receive top-tier medical services. ## Sample essay Health care is an essential service, yet private medical facilities often make it unaffordable for many individuals. While private health care offers significant advantages such as efficiency, superior technology, and greater patient choice, it also has considerable drawbacks, particularly its high costs and contribution to social inequality. In my view, although private health care enhances medical standards, its disadvantages outweigh its benefits due to its exclusive and financial burden on ordinary citizens.\nOne of the key benefits of private health care is it efficiency. Unlike public hospitals, which often suffer from overcrowding and long waiting times, private institutions provide quicker access to medical attention. This is particularly crucial for emergency treatments or specialized procedures that require immediate intervention. Additionally, private hospitals invest heavily in advanced medical technology and modern infrastructure, ensuring high-quality treatment and specialized care. For instance, many private institutions offer robotic surgeries and personalized cancer treatments, which are not always available in public hospitals. Furthermore, private health care allows patients to choose their doctors and treatment plans, leading to a more personalized and comfortable experience.\nHowever, the drawbacks of private health care are significant. The most pressing concern is its exorbitant cost, making it unaffordable for a large portion of the population. Many life-saving procedures and medications are out of reach for lower-income individuals, forcing them to rely on overstretched public health systems. Furthermore, because private hospitals overate on a profit-driven model, there is a risk of unnecessary procedures being performed to maximize revenue, sometimes at the expensive of patient welfare. Lastly, the presence of an expensive private health care sector creates a divide between the wealthy and the poor, exacerbating health inequalities. In countries without strong public health care, the underprivileged often suffer from preventable diseases simply due to financial constraints.\nIn conclusion, while private health care provides high-quality services and efficiency, its high costs and potential for exploitation make it a less viable option for the general population. A well-functioning public health system remains crucial to ensuring that medical care is accessible to all, regardless of financial status. Governments should regulate private health care costs and prioritize investment in public health services to balance quality with accessibility."
  },
  {
    "objectID": "english/english-health-services/index.html#structure",
    "href": "english/english-health-services/index.html#structure",
    "title": "Health Services",
    "section": "Structure",
    "text": "Structure\n1. Health care is an essential service, yet private medical facilities often make it unaffordable for many individuals. 2. While private health care offers significant advantages such as efficiency, superior technology, and greater patient choice, it also has considerable drawbacks, particularly its high costs and contribution to social inequality. 3. In my view, although private health care enhances medical standards, its disadvantages outweigh its benefits due to its exclusive and financial burden on ordinary citizens. 1.Paraphrase the question. 2.Mention both advantages and disadvantages. 3.Clearly state your opinion.* 1. One of the key benefits of private health care is its efficiency, as it ensures faster access to treatment, advanced medical technology, and personalized care. 2. Unlike public hospitals, which often suffer from overcrowding and long waiting times, private institutions provide quicker access to medical attention. 3. This is particularly crucial for emergency treatments or specialized procedures that require immediate intervention. Additionally, private hospitals invest heavily in advanced medical technology and modern infrastructure, ensuring high-quality treatment and specialized care. 4. For instance, many private institutions offer robotic surgeries and personalized cancer treatments, which are not always available in public hospitals. 5. Furthermore, private health care allows patients to choose their doctors and treatment plans, leading to a more personalized and comfortable experience. 1.Write a topic sentence with a clear main idea at the end. 2.Explain our main idea 3.Develop it fully 4.Develop it with specific or hypothetical examples 5.Better to have more detail\n1. However, the drawbacks of private health care are significant. 2. The most pressing concern is its exorbitant cost, making it unaffordable for a large portion of the population. 3. Many life-saving procedures and medications are out of reach for lower-income individuals, forcing them to rely on overstretched public health systems. 4. Furthermore, because private hospitals overate on a profit-driven model, there is a risk of unnecessary procedures being performed to maximize revenue, sometimes at the expensive of patient welfare. Lastly, the presence of an expensive private health care sector creates a divide between the wealthy and the poor, exacerbating health inequalities. 5. In countries without strong public health care, the underprivileged often suffer from preventable diseases simply due to financial constraints.\n1.Write a new topic sentence with a new main idea at the end 2.Explain our new main idea. 3.Include specific details and examples. 4.Add as much information as we can and make sure it links logically. 5.This country is a bit long - aim for about 275 words.\n1. In conclusion, while private health care provides high-quality services and efficiency, its high costs and potential for exploitation make it a less viable option for the general population. 2. A well-functioning public health system remains crucial to ensuring that medical care is accessible to all, regardless of financial status. Governments should regulate private health care costs and prioritize investment in public health services to balance quality with accessibility.\n1.Summaries our main idea. 2.Include a final thought."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Kim Hung Bui",
    "section": "",
    "text": "ANTI-CAPITALIST SOFTWARE LICENSE (v 1.4)\nCopyright © 2024 Erick Ratamero\nThis is anti-capitalist software, released for free use by individuals and organizations that do not operate by capitalist principles.\nPermission is hereby granted, free of charge, to any person or organization (the “User”) obtaining a copy of this software and associated documentation files (the “Software”), to use, copy, modify, merge, distribute, and/or sell copies of the Software, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or modified versions of the Software.\nThe User is one of the following:\n\n\nAn individual person, laboring for themselves\nA non-profit organization\nAn educational institution\nAn organization that seeks shared profit for all of its members, and allows non-members to set the cost of their labor\n\n\nIf the User is an organization with owners, then all owners are workers and all workers are owners with equal equity and/or equal vote.\nIf the User is an organization, then the User is not law enforcement or military, or working for or under either.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT EXPRESS OR IMPLIED WARRANTY OF ANY KIND, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "The Bernoulli distribution is a discrete probability distribution that models the outcome of a single trial with two possible outcomes: success (1) and failure (0).\n\n\n\nA Bernoulli random variable ( X ) takes the value: - ( X = 1 ) with probability ( p ) (success), - ( X = 0 ) with probability ( 1 - p ) (failure).\nMathematically, the probability mass function (PMF) is given by: \\[\nP(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}, \\ 0 \\leq p \\leq 1.\n\\]\n\n\n\n\n\n\nThe mean represents the expected outcome of the random variable: \\[\nE[X] = p.\n\\]\nWe expected value \\(E(x)\\) of a random variable \\(X\\) is given by: \\[\nE(X) = \\sigma x \\dot P(X = x)\n\\] For a Bernoulli random variable: \\[\nE(X) = 1 \\dot p + 0 \\dot( 1 - p) = p\n\\]\n\n\n\nThe variance measures how much the outcomes deviate from the mean: \\[\n\\operatorname{Var}(X) = p(1 - p).\n\\] The variance of a random variable \\(X\\) measures how much the values of \\(X\\) deviate from its mean: \\[\nVar(X) = E[(X-E(X))^2]\n\\] expand this: \\[\nVar(X) = E(X^2 - 2pX + p^2)\n\\] Since \\(p^2\\) is constant and \\(E(X)= p\\), we have: \\[\nVar(X) = E(X^2) - 2pE(X) + p^2\n\\]\nFor a Bernoulli variable, \\(X^2 = X\\) (because \\(1^2 = 1\\) and \\(0^2=0\\)): \\[\nE(X^2) = E(X) = p\n\\] Substituting back, \\[\nVar(X) = p - 2p^2 + p = p - p^2 = p(1)\n\\] ### 📝 Standard Deviation The standard deviation is the square root of the variance: \\[\n\\sigma = \\sqrt{p(1 - p)}.\n\\]\n\n\n\nSkewness measures the asymmetry of the distribution: \\[\n\\gamma_1 = \\frac{1 - 2p}{\\sqrt{p(1 - p)}}.\n\\]\n\n\n\nThe kurtosis of the Bernoulli distribution is: \\[\n\\gamma_2 = \\frac{1 - 6p(1 - p)}{p(1 - p)}.\n\\]\n\n\n\nThe estimator for \\(p\\) based on \\(n\\) independent observations \\(X_1, X_2, \\dots, X_n\\) is the sample mean:\n\\[\n\\hat{p}_n = \\frac{1}{n}\\sigma^{n}_{i=1}X_i.\n\\]\n\n\n\n\n\nDomain: ( x {0, 1} ).\nParameter: Single parameter ( p ), where ( 0 p ).\nSupport: The distribution is defined on two points: 0 and 1.\nMemoryless: The Bernoulli distribution is not memoryless.\nSpecial Case:\n\nIf ( p = 0.5 ), the distribution is symmetric.\nIf ( p ), the distribution is skewed.\n\n\n\n\n\n\n\nBinomial Distribution:\nThe Bernoulli distribution is a special case of the Binomial distribution with ( n = 1 ): \\[\n\\text{Bernoulli}(p) = \\text{Binomial}(n=1, p).\n\\]\nGeometric Distribution:\nA geometric random variable models the number of Bernoulli trials until the first success.\nBeta Distribution (Conjugate Prior):\nIn Bayesian statistics, the Beta distribution is the conjugate prior for the Bernoulli likelihood.\n\n\n\n\n\n\nModeling Binary Outcomes:\n\nCoin flips (Heads/Tails)\nPass/Fail tests\nYes/No survey responses\nOn/Off states in systems\n\nMachine Learning:\n\nLogistic regression for binary classification.\nBernoulli Naive Bayes classifiers.\n\nStatistical Inference:\n\nEstimating proportions (e.g., percentage of people supporting a policy).\n\n\n\n\n\n\nimport numpy as np\n\n# Parameters\np = 0.7  # Probability of success\nn = 1000  # Number of trials\n\n# Simulate Bernoulli trials\ndata = np.random.binomial(n=1, p=p, size=n)\n\n# Estimating p\np_estimate = np.mean(data)\n\nprint(f\"True probability p: {p}\")\nprint(f\"Estimated probability p̂: {p_estimate:.4f}\")"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#definition",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#definition",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "A Bernoulli random variable ( X ) takes the value: - ( X = 1 ) with probability ( p ) (success), - ( X = 0 ) with probability ( 1 - p ) (failure).\nMathematically, the probability mass function (PMF) is given by: \\[\nP(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}, \\ 0 \\leq p \\leq 1.\n\\]"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#properties-of-the-bernoulli-distribution",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#properties-of-the-bernoulli-distribution",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "The mean represents the expected outcome of the random variable: \\[\nE[X] = p.\n\\]\nWe expected value \\(E(x)\\) of a random variable \\(X\\) is given by: \\[\nE(X) = \\sigma x \\dot P(X = x)\n\\] For a Bernoulli random variable: \\[\nE(X) = 1 \\dot p + 0 \\dot( 1 - p) = p\n\\]\n\n\n\nThe variance measures how much the outcomes deviate from the mean: \\[\n\\operatorname{Var}(X) = p(1 - p).\n\\] The variance of a random variable \\(X\\) measures how much the values of \\(X\\) deviate from its mean: \\[\nVar(X) = E[(X-E(X))^2]\n\\] expand this: \\[\nVar(X) = E(X^2 - 2pX + p^2)\n\\] Since \\(p^2\\) is constant and \\(E(X)= p\\), we have: \\[\nVar(X) = E(X^2) - 2pE(X) + p^2\n\\]\nFor a Bernoulli variable, \\(X^2 = X\\) (because \\(1^2 = 1\\) and \\(0^2=0\\)): \\[\nE(X^2) = E(X) = p\n\\] Substituting back, \\[\nVar(X) = p - 2p^2 + p = p - p^2 = p(1)\n\\] ### 📝 Standard Deviation The standard deviation is the square root of the variance: \\[\n\\sigma = \\sqrt{p(1 - p)}.\n\\]\n\n\n\nSkewness measures the asymmetry of the distribution: \\[\n\\gamma_1 = \\frac{1 - 2p}{\\sqrt{p(1 - p)}}.\n\\]\n\n\n\nThe kurtosis of the Bernoulli distribution is: \\[\n\\gamma_2 = \\frac{1 - 6p(1 - p)}{p(1 - p)}.\n\\]\n\n\n\nThe estimator for \\(p\\) based on \\(n\\) independent observations \\(X_1, X_2, \\dots, X_n\\) is the sample mean:\n\\[\n\\hat{p}_n = \\frac{1}{n}\\sigma^{n}_{i=1}X_i.\n\\]"
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#key-characteristics",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#key-characteristics",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Domain: ( x {0, 1} ).\nParameter: Single parameter ( p ), where ( 0 p ).\nSupport: The distribution is defined on two points: 0 and 1.\nMemoryless: The Bernoulli distribution is not memoryless.\nSpecial Case:\n\nIf ( p = 0.5 ), the distribution is symmetric.\nIf ( p ), the distribution is skewed."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#relationship-to-other-distributions",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#relationship-to-other-distributions",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Binomial Distribution:\nThe Bernoulli distribution is a special case of the Binomial distribution with ( n = 1 ): \\[\n\\text{Bernoulli}(p) = \\text{Binomial}(n=1, p).\n\\]\nGeometric Distribution:\nA geometric random variable models the number of Bernoulli trials until the first success.\nBeta Distribution (Conjugate Prior):\nIn Bayesian statistics, the Beta distribution is the conjugate prior for the Bernoulli likelihood."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#applications-of-the-bernoulli-distribution",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#applications-of-the-bernoulli-distribution",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "Modeling Binary Outcomes:\n\nCoin flips (Heads/Tails)\nPass/Fail tests\nYes/No survey responses\nOn/Off states in systems\n\nMachine Learning:\n\nLogistic regression for binary classification.\nBernoulli Naive Bayes classifiers.\n\nStatistical Inference:\n\nEstimating proportions (e.g., percentage of people supporting a policy)."
  },
  {
    "objectID": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#python-example-simulating-a-bernoulli-random-variable",
    "href": "math/math-statistic-bernoulli-random-variable/bernoulli-random-variable.html#python-example-simulating-a-bernoulli-random-variable",
    "title": "Bernoulli Randome Variable",
    "section": "",
    "text": "import numpy as np\n\n# Parameters\np = 0.7  # Probability of success\nn = 1000  # Number of trials\n\n# Simulate Bernoulli trials\ndata = np.random.binomial(n=1, p=p, size=n)\n\n# Estimating p\np_estimate = np.mean(data)\n\nprint(f\"True probability p: {p}\")\nprint(f\"Estimated probability p̂: {p_estimate:.4f}\")"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Family Man Retires at 39 – Extreme Early Retirement | FIRE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost template\n\n\n\n\n\n\ntemplate\n\n\nany-category-you-want\n\n\n\nYour post description\n\n\n\n\n\nSep 11, 2024\n\n\nYour name\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/leetcode-minimum-cost-walk-in-weight-graph/index.html",
    "href": "programming/leetcode-minimum-cost-walk-in-weight-graph/index.html",
    "title": "Minimum Cost Walk in Weighted Graph",
    "section": "",
    "text": "Question\nThere is an undirected weighted graph with n vertices labeled from 0 to n - 1.\nYou are given the integer n and an array edges, where edges[i] = [ui, vi, wi] indicates that there is an edge between vertices ui and vi with a weight of wi.\nA walk on a graph is a sequence of vertices and edges. The walk starts and ends with a vertex, and each edge connects the vertex that comes before it and the vertex that comes after it. It’s important to note that a walk may visit the same edge or vertex more than once.\nThe cost of a walk starting at node u and ending at node v is defined as the bitwise AND of the weights of the edges traversed during the walk. In other words, if the sequence of edge weights encountered during the walk is w0, w1, w2, …, wk, then the cost is calculated as w0 & w1 & w2 & … & wk, where & denotes the bitwise AND operator.\nYou are also given a 2D array query, where query[i] = [si, ti]. For each query, you need to find the minimum cost of the walk starting at vertex si and ending at vertex ti. If there exists no such walk, the answer is -1.\nReturn the array answer, where answer[i] denotes the minimum cost of a walk for query i.\nExample 1:\nInput: n = 5, edges = [[0,1,7],[1,3,7],[1,2,1]], query = [[0,3],[3,4]]\nOutput: [1,-1]\nExplanation:\nTo achieve the cost of 1 in the first query, we need to move on the following edges: 0-&gt;1 (weight 7), 1-&gt;2 (weight 1), 2-&gt;1 (weight 1), 1-&gt;3 (weight 7).\nIn the second query, there is no walk between nodes 3 and 4, so the answer is -1.\nExample 2:\nInput: n = 3, edges = [[0,2,7],[0,1,15],[1,2,6],[1,2,1]], query = [[1,2]]\nOutput: [0]\nExplanation:\nTo achieve the cost of 0 in the first query, we need to move on the following edges: 1-&gt;2 (weight 1), 2-&gt;1 (weight 6), 1-&gt;2 (weight 1).\nConstraints:\n2 &lt;= n &lt;= 105\n0 &lt;= edges.length &lt;= 105\nedges[i].length == 3\n0 &lt;= ui, vi &lt;= n - 1\nui != vi\n0 &lt;= wi &lt;= 105\n1 &lt;= query.length &lt;= 105\nquery[i].length == 2\n0 &lt;= si, ti &lt;= n - 1\nsi != ti\n\n\nAnalysis\nLet’s determine when the answer to a query is -1. This happens when no walk exists between the two nodes, meaning they belong to different connected components.\nNow, suppose the two nodes belong to the same connected component. What is the minimum cost of a walk connecting them? As mentioned, the optimal walk includes as many edges as possible. Since revisiting an edge does not affect the total score, we can freely traverse the edges of the component, meaning that we can move back and forth to reach all of them. Therefore, the best way to achieve the lowest cost is to visit every edge in the component.\nTo efficiently find and process the connected components of the graph, we use the Disjoint Set (Union-Find) data structure. When we Union two nodes, we merge their entire groups, as now a path exists between every node in one group and every node in the other. To maintain efficiency, the root of the larger group is chosen as the representative of the merge group. This minimizes the time needed for future Find operations by reducing the number of steps required to reach the current representative."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html",
    "href": "programming/disjoint-set-union/index.html",
    "title": "Disjoint Set Union",
    "section": "",
    "text": "This data structure provides the following capabilities. We are given several elements, each of which is a separate set. A DSU will have an operation to combine any two sets, and it will be able to tell in which set a specific element is. The classical version also introduces a third operation, it can create a set from a new element.\nThus the basic interface of this data structure consists of only three operations: - make_set() - create a new set consisting of the new element v. - union_sets(a, b) - merges the two specified sets (the set in which the element a is located, and the set in which the element b is located). - find_set(v) - return the representative (also called leader) of the set that contains the element v. This representative is an element of its corresponding set. It is selected in each set by the data structure itself (and can change over time, namely after union_sets call). This representative can be used to check if two elements are part of the same set or not. a and b are exactly in the same set, if find_set(a) == find_set(b). Otherwise they are in different sets.\nAs described on more detail later, the data structure allows us to do each of these operations in almost \\(O(1)\\) time on average.\nAlso in one of the subsections an alternative structure of a DSU is explained, which achieves a slower average complexity of \\(O(\\log n)\\), but can be more powerful than the regular DSU structure."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#naive-implementation",
    "href": "programming/disjoint-set-union/index.html#naive-implementation",
    "title": "Disjoint Set Union",
    "section": "Naive implementation",
    "text": "Naive implementation\nWe can already write the first implementation of the Disjoint Set Union data structure. It will be pretty inefficient at first, but later we can improve it using two optimizations, so that it will take nearly constant time for each function call.\nAs we said, all the information about the sets of elements will be kept in an array parent.\nTo create a new set (operation make_set(v)), we simply create a tree with root in the vertex x, meaning that it is its own ancestor.\nTo combine two sets (operation union_sets(a, b)), we first find the representative of the set in which a is located, and the representative of the set in which b is located. If the representatives are identical, that we have nothing to do, the sets are already merged. Otherwise, we can simply specify that one of the representatives is the parent of the other representative - thereby combining the two trees.\nFinally the implementation of the find representative function (operation find_set(v)): we simply climb the ancestors of the vertex v until we reach the root, i.e. a vertex such that the reference to the ancestor leads to itself. This operation is easily implemented recursively.\nvoid make_set(int v) {\n    parent[v] = v;\n}\n\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    return find_set(parent[v]);\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b)\n        parent[b] = a;\n}\nHowever this implementation is inefficient. It is easy to construct an example, so that the trees degenerate into long chains. In that case each call find_set(v) can take \\(O(n)\\) time.\nThis is far away from the complexity that we want to have (nearly constant time). Therefore we will consider two optimizations that will allow to significantly accelerate the work."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#path-compression-optimization",
    "href": "programming/disjoint-set-union/index.html#path-compression-optimization",
    "title": "Disjoint Set Union",
    "section": "Path compression optimization",
    "text": "Path compression optimization\nThis optimization is designed for speeding up find_set.\nIf we call find_set() for some vertex x, we actually find the representative p for all vertices that we visit on the path between v and the actually representative p. The trick is to make the paths for all those nodes shorter, by setting the parent of each visited vertex directly to p.\nWe can see the operation in the following image. On the left there is a tree, and on the right side there is the compressed tree after calling find_set(7), which shortens the paths for the visited node 7, 5, 3 and 2.\n![[DSU_path_compression.png]]\nThe new implementation of find_set is as follows:\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    return parent[v] = find_set(parent[v]);\n}\nThe simple implementation does what was intended: first find the representative of the set (root vertex), and then in the process of stack unwinding the visited nodes are attached directly to the representative.\nThis simple modification of the operation already achieves the time complexity \\(O(\\log n)\\) per call on average (here without proof). There is a second modification, that will make it even faster."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#union-by-size-rank",
    "href": "programming/disjoint-set-union/index.html#union-by-size-rank",
    "title": "Disjoint Set Union",
    "section": "Union by size / rank",
    "text": "Union by size / rank\nIn this optimization we will change the union_set operation. To be precise, we will change which tree gets attached to the other one. In the naive implementation the second tree always got attached to the first one. In practice that can lead to trees containing chains of length \\(O(n)\\). With this optimization we will avoid this by choosing very carefully which tree gets attached.\nThere are many possible heuristic that can be used. Most popular are the following two approaches: In the first approach we use the size of the trees as rank, and in the second one we use the depth of the tree (more precisely, the upper bound on the tree depth, because the depth will get smaller when applying path compression).\nIn both approaches the essence of the optimization is the same: we attach the tree with the lower rank to the one with the bigger rank.\nHere is the implementation of union by size:\nvoid make_set(int v) {\n    parent[v] = v;\n    size[v] = 1;\n}\n\nvoid union_sets(int a, int b){\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (size[a] &lt; size[b])\n            swap(a, b);\n        parent[b] = a;\n        size[a] += size[b];\n    }\n}\nand here is the implementation of union by rank based on the depth of the trees:\nvoid make_set(int v) {\n    parent[v] = v;\n    rank[v] = 0;\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (rank[a] &lt; rank[b])\n            swap(a, b);\n        parent[b] = a;\n        if (rank[a] == rank[b])\n            rank[a]++;\n    }\n}\nBoth optimizations are equivalent in terms of time and space complexity. So in practice we can use any of them."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#time-complexity",
    "href": "programming/disjoint-set-union/index.html#time-complexity",
    "title": "Disjoint Set Union",
    "section": "Time complexity",
    "text": "Time complexity\nAs mentioned before, if we combine both optimizations - path compression with union by size / rank - we will reach nearly constant time queries. It turns out, that the final amortized complexity is \\(O(\\alpha(n))\\), where \\(\\alpha (n)\\) is the inverse Ackermann function, which grows very slowly. In fact it grows so lowly, that it doesn’t exceed 4 for all reasonable \\(n\\) (approximately \\(n&lt;10^{600}\\)).\nAmortized complexity is the total time per operation, evaluated over a sequence of multiple operations. The idea is to guarantee the total time of the entire sequence, while allowing single operations to be much slower than the amortized time. E.g. in our case a single call might take \\(O(\\log n)\\) in the worst case, but if we do \\(m\\) such calls back to back we will end up with an average time of \\(O(\\alpha (n))\\).\nWe will also not present a proof for this time complexity, since it is quite long and complicated.\nAlso, it’s worth mentioning that DSU with union by size / rank, but without path compression works in \\(O(\\log n)\\) time per query."
  },
  {
    "objectID": "programming/disjoint-set-union/index.html#linking-by-index-coin-flip-linking",
    "href": "programming/disjoint-set-union/index.html#linking-by-index-coin-flip-linking",
    "title": "Disjoint Set Union",
    "section": "Linking by index / coin-flip linking",
    "text": "Linking by index / coin-flip linking\nBoth union by rank and union by size require that we store additional data for each set, and maintain these values during each union operation. There exist also a randomized algorithm, that simplifies the union operation a little bit: linking by index.\nWe assign each set a random value called the index, and we attach the set with the smaller index to the one with the larger one. It is likely that a bigger set will have a bigger than the smaller set, therefore this operation is closely related to union by size. In fact it can be proven, that this operation has the same time complexity as union by size. However in practice it is slightly slower than union by size."
  },
  {
    "objectID": "programming/leetcodeminimum-operations-to-make-binary-array-elements-equal-to-one-i/3191.html",
    "href": "programming/leetcodeminimum-operations-to-make-binary-array-elements-equal-to-one-i/3191.html",
    "title": "3191. Minimum Operations to Make Binary Array Elements Equal to One I",
    "section": "",
    "text": "You are given a\nnums.\nYou can do the following operation on the array any number of times (possibly zero):\nChoose any 3 consecutive elements from the array and flip all of them.\nFlipping an element means changing its value from 0 to 1, and from 1 to 0.\nReturn the minimum number of operations required to make all elements in nums equal to 1. If it is impossible, return -1.\nExample 1:\nInput: nums = [0,1,1,1,0,0]\nOutput: 3\nExplanation: We can do the following operations:\nChoose the elements at indices 0, 1 and 2. The resulting array is nums = [1,0,0,1,0,0].\nChoose the elements at indices 1, 2 and 3. The resulting array is nums = [1,1,1,0,0,0].\nChoose the elements at indices 3, 4 and 5. The resulting array is nums = [1,1,1,1,1,1].\nExample 2:\nInput: nums = [0,1,1,1]\nOutput: -1\nExplanation: It is impossible to make all elements equal to 1.\nConstraints:\n3 &lt;= nums.length &lt;= 105\n0 &lt;= nums[i] &lt;= 1\n\nAnalysis\nSince we can only flip three consecutive elements at a time, isolated 0s or certain patterns of 0s may prevent us from turning everything into 1. If the number of 0s in certain positions makes it impossible to fully eliminate them using groups of three, the transformation cannot be achieved."
  },
  {
    "objectID": "programming/leetcode-longest-nice-subarray/leetcode-Longest-Nice-Subarray.html",
    "href": "programming/leetcode-longest-nice-subarray/leetcode-Longest-Nice-Subarray.html",
    "title": "leetcode-Longest-Nice-Subarray",
    "section": "",
    "text": "Topic: array, bit manipulation, slide windown\n\nQuestion\nYou are given an array nums consisting of positive integers.\nWe call a subarray of nums nice if the bitwise AND of every pair of elements that are in different positions in the subarray is equal to 0.\nReturn the length of the longest nice subarray.\nA subarray is a contiguous part of an array.\nNote that subarrays of length 1 are always considered nice.\nExample 1:\nInput: nums = [1,3,8,48,10] Output: 3 Explanation: The longest nice subarray is [3,8,48]. This subarray satisfies the conditions: - 3 AND 8 = 0. - 3 AND 48 = 0. - 8 AND 48 = 0. It can be proven that no longer nice subarray can be obtained, so we return 3.\nExample 2:\nInput: nums = [3,1,5,11,13] Output: 1 Explanation: The length of the longest nice subarray is 1. Any subarray of length 1 can be chosen.\nConstraints:\n1 &lt;= nums.length &lt;= 105\n1 &lt;= nums[i] &lt;= 109\n\n\nAnalysis\n    example 1:\n    [1, 3, 8, 48, 10]\n    3 in bin: 0011\n    8 in bin: 1000\n    48d = 1100000\n    10d = 1010\n    0011 AND 1000 = 0\n    3, 8, 48 AND = 0\n    10 and 8 not = 0\n    so, if pair AND = 0\n    that pair have no common bit\n    so, we store a bit array to check the state of bit\n    and if, to better, we just need to store number of bit in that array instead.\n    oh no, it wrong.\n    so if we must store a array.\n    no, we can use bitwise operator & to check if a AND b == 0 or not\n    and OR for cumulative bit\n    x = 5\n    # 101\n    x |= 3\n    # 3 == 011\n    # 101 |= 011 = 111\n    print(x)\n    # 7\n    to search for longest (can use i, and j) for check all the begin and end\n    improve it by two pointer to decrease TC from O(n^2) to O(n)\n    and now, how to get rid of num of left from cumulative bit in slide windown\n    check that case: [011, 100]\n    now, culmulative bit: 111\n    we want it after left += 1, is 100\n    in XOR: 111 XOR 011 == 100\n    XOR parameter in python is ^=\n\n\nCode\n    def longestNiceSubarray(self, nums: List[int]) -&gt; int:\n\n        cumulative_bit = 0\n\n        ans = 0\n\n        left = 0\n\n        for right in range(len(nums)):\n\n            # when AND not ease\n\n            while cumulative_bit & nums[right] != 0: # right can not cumulative, increase left until it can ease, use XOR for get rid of it\n\n                cumulative_bit ^= nums[left]\n\n                left += 1\n\n            # until can AND\n\n            # use OR to cumulative it\n\n            cumulative_bit |= nums[right]\n\n            ans = max(ans, right - left + 1)\n\n        return ans"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html",
    "title": "Divide Array Into Equal Pairs",
    "section": "",
    "text": "Topic: array, hash table, bit manipulation, counting"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#count-array",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#count-array",
    "title": "Divide Array Into Equal Pairs",
    "section": "Count array",
    "text": "Count array\nnums that have 2 * n intenger\ndivide nums into n pairs\n1 element in 1 pair\nelements in pair is equal\nreturn true if can devide to n pair,\nSo, we can use count array\nif all even \\`return true\\`\n\nelse \\`return false\\`\nTC: O(n)\nclass Solution:\n    def divideArray(self, nums: List[int]) -&gt; bool:\n    ans = True\n    count_array = [0]*(500+1)\n    for num in nums:\n        count_array[num] += 1\n    print(count_array)\n    for num in count_array:\n        if num % 2 != 0:\n            return False\n    return ans"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#map",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#map",
    "title": "Divide Array Into Equal Pairs",
    "section": "Map",
    "text": "Map\nlike approach 1, we can use map for that (better code)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        frequency = Counter(nums)\n        # check consecutive pairs in sorted array\n        return all(count % 2 == 0 for count in frequency.values())"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#bool-array",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#bool-array",
    "title": "Divide Array Into Equal Pairs",
    "section": "Bool array",
    "text": "Bool array\nan improve, use boolean array\nO(n)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n\n        max_num = max(nums)\n\n        needs_pair = [False] * (max_num + 1)\n\n        for num in nums:\n            needs_pair[num] = not needs_pair[num]\n\n        return not any(needs_pair[num] for num in nums)"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#sorted",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#sorted",
    "title": "Divide Array Into Equal Pairs",
    "section": "Sorted",
    "text": "Sorted\nsorted that can have TC: O(nlogn)\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        nums.sort()\n        # check consecutive pairs in sorted array\n        return all(nums[i] == nums[i+1] for i in range (0, len(nums), 2))"
  },
  {
    "objectID": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#hash-set",
    "href": "programming/leetcode-divide-array-into-equal-pairs/Leetcode-Divide-array-into-equal-pairs.html#hash-set",
    "title": "Divide Array Into Equal Pairs",
    "section": "Hash set",
    "text": "Hash set\nwe can store a element when first meet it, and even get it, we remote from set\nwhen retrieve all, if set have element.\nhash set have TC of lookup, addition, removal in constant time.\n    def divideArray(self, nums: List[int]) -&gt; bool:\n        unpaired = set()\n\n        for num in nums:\n            if num in unpaired:\n                unpaired.remove(num)\n            else:\n                unpaired.add(num)\n        return not unpaired"
  },
  {
    "objectID": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html",
    "href": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html",
    "title": "How to approach most DP problems",
    "section": "",
    "text": "To solve a dp problem: https://leetcode.com/problems/house-robber/solutions/156523/from-good-to-great-how-to-approach-most-of-dp-problems/\nThis particular problem can be approached using the following sequence:"
  },
  {
    "objectID": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html#step-5-iterative-2-variables-bottom-up",
    "href": "programming/programming-how-to-approach-most-dp-problems/how-to-approach-most-dp-problems.html#step-5-iterative-2-variables-bottom-up",
    "title": "How to approach most DP problems",
    "section": "Step 5: Iterative + 2 variables (bottom-up)",
    "text": "Step 5: Iterative + 2 variables (bottom-up)\nIn the previous step, we use only memo[i] and memo[i-1], so going just 2 step back. We can hold them in 2 variables instead. This optimization is met in Fibonacci sequence creation and some other problems [[Optimize Fibonacci]]\n    def rob(self, nums: List[int]) -&gt; int:\n        if len(nums) == 0:\n            return 0\n        prev1, prev2 = 0, 0\n        for num in nums:\n            temp = prev1\n            prev1 = max(prev2 + num, prev1)\n            prev2 = temp\n        return prev1"
  },
  {
    "objectID": "programming/leetcode-house-robber/house-robber.html",
    "href": "programming/leetcode-house-robber/house-robber.html",
    "title": "House robber",
    "section": "",
    "text": "Topic: array, dymanic programming\n\nQuestion\nYou are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and it will automatically contact the police if two adjacent houses were broken into on the same night.\nGiven an integer array nums representing the amount of money of each house, return the maximum amount of money you can rob tonight without alerting the police.\nExample 1:\n\nInput: nums = [1,2,3,1]\nOutput: 4\nExplanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4.\n\nExample 2:\n\nInput: nums = [2,7,9,3,1]\nOutput: 12\nExplanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1). Total amount you can rob = 2 + 9 + 1 = 12.\n\nConstraints:\n1 &lt;= nums.length &lt;= 100\n0 &lt;= nums[i] &lt;= 400\n\n\nApproaches"
  },
  {
    "objectID": "programming/program-parity-invariance/parity-invariance.html",
    "href": "programming/program-parity-invariance/parity-invariance.html",
    "title": "Parity invariance",
    "section": "",
    "text": "Parity invariance means that the number of times a position is flipped determines its final value. If a position is flipped an odd number of times, its value changes, but if it is flipped an even number of times, it stays the same.\nConsider the array [1, 0, 0, 1, 0, 1, 1]. We start by flipping three consecutive elements to try and transform all 0s into 1s. First, flipping the subarray [0, 0, 1] at indices 1..3 changes the array to [1, 1, 1, 0, 0, 1, 1]. Then, flipping [0, 0, 1] at indices [3..5] gives [1, 1, 1, 1, 1, 0, 1]. Finally, flipping [1, 0, 1] at indices [1, 0, 1] at indices 4..6 results in [1, 1, 1, 1, 1, 1, 0, 1, 0].\nAt this point, we see that the 0s at positions 4 and 6 remain, and there is no way to flip them without also flipping the other elements. Since we can only flip three elements at a time, we cannot isolate these 0s in a way that allow us to change them to 1s. This happens because these positions were flipped an even number of times, so they retain their original value. Because of this parity constraint, the transformation is impossible."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Write more markdown here!\nI’am Hung"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#i.-overview-of-statistical-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#i.-overview-of-statistical-inference",
    "title": "Statistical Inference and Learning",
    "section": "I. Overview of Statistical Inference",
    "text": "I. Overview of Statistical Inference\n\nDefinition:\nStatistical inference (often called “learning” in computer science) is the process of using data to deduce the underlying distribution \\(F\\) that generated the data. This may involve estimating the entire distribution or specific features (such as the mean).\nApplications:\n\nExtracting meaningful information from data\n\nMaking informed decisions and predictions\n\nServing as the foundation for more advanced topics in statistics and machine learning"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#ii.-modeling-approaches",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#ii.-modeling-approaches",
    "title": "Statistical Inference and Learning",
    "section": "II. Modeling Approaches",
    "text": "II. Modeling Approaches\n\nA. Parametric Models\n\nDefinition:\nA model defined by a finite number of parameters.\n\nExample (Normal Distribution):\n\\[\nf(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nCharacteristics:\n\nSimpler to analyze and interpret\n\nMore efficient when the assumptions hold true\n\n\n\n\nB. Nonparametric Models\n\nDefinition:\nModels that do not restrict the distribution to a finite-dimensional parameter space.\nExamples:\n\nEstimating the entire cumulative distribution function (cdf)\\(F\\)\n\nEstimating a probability density function (pdf) with smoothness assumptions (e.g., assuming the pdf belongs to a [[Sobolev space]])\n\nCharacteristics:\n\nGreater flexibility to model complex data\n\nFewer assumptions about the form of the distribution"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.1-one-dimensional-parametric-estimation--",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.1-one-dimensional-parametric-estimation--",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.1: One-Dimensional Parametric Estimation -",
    "text": "Example 6.1: One-Dimensional Parametric Estimation -\nScenario: We observe independent Bernoulli(\\(p\\)) random variables \\(X_1, X_2, \\dots, X_n\\).\nGoal: Estimate the unknown parameter \\(p\\) (the probability of success).\nEstimator: The natural estimator is the sample mean: \\[ \\hat{p}_n = \\frac{1}{n}\\sum_{i=1}^n X_i. \\] Key Points: - Unbiasedness: \\[E(\\hat{p}_n) = p.\\] Thus, the estimator is unbiased.\nVariance: Since \\[\\operatorname{Var}(X_i) = p(1-p)\\], the variance of the estimator is\n\\[ \\operatorname{Var}(\\hat{p}_n) = \\frac{p(1-p)}{n}. \\] - Consistency: As \\(n\\) increases, the variance shrinks, making \\(\\hat{p}_n\\) a consistent estimator of \\(p\\).\n\n# Example 6.1: One-Dimensional Parametric Estimation (Bernoulli)\nimport numpy as np\n\n# True parameter for Bernoulli distribution\np_true = 0.7\nn = 1000  # number of observations\n\n# Generate n independent Bernoulli(p) observations (0 or 1)\nX = np.random.binomial(1, p_true, n)\n\n# Estimator: sample mean is the natural estimator for p\np_hat = np.mean(X)\n\nprint(\"Example 6.1: Bernoulli Parameter Estimation\")\nprint(\"True p:\", p_true)\nprint(\"Estimated p:\", p_hat)"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.2-two-dimensional-parametric-estimation--",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.2-two-dimensional-parametric-estimation--",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.2: Two-Dimensional Parametric Estimation -",
    "text": "Example 6.2: Two-Dimensional Parametric Estimation -\nScenario: Suppose \\[X_1, X_2, \\dots, X_n\\] are independent observations from a distribution \\[F\\] whose probability density function is given by a parametric family: \\[ f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right). \\] - Goal: Estimate the two parameters: the mean \\[\\mu\\] and the standard deviation \\[\\sigma\\]. - Nuisance Parameter: If we are primarily interested in \\[\\mu\\], then \\[\\sigma\\] becomes a nuisance parameter—an additional parameter that must be estimated but is not of direct interest. - Key Points: - Multidimensionality: The estimation problem involves simultaneous estimation of \\[\\mu\\] and \\[\\sigma\\]. - Methods: Techniques such as maximum likelihood estimation (MLE) are commonly used, sometimes incorporating methods (like profile likelihood) to eliminate the effect of nuisance parameters when focusing on \\[\\mu\\]. . # Analytical Explanation of Two Nonparametric Estimation Examples\nBelow, we analyze and explain two examples that illustrate nonparametric estimation techniques: one for estimating the cumulative distribution function (CDF) and another for estimating the probability density function (PDF)."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.3-nonparametric-estimation-of-the-cdf",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.3-nonparametric-estimation-of-the-cdf",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.3: Nonparametric Estimation of the CDF",
    "text": "Example 6.3: Nonparametric Estimation of the CDF\n\nProblem Statement\n\nData:\nWe have independent observations \\[X_1, X_2, \\dots, X_n\\] drawn from an unknown distribution with CDF \\[F\\].\nObjective:\nEstimate the entire cumulative distribution function \\[F\\], assuming minimal assumptions—namely, that \\[F\\] is any valid CDF (denoted by \\[\\mathcal{F}_{\\text{ALL}}\\]).\n\n\n\nApproach\n\nEstimator:\nThe natural nonparametric estimator for the CDF is the empirical distribution function (EDF) defined as: \\[\n\\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\},\n\\] where \\[\\mathbf{1}\\{X_i \\le x\\}\\] is an indicator function that is 1 if \\[X_i \\le x\\] and 0 otherwise.\n\n\n\nWhy This Works\n\nMinimal Assumptions:\nNo specific parametric form for \\[F\\] is assumed; all that is required is that \\[F\\] is a valid CDF. This makes the method very general.\nConvergence Properties:\nThe Glivenko–Cantelli theorem guarantees that the empirical CDF converges uniformly to the true CDF: \\[\n\\sup_x \\left| \\hat{F}_n(x) - F(x) \\right| \\to 0 \\quad \\text{as} \\quad n \\to \\infty.\n\\] This property ensures that the estimator is consistent.\nIntuitive Interpretation:\nThe EDF simply calculates the proportion of observations less than or equal to a given value, which is the natural way to “build” the CDF from data."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.4-nonparametric-density-estimation",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#example-6.4-nonparametric-density-estimation",
    "title": "Statistical Inference and Learning",
    "section": "Example 6.4: Nonparametric Density Estimation",
    "text": "Example 6.4: Nonparametric Density Estimation\n\nProblem Statement\n\nData:\nAgain, we have independent observations \\[X_1, X_2, \\dots, X_n\\] from a distribution with CDF \\[F\\]. Let the associated PDF be \\[f = F'\\].\nObjective:\nEstimate the PDF \\[f\\]. However, unlike the CDF, estimating the density function nonparametrically is not possible under the sole assumption that \\[F\\] is any CDF.\n\n\n\nNeed for Additional Assumptions\n\nIll-Posed Without Smoothness:\nThe space of all CDFs (denoted by \\[\\mathcal{F}_{\\text{ALL}}\\]) is too vast; a generic CDF need not be differentiable. Even if a density exists, it can be highly irregular, making consistent estimation difficult or impossible.\nIntroducing Smoothness via Sobolev Spaces:\nTo estimate \\[f\\] reliably, we assume that \\[f\\] belongs to a more restricted function class. One common assumption is that \\[f\\] lies in a Sobolev space (denoted by \\[\\mathcal{F}_{\\text{SOB}}\\]).\nFor instance, one might assume: \\[\n\\mathcal{F}_{\\text{SOB}} = \\left\\{ f \\in \\mathcal{F}_{\\text{DENS}} : \\int \\left(f^{(s)}(x)\\right)^2 dx &lt; \\infty \\right\\},\n\\] where:\n\n\\[\\mathcal{F}_{\\text{DENS}}\\] is the set of all probability density functions.\n\\[f^{(s)}(x)\\] denotes the \\[s\\]-th derivative of \\[f\\].\nThe condition \\[\\int \\left(f^{(s)}(x)\\right)^2 dx &lt; \\infty\\] ensures that \\[f\\] is not “too wiggly” or irregular.\n\n\n\n\nEstimation Methods\n\nKernel Density Estimation (KDE):\nWith the smoothness assumption in place, methods such as kernel density estimation can be employed. A kernel density estimator has the form: \\[\n\\hat{f}_n(x) = \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right),\n\\] where:\n\n\\[K(\\cdot)\\] is a smooth kernel function (e.g., Gaussian).\n\\[h\\] is a bandwidth parameter that controls the smoothness of the estimate.\n\n\n\n\nWhy These Assumptions are Necessary\n\nRegularization:\nThe smoothness condition imposed by the Sobolev space helps regularize the estimation problem. It restricts the set of possible densities to those that have bounded variation or a controlled number of oscillations.\nImproved Convergence:\nSmoothness assumptions lead to better convergence properties of the density estimator, allowing for rates of convergence that can be rigorously analyzed.\nPractical Feasibility:\nIn many real-world scenarios, the underlying density is indeed smooth (e.g., physical phenomena, economic variables), making this assumption both realistic and useful."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#summary",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#summary",
    "title": "Statistical Inference and Learning",
    "section": "Summary",
    "text": "Summary\n\nExample 6.3:\n\nTask: Estimate the CDF \\[F\\] from data with minimal assumptions.\nMethod: Use the empirical CDF \\[\\hat{F}_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\}\\].\nKey Property: Convergence guaranteed by the Glivenko–Cantelli theorem.\n\nExample 6.4:\n\nTask: Estimate the density \\[f\\] from data.\nChallenge: Estimation is ill-posed without additional assumptions.\nSolution: Assume that \\[f\\] is smooth by requiring it to belong to a Sobolev space (e.g., \\[\\mathcal{F}_{\\text{DENS}} \\cap \\mathcal{F}_{\\text{SOB}}\\]), then use methods like kernel density estimation.\nBenefit: Smoothness constraints make the problem well-posed and lead to estimators with favorable convergence properties.\n\n\nThese examples highlight the progression from estimating a distribution function under minimal assumptions to needing extra regularity conditions when estimating derivatives like the density."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iii.-core-concepts-in-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iii.-core-concepts-in-inference",
    "title": "Statistical Inference and Learning",
    "section": "III. Core Concepts in Inference",
    "text": "III. Core Concepts in Inference\n\n1. Point Estimation\n\nConcept:\nA point estimator is a function of the data, denoted as\n\\[\n\\hat{\\theta}_n = g(X_1, X_2, \\dots, X_n)\n\\]\nused to provide a single “best guess” for the unknown parameter \\[\\theta\\].\nKey Properties:\n\nBias:\n\\[\n\\text{bias}(\\hat{\\theta}_n) = E(\\hat{\\theta}_n) - \\theta\n\\]\nVariance and Standard Error (se):\n\\[\n\\text{se} = \\sqrt{Var(\\hat{\\theta}_n)}\n\\]\nMean Squared Error (MSE):\n\\[\n\\text{mse} = E\\left[(\\hat{\\theta}_n - \\theta)^2\\right] = \\text{bias}^2(\\hat{\\theta}_n) + Var(\\hat{\\theta}_n)\n\\]\nConsistency:\nAn estimator is consistent if\n\\[\n\\hat{\\theta}_n \\xrightarrow{P} \\theta \\quad \\text{as } n \\to \\infty\n\\]\nAsymptotic Normality:\nMany estimators satisfy\n\\[\n\\frac{\\hat{\\theta}_n - \\theta}{\\text{se}} \\approx N(0, 1)\n\\]\nfor large samples, which facilitates the construction of confidence intervals.\n\n\n\n\n2. Confidence Sets\n\nConcept:\nA confidence interval (or set) is a range constructed from the data that, over many repetitions of the experiment, contains the true parameter \\[\\theta\\] with a specified probability (coverage).\nExample (Normal-Based Interval):\nWhen \\[\\hat{\\theta}_n\\] is approximately normally distributed, an approximate \\[1-\\alpha\\] confidence interval is: \\[\nC_n = \\left( \\hat{\\theta}_n - z_{\\alpha/2}\\,\\text{se}, \\quad \\hat{\\theta}_n + z_{\\alpha/2}\\,\\text{se} \\right)\n\\] where \\[z_{\\alpha/2}\\] is the quantile of the standard Normal distribution such that \\[\nP(Z &gt; z_{\\alpha/2}) = \\frac{\\alpha}{2}.\n\\]\n\n\n\n3. Hypothesis Testing\n\nConcept:\nHypothesis testing involves formulating a null hypothesis \\[H_0\\] (a default statement, such as a coin being fair) and an alternative hypothesis \\[H_1\\], then using the data to decide whether there is sufficient evidence to reject \\[H_0\\].\nExample (Testing Coin Fairness):\n\\[\nH_0: p = 0.5 \\quad \\text{versus} \\quad H_1: p \\neq 0.5\n\\]\nProcess:\n\nDefine an appropriate test statistic (e.g., \\[T = |\\hat{p}_n - 0.5|\\])\n\nSet a significance level \\[\\alpha\\]\n\nDetermine the rejection region based on \\[\\alpha\\] or compute a p-value\n\nReject \\[H_0\\] if the test statistic falls into the rejection region"
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iv.-frequentist-vs.-bayesian-inference",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#iv.-frequentist-vs.-bayesian-inference",
    "title": "Statistical Inference and Learning",
    "section": "IV. Frequentist vs. Bayesian Inference",
    "text": "IV. Frequentist vs. Bayesian Inference\n\nFrequentist Inference:\n\nTreats parameters as fixed but unknown\n\nFocuses on the properties of estimators over repeated sampling (e.g., confidence intervals, hypothesis tests)\n\nBayesian Inference:\n\nTreats parameters as random variables with prior distributions\n\nUses Bayes’ theorem to update beliefs in light of new data, allowing direct probability statements about parameters\n\nComparison:\n\nFrequentist methods emphasize long-run frequency properties.\n\nBayesian methods provide a framework for incorporating prior knowledge and making probabilistic statements about parameters."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#v.-additional-information",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#v.-additional-information",
    "title": "Statistical Inference and Learning",
    "section": "V. Additional Information",
    "text": "V. Additional Information\n\nBibliographic References\n\nElementary Level:\n\nDeGroot and Schervish (2002)\n\nLarsen and Marx (1986)\n\nIntermediate Level:\n\nCasella and Berger (2002)\n\nBickel and Doksum (2000)\n\nRice (1995)\n\nAdvanced Level:\n\nCox and Hinkley (2000)\n\nLehmann and Casella (1998)\n\nLehmann (1986)\n\nvan der Vaart (1998)\n\n\n\n\nExercises\n\nPoisson Estimation:\nFor \\[X_1, X_2, \\dots, X_n \\sim \\text{Poisson}(\\lambda)\\] with the estimator\n\\[\n\\hat{\\lambda} = \\frac{1}{n}\\sum_{i=1}^n X_i,\n\\]\ndetermine the bias, standard error, and mean squared error.\nUniform Distribution Estimation (Method 1):\nFor \\[X_1, X_2, \\dots, X_n \\sim \\text{Uniform}(0, \\theta)\\] and the estimator\n\\[\n\\hat{\\theta} = \\max\\{X_1, X_2, \\dots, X_n\\},\n\\]\ncalculate the bias, standard error, and mse.\nUniform Distribution Estimation (Method 2):\nFor the same model with the estimator\n\\[\n\\hat{\\theta} = 2X_{(n)},\n\\]\nwhere \\[X_{(n)}\\] is the maximum, compute the bias, standard error, and mse."
  },
  {
    "objectID": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#vi.-key-takeaways",
    "href": "math/math-statistic-statistical-inference-and-learning/Statistical-Inference-and-Learning.html#vi.-key-takeaways",
    "title": "Statistical Inference and Learning",
    "section": "VI. Key Takeaways",
    "text": "VI. Key Takeaways\n\nInference Fundamentals:\nLearning how to deduce properties of a population from a sample is central to statistics and machine learning.\nModel Choice:\n\nParametric models are simpler but rely on strong assumptions.\n\nNonparametric models offer flexibility with fewer assumptions.\n\nEstimator Evaluation:\nProperties such as bias, variance (or standard error), and mean squared error are essential in assessing the quality of estimators.\nConfidence and Testing:\n\nConfidence intervals quantify the uncertainty in estimates.\n\nHypothesis testing provides a formal framework for decision-making.\n\nPhilosophical Approaches:\nThe frequentist and Bayesian paradigms provide different perspectives on probability and inference, influencing how uncertainty is quantified and interpreted."
  },
  {
    "objectID": "math/math-statistic-why-normal-distribution-formular/index.html",
    "href": "math/math-statistic-why-normal-distribution-formular/index.html",
    "title": "Why we have normal distribution formular",
    "section": "",
    "text": "To do\n\n\n\nThis is just a photo I took last time I was in Dundee.\n\n\nThat’s it! go for it!"
  },
  {
    "objectID": "english/performance-enhancing-drug/index.html",
    "href": "english/performance-enhancing-drug/index.html",
    "title": "Performance-Enhancing Drugs in Sports",
    "section": "",
    "text": "In recent years, there has been a rise in the use of performance-enhancing substances in sports. What are the consequences of doping for athletes? What measures should be taken to combat this issue?\nThere has been a considerable rise in detections for performance-enhancing substances in athletic competitions recently. In my opinion, the most likely consequences for athletes concern their careers and this infraction is best tackled through more sophisticated screening tests.\nIn the majority of cases, athletes risk lasting career repercussions when caught doping. The first and most obvious consequences are financial. An athlete who tests positive for a banned substance will certainly be banned for a defined amount of time, potentially permanently in serious cases, and will forfeit salary and prize money over this period. Beyond raw salary, most athletes receive endorsements and a worsening public image will undoubtedly damage those business relationships as well. Relatedly, an athlete’s legacy is also at stake. A well-known example of this would be the baseball player Barry Bonds who not only lost the final years of an illustrious career to suspensions but has also not been inducted into the Baseball Hall of Fame due to his past transgressions and numerous failed tests.\nIn order to combat the pervasiveness of performance-enhancing drugs, sporting authorities must invest in modernized testing procedures. The most complex testing is currently utilized in the Olympics, however there are flaws in this system as it is often compromised by corruption and the equally sophisticated doping methods of certain nations. A more pertinent case of study would be in sports such as boxing and MMA where testing can literally mean the difference between life and death. In boxing, athletes often agree to their own testing procedures contractually. This method is inferior to vesting power in an authority, such as the UFC, that can ensure tests are modern and fairly conducted. Most UFC athletes are tested at random points throughout the year as well as prior to and immediately after bouts. Applying such a policy across various sports would not only catch the bast majority of infractions but also, more importantly, deter abuse even beginning.\nIn conclusion, athletes risk reputational and financial damage by doping and this can be prevented as testing becomes more comprehensive and potent. These measures are necessary to safeguard the integrity of sporting competitions around the world.\n\nIn recent years, the use of performance-enhancing drugs in sports has increased, posing significant challenges to fair competition. Doping not only harm an athlete’s career and finances but also damages the trust between fans, sponsors, and sporting institutions. I firmly believe that to protect the integrity of sports, it is essential to implement more advanced and transparent testing procedures.\nAthletes who resort to doping face serious long-term consequences that extend beyond temporary suspensions. When an athletes tests positive for banned substances, they risk losing salaries, prize money, and lucrative endorsements. For example, a renowned baseball player might be suspended, causing them to forfeit not only their immediate earnings but also potential future opportunities, such as Hall of Fame induction. This example clearly shows that doping can irreversibly tarnish an athlete’s legacy and career, leading to both financial ruin and a permanent stain on their reputation.\nTo effectively counter the issue of doping, sports authorities must adopt modern, comprehensive testing protocols. While major competitions like the Olympics employ advanced testing, the current is sometimes undermined by corruption and the ever-evolving sophistication of doping techniques. In sports like boxing and mixed martial arts, for instance, some athletes are subjected to self-regulated tests and lack objectivity. In contrast, organizations such as the UFC implement rigorous, random testing both before and after events, ensuring a fairer system. Adopting these strict measures across all sports would not only detect infractions more reliably but also serve as a strong deterrent against doping. By ensuring that testing is both modern and impartial, the integrity of competitive sports can be maintained, safeguarding athletes’ careers and reputation of the sport.\nIn conclusion, doping leads to severe financial and reputational damage for athletes while undermining the fairness of sports competitions. Only through the implementation of advanced, transparent testing procedures can we hope to deter doping and preserve the true spirit of sportsmanship for future generations.\n\nVocabulary\nFor extra practice, write an antonym (opposite word) on a piece of paper to help you remember the new vocabulary: 1. There has been a considerable rise in detections - There has been a significant increase in identifications 2. performance-enhancing substances - performance-boosting substances 3. athletic competitions - sporting events 4. most likely consequences for - probable outcomes for"
  },
  {
    "objectID": "english/two-question-essay/index.html",
    "href": "english/two-question-essay/index.html",
    "title": "How to Structure a ‘Two Question’ Essay",
    "section": "",
    "text": "Let’s look at an example of this kind of question:\n*Nowadays governments are investing more in public transport such as buses and trains rather than in building new roads.*\n*What are the reasons for this?*\n*Is this a positive or negative development?*\n\nClear position\nFor this kind of task we need to discuss both questions. In the first question, we are given a situation and asked to explain why. The second question requires a clear opinion so we must choose a side. This is very important for our task response score. If we don’t choose a side and support that ‘position’ clearly, we will lose marks for our task response. We will get a maximum of 5 for task achievement if we do not choose a side.\nIt’s not important which side we choose as long as we choose one. Here are the options for the second questions:\n\nAgree: We think it’s a positive development that governments are spending more money on public transport. We support this opinion.\nDisagree: We think it’s a negative development that governments are spending more money on public transport. We support this opinion.\n\n\n\nClear essay structure\nDavid Lang recommend that we should give our opinion in the introduction as well as the conclusion (for all IELTS Task 2 questions). It’s OK to write a very general introduction and leave our opinion to the end. But it’s not as clear and we might run out of time before we’ve hot to our conclusion, so overall it’s a risky strategy. If we don’t choose a side and support that ‘position’ clearly, we will lose marks for our task response.\nTherefore, him suggested essay structure is four paragraphs. The opinion is presented in the introduction and re-stated in the conclusion. Ideally each body paragraph in the introduction and re-stated in the conclusion.\n\n\nPossible Essay structures\n1. Agree - we feel it’s a positive development that governments are spending more money on public transport.\nWe need at least one good reason why it’s happening and one good reason why it’s a positive development.\n\n\n\n\n\n\n\nParagraph 1 (intro)\nParaphrase both questions and state your overall opinion (you feel it’s a positive development)\n\n\nParagraph 2 (body 1)\nExplain and support your reason why governments are spending more money on public transport.\n\n\nParagraph 3 (body 2)\nExplain and support your reason why this is a positive development.\n\n\nParagraph 4 (conclusion)\nRe-state your overall opinion (you feel it’s a positive development) and  summarise your main reasons.\n\n\n\n2. Disagree: - we feel it’s a negative development that governments are spending more money on public transport.\nWe need at least one good reason why it’s happening and one good reason why it’s a negative development. Only the parts highlighted below are different from the first essay\n\n\n\n\n\n\n\n#### Paragraph 1 (intro)\n#### Paraphrase both questions and state your overall opinion (you feel it’s a negative development)\n\n\n#### Paragraph 2 (body 1)\n#### Explain and support your reason why governments are spending more money on public transport.\n\n\n#### Paragraph 3 (body 2)\n#### Explain and support your reason why this is a negative development.\n\n\n#### Paragraph 4 (conclusion)\n#### Re-state your overall opinion (you feel it’s a negative development) and  summarise your main reasons."
  },
  {
    "objectID": "english/english-writting-zoos/zoos.html",
    "href": "english/english-writting-zoos/zoos.html",
    "title": "Zoos",
    "section": "",
    "text": "Zoos have long been a subject of debate. While some people argue that zoos play a crucial role in conserving endangered species and educating the public about wildlife, others contend that confining animals to artificial environments is unethical. Discuss both views and give your own opinion.\n\nAnalysis\nAdvantage: pros: Conservation zoos often participate in breeding programs and reintroduction efforts for endangered species. Education: They provide a unique opportunity for the public to learn about wildlife and environmental issues. Research: Zoos support scientific research and rehabilitation programs that can benefit species survival\nDisadvantage Cons: Ethical concerns: Confining animals can lead to physical and psychological stress, raising serious animal welfare issues. Natural Behavior: The artificial habitats in zoos may prevent animals from expressing their natural behaviors, potentially leading to a diminished quality of life.\n\n\nSample\nIn recent years, zoos have become a focal point of heated debate. On the one hand, proponents argue that zoos are indispensable for conserving endangered species and educating the public. On the other hand, critics maintain that confining animals to unnatural settings is inherently unethical. In my view, while the welfare of captive animals must be a priority, the benefits of well-managed zoos in conservation and education are substantial.\nSupporters of zoos highlight their critical role in preserving biodiversity. With numerous species facing extinction, zoos provide a controlled environment where endangered animals can be bred and nurtured away from the threats of habitat loss and poaching. Many institutions have pioneered successful breeding programs, subsequently reintroducing species into the wild. Moreover, zoos serve as a vital educational platform, offering visitors first-hand exposure to wildlife. This experience not only fosters environmental awareness but also inspires future generations to engage in conservation efforts. Additionally, ongoing research in these settings has led to significant advancements in veterinary medicine and species rehabilitation.\nConversely, opponents argue that the confinement of animals infringes on their natural rights. Critics assert that even the most well-intentioned zoos cannot replicate the complex ecosystems that wild animals depend on, often leading to behavioral and psychological issues. The ethical debate is further intensified by instances of inadequate care and substandard living conditions in some facilities. While these concerns are valid, they underscore the need for stringent regulations rather than the outright dismissal of zoos as an institution.\nIn conclusion, although the ethical implications of captivity are significant, I believe that the conservation, education, and research benefits provided by zoos justify their existence - provided that animal welfare is rigorously safeguarded. This balanced approach can ensure that zoos continue to contribute positively to both wildlife preservation and public understanding of environmental issues."
  },
  {
    "objectID": "posts/post_template/index.html",
    "href": "posts/post_template/index.html",
    "title": "Post template",
    "section": "",
    "text": "Just write more markdown here!"
  },
  {
    "objectID": "posts/post_template/index.html#this-is-a-subsection",
    "href": "posts/post_template/index.html#this-is-a-subsection",
    "title": "Post template",
    "section": "This is a subsection",
    "text": "This is a subsection\nSubsection text! You get it now I assume\nThis is how you reference an image in your blog post\n\n\n\nThis is just a photo I took last time I was in Dundee.\n\n\nThat’s it! go for it!"
  },
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "Programming",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 21, 2025\n\n\nDisjoint Set Union\n\n\nKim Hung Bui\n\n\n\n\nMar 21, 2025\n\n\nMinimum Cost Walk in Weighted Graph\n\n\nKim Hung Bui\n\n\n\n\nMar 20, 2025\n\n\n3191. Minimum Operations to Make Binary Array Elements Equal to One I\n\n\nHung \n\n\n\n\nMar 20, 2025\n\n\nParity invariance\n\n\nHung \n\n\n\n\nMar 18, 2025\n\n\nleetcode-Longest-Nice-Subarray\n\n\nHung \n\n\n\n\nMar 17, 2025\n\n\nDivide Array Into Equal Pairs\n\n\nHung \n\n\n\n\nMar 17, 2025\n\n\nHow to approach most DP problems\n\n\nKim Hung Bui\n\n\n\n\nMar 17, 2025\n\n\nHouse robber\n\n\nHung \n\n\n\n\n\nNo matching items"
  }
]