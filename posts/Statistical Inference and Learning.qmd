

---
## I. Overview of Statistical Inference

- **Definition:**  
  Statistical inference (often called "learning" in computer science) is the process of using data to deduce the underlying distribution $$F$$ that generated the data. This may involve estimating the entire distribution or specific features (such as the mean).

- **Applications:**  
  - Extracting meaningful information from data  
  - Making informed decisions and predictions  
  - Serving as the foundation for more advanced topics in statistics and machine learning

---

## II. Modeling Approaches

### A. Parametric Models

- **Definition:**  
  A model defined by a finite number of parameters.  
- **Example (Normal Distribution):**  
  $$ 
  f(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
  $$
- **Characteristics:**  
  - Simpler to analyze and interpret  
  - More efficient when the assumptions hold true

### B. Nonparametric Models

- **Definition:**  
  Models that do not restrict the distribution to a finite-dimensional parameter space.
- **Examples:**  
  - Estimating the entire cumulative distribution function (cdf) $$F$$  
  - Estimating a probability density function (pdf) with smoothness assumptions (e.g., assuming the pdf belongs to a [[Sobolev space]])
- **Characteristics:**  
  - Greater flexibility to model complex data  
  - Fewer assumptions about the form of the distribution

---

## Example 6.1: One-Dimensional Parametric Estimation - 
**Scenario:** We observe independent Bernoulli($$p$$) random variables $$X_1, X_2, \dots, X_n$$. - **Goal:** Estimate the unknown parameter $$p$$ (the probability of success). - **Estimator:** The natural estimator is the sample mean: $$ \hat{p}_n = \frac{1}{n}\sum_{i=1}^n X_i. $$ - **Key Points:** - **Unbiasedness:** $$E(\hat{p}_n) = p.$$ Thus, the estimator is unbiased. - **Variance:** Since $$\operatorname{Var}(X_i) = p(1-p)$$, the variance of the estimator is $$ \operatorname{Var}(\hat{p}_n) = \frac{p(1-p)}{n}. $$ - **Consistency:** As $$n$$ increases, the variance shrinks, making $$\hat{p}_n$$ a consistent estimator of $$p$$.

## Example 6.2: Two-Dimensional Parametric Estimation - 
**Scenario:** Suppose $$X_1, X_2, \dots, X_n$$ are independent observations from a distribution $$F$$ whose probability density function is given by a parametric family: $$ f(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right). $$ - **Goal:** Estimate the two parameters: the mean $$\mu$$ and the standard deviation $$\sigma$$. - **Nuisance Parameter:** If we are primarily interested in $$\mu$$, then $$\sigma$$ becomes a nuisance parameterâ€”an additional parameter that must be estimated but is not of direct interest. - **Key Points:** - **Multidimensionality:** The estimation problem involves simultaneous estimation of $$\mu$$ and $$\sigma$$. - **Methods:** Techniques such as maximum likelihood estimation (MLE) are commonly used, sometimes incorporating methods (like profile likelihood) to eliminate the effect of nuisance parameters when focusing on $$\mu$$.

## III. Core Concepts in Inference

### 1. Point Estimation

- **Concept:**  
  A point estimator is a function of the data, denoted as  
  $$ 
  \hat{\theta}_n = g(X_1, X_2, \dots, X_n)
  $$  
  used to provide a single "best guess" for the unknown parameter $$\theta$$.

- **Key Properties:**
  - **Bias:**  
    $$ 
    \text{bias}(\hat{\theta}_n) = E(\hat{\theta}_n) - \theta
    $$
  - **Variance and Standard Error (se):**  
    $$ 
    \text{se} = \sqrt{Var(\hat{\theta}_n)}
    $$
  - **Mean Squared Error (MSE):**  
    $$ 
    \text{mse} = E\left[(\hat{\theta}_n - \theta)^2\right] = \text{bias}^2(\hat{\theta}_n) + Var(\hat{\theta}_n)
    $$
  - **Consistency:**  
    An estimator is consistent if  
    $$ 
    \hat{\theta}_n \xrightarrow{P} \theta \quad \text{as } n \to \infty
    $$
  - **Asymptotic Normality:**  
    Many estimators satisfy  
    $$ 
    \frac{\hat{\theta}_n - \theta}{\text{se}} \approx N(0, 1)
    $$  
    for large samples, which facilitates the construction of confidence intervals.

### 2. Confidence Sets

- **Concept:**  
  A confidence interval (or set) is a range constructed from the data that, over many repetitions of the experiment, contains the true parameter $$\theta$$ with a specified probability (coverage).

- **Example (Normal-Based Interval):**  
  When $$\hat{\theta}_n$$ is approximately normally distributed, an approximate $$1-\alpha$$ confidence interval is:
  $$
  C_n = \left( \hat{\theta}_n - z_{\alpha/2}\,\text{se}, \quad \hat{\theta}_n + z_{\alpha/2}\,\text{se} \right)
  $$
  where $$z_{\alpha/2}$$ is the quantile of the standard Normal distribution such that
  $$ 
  P(Z > z_{\alpha/2}) = \frac{\alpha}{2}.
  $$

### 3. Hypothesis Testing

- **Concept:**  
  Hypothesis testing involves formulating a null hypothesis $$H_0$$ (a default statement, such as a coin being fair) and an alternative hypothesis $$H_1$$, then using the data to decide whether there is sufficient evidence to reject $$H_0$$.

- **Example (Testing Coin Fairness):**  
  $$ 
  H_0: p = 0.5 \quad \text{versus} \quad H_1: p \neq 0.5
  $$
- **Process:**  
  - Define an appropriate test statistic (e.g., $$T = |\hat{p}_n - 0.5|$$)  
  - Set a significance level $$\alpha$$  
  - Determine the rejection region based on $$\alpha$$ or compute a p-value  
  - Reject $$H_0$$ if the test statistic falls into the rejection region

---

## IV. Frequentist vs. Bayesian Inference

- **Frequentist Inference:**  
  - Treats parameters as fixed but unknown  
  - Focuses on the properties of estimators over repeated sampling (e.g., confidence intervals, hypothesis tests)

- **Bayesian Inference:**  
  - Treats parameters as random variables with prior distributions  
  - Uses Bayes' theorem to update beliefs in light of new data, allowing direct probability statements about parameters

- **Comparison:**  
  - **Frequentist methods** emphasize long-run frequency properties.  
  - **Bayesian methods** provide a framework for incorporating prior knowledge and making probabilistic statements about parameters.

---

## V. Additional Information

### Bibliographic References

- **Elementary Level:**  
  - DeGroot and Schervish (2002)  
  - Larsen and Marx (1986)

- **Intermediate Level:**  
  - Casella and Berger (2002)  
  - Bickel and Doksum (2000)  
  - Rice (1995)

- **Advanced Level:**  
  - Cox and Hinkley (2000)  
  - Lehmann and Casella (1998)  
  - Lehmann (1986)  
  - van der Vaart (1998)

### Exercises

1. **Poisson Estimation:**  
   For $$X_1, X_2, \dots, X_n \sim \text{Poisson}(\lambda)$$ with the estimator  
   $$ 
   \hat{\lambda} = \frac{1}{n}\sum_{i=1}^n X_i,
   $$  
   determine the bias, standard error, and mean squared error.

2. **Uniform Distribution Estimation (Method 1):**  
   For $$X_1, X_2, \dots, X_n \sim \text{Uniform}(0, \theta)$$ and the estimator  
   $$ 
   \hat{\theta} = \max\{X_1, X_2, \dots, X_n\},
   $$  
   calculate the bias, standard error, and mse.

3. **Uniform Distribution Estimation (Method 2):**  
   For the same model with the estimator  
   $$ 
   \hat{\theta} = 2X_{(n)},
   $$  
   where $$X_{(n)}$$ is the maximum, compute the bias, standard error, and mse.

---

## VI. Key Takeaways

- **Inference Fundamentals:**  
  Learning how to deduce properties of a population from a sample is central to statistics and machine learning.

- **Model Choice:**  
  - **Parametric models** are simpler but rely on strong assumptions.  
  - **Nonparametric models** offer flexibility with fewer assumptions.

- **Estimator Evaluation:**  
  Properties such as bias, variance (or standard error), and mean squared error are essential in assessing the quality of estimators.

- **Confidence and Testing:**  
  - Confidence intervals quantify the uncertainty in estimates.  
  - Hypothesis testing provides a formal framework for decision-making.

- **Philosophical Approaches:**  
  The frequentist and Bayesian paradigms provide different perspectives on probability and inference, influencing how uncertainty is quantified and interpreted.

---

These detailed notes, organized into clear sections and using $$ for mathematical expressions, should provide a comprehensive reference and enhance your understanding of the fundamental concepts in statistical inference and learning.
