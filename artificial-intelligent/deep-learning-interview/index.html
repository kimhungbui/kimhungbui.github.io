<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Your name">
<meta name="dcterms.date" content="2025-06-12">
<meta name="description" content="Your post description">

<title>Logistic regression – Kim Hung’s ThinkStack</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-R5TB4V4ZYS"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-R5TB4V4ZYS', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean"
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

<!-- 
Load Academicons v1: https://jpswalsh.github.io/academicons/
-->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

<link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


<!---
The following code are needed to show dimension citation and altmetrics.
https://api.altmetric.com/embeds.html
https://badge.dimensions.ai/
--->

<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

<script async="" src="https://badge.dimensions.ai/badge.js" charset="utf-8"></script>

<script type="text/javascript" src="//cdn.plu.mx/widget-popup.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Kim Hung’s ThinkStack</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.md"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.html"> 
<span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-mathematics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Mathematics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-mathematics">    
        <li>
    <a class="dropdown-item" href="../../math/convex-optimization/">
 <span class="dropdown-text">Convex Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../math/probability/">
 <span class="dropdown-text">Probability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../math/statistics/">
 <span class="dropdown-text">Statistics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../artificial-intelligent.html"> 
<span class="menu-text">AI</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../programming.html"> 
<span class="menu-text">Programming</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../english.html"> 
<span class="menu-text">English</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../posts.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-logistic-regression" id="toc-introduction-to-logistic-regression" class="nav-link active" data-scroll-target="#introduction-to-logistic-regression"><span class="header-section-number">1</span> Introduction to Logistic Regression</a></li>
  <li><a href="#general-concepts" id="toc-general-concepts" class="nav-link" data-scroll-target="#general-concepts"><span class="header-section-number">2</span> General Concepts</a>
  <ul class="collapse">
  <li><a href="#problem-1-increase-observation" id="toc-problem-1-increase-observation" class="nav-link" data-scroll-target="#problem-1-increase-observation"><span class="header-section-number">2.1</span> Problem 1: Increase observation</a>
  <ul class="collapse">
  <li><a href="#explanation" id="toc-explanation" class="nav-link" data-scroll-target="#explanation"><span class="header-section-number">2.1.1</span> Explanation</a></li>
  <li><a href="#drawbacks-of-this-strategy" id="toc-drawbacks-of-this-strategy" class="nav-link" data-scroll-target="#drawbacks-of-this-strategy"><span class="header-section-number">2.1.2</span> Drawbacks of This Strategy</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.1.3</span> Summary</a></li>
  <li><a href="#best-practice" id="toc-best-practice" class="nav-link" data-scroll-target="#best-practice"><span class="header-section-number">2.1.4</span> Best Practice</a></li>
  </ul></li>
  <li><a href="#problem-2-odds" id="toc-problem-2-odds" class="nav-link" data-scroll-target="#problem-2-odds"><span class="header-section-number">2.2</span> Problem 2: Odds</a>
  <ul class="collapse">
  <li><a href="#definition-odds-of-success" id="toc-definition-odds-of-success" class="nav-link" data-scroll-target="#definition-odds-of-success"><span class="header-section-number">2.2.1</span> Definition: Odds of Success</a></li>
  <li><a href="#qualitative-definition" id="toc-qualitative-definition" class="nav-link" data-scroll-target="#qualitative-definition"><span class="header-section-number">2.2.2</span> Qualitative Definition</a></li>
  <li><a href="#formal-definition" id="toc-formal-definition" class="nav-link" data-scroll-target="#formal-definition"><span class="header-section-number">2.2.3</span> Formal Definition</a></li>
  <li><a href="#numerical-example" id="toc-numerical-example" class="nav-link" data-scroll-target="#numerical-example"><span class="header-section-number">2.2.4</span> Numerical Example</a></li>
  <li><a href="#additional-comparison" id="toc-additional-comparison" class="nav-link" data-scroll-target="#additional-comparison"><span class="header-section-number">2.2.5</span> Additional Comparison</a></li>
  <li><a href="#inverse-from-odds-to-probability" id="toc-inverse-from-odds-to-probability" class="nav-link" data-scroll-target="#inverse-from-odds-to-probability"><span class="header-section-number">2.2.6</span> Inverse: From Odds to Probability</a></li>
  </ul></li>
  <li><a href="#problem-3-interaction" id="toc-problem-3-interaction" class="nav-link" data-scroll-target="#problem-3-interaction"><span class="header-section-number">2.3</span> Problem 3: Interaction</a>
  <ul class="collapse">
  <li><a href="#definition-of-interaction-in-logistic-regression" id="toc-definition-of-interaction-in-logistic-regression" class="nav-link" data-scroll-target="#definition-of-interaction-in-logistic-regression"><span class="header-section-number">2.3.1</span> 1. Definition of Interaction in Logistic Regression</a></li>
  <li><a href="#simplest-form-of-an-interaction" id="toc-simplest-form-of-an-interaction" class="nav-link" data-scroll-target="#simplest-form-of-an-interaction"><span class="header-section-number">2.3.2</span> 2. Simplest Form of an Interaction</a></li>
  <li><a href="#statistical-tests-for-interaction-terms" id="toc-statistical-tests-for-interaction-terms" class="nav-link" data-scroll-target="#statistical-tests-for-interaction-terms"><span class="header-section-number">2.3.3</span> 3. Statistical Tests for Interaction Terms</a></li>
  <li><a href="#note-interaction-and-information-theory" id="toc-note-interaction-and-information-theory" class="nav-link" data-scroll-target="#note-interaction-and-information-theory"><span class="header-section-number">2.3.4</span> Note: Interaction and Information Theory</a></li>
  <li><a href="#interaction-as-additional-information" id="toc-interaction-as-additional-information" class="nav-link" data-scroll-target="#interaction-as-additional-information"><span class="header-section-number">2.3.5</span> Interaction as Additional Information</a></li>
  <li><a href="#impact-on-model-performance" id="toc-impact-on-model-performance" class="nav-link" data-scroll-target="#impact-on-model-performance"><span class="header-section-number">2.3.6</span> Impact on Model Performance</a></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1"><span class="header-section-number">2.3.7</span> Summary</a></li>
  </ul></li>
  <li><a href="#problem-4" id="toc-problem-4" class="nav-link" data-scroll-target="#problem-4"><span class="header-section-number">2.4</span> Problem 4:</a>
  <ul class="collapse">
  <li><a href="#explanation-1" id="toc-explanation-1" class="nav-link" data-scroll-target="#explanation-1"><span class="header-section-number">2.4.1</span> Explanation</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">2.4.2</span> Definitions:</a></li>
  <li><a href="#why-the-statement-is-false" id="toc-why-the-statement-is-false" class="nav-link" data-scroll-target="#why-the-statement-is-false"><span class="header-section-number">2.4.3</span> Why the Statement is False:</a></li>
  <li><a href="#corrected-version" id="toc-corrected-version" class="nav-link" data-scroll-target="#corrected-version"><span class="header-section-number">2.4.4</span> Corrected Version:</a></li>
  </ul></li>
  <li><a href="#problem-5" id="toc-problem-5" class="nav-link" data-scroll-target="#problem-5"><span class="header-section-number">2.5</span> Problem 5:</a>
  <ul class="collapse">
  <li><a href="#explanation-2" id="toc-explanation-2" class="nav-link" data-scroll-target="#explanation-2"><span class="header-section-number">2.5.1</span> Explanation</a></li>
  <li><a href="#clarification" id="toc-clarification" class="nav-link" data-scroll-target="#clarification"><span class="header-section-number">2.5.2</span> Clarification:</a></li>
  </ul></li>
  <li><a href="#problem-6" id="toc-problem-6" class="nav-link" data-scroll-target="#problem-6"><span class="header-section-number">2.6</span> Problem 6:</a>
  <ul class="collapse">
  <li><a href="#logistic-regression-transformation-of-the-response-variable" id="toc-logistic-regression-transformation-of-the-response-variable" class="nav-link" data-scroll-target="#logistic-regression-transformation-of-the-response-variable"><span class="header-section-number">2.6.1</span> Logistic Regression: Transformation of the Response Variable</a></li>
  <li><a href="#step-by-step-transformation" id="toc-step-by-step-transformation" class="nav-link" data-scroll-target="#step-by-step-transformation"><span class="header-section-number">2.6.2</span> Step-by-Step Transformation</a></li>
  <li><a href="#why-this-is-informative" id="toc-why-this-is-informative" class="nav-link" data-scroll-target="#why-this-is-informative"><span class="header-section-number">2.6.3</span> Why This Is Informative</a></li>
  <li><a href="#summary-2" id="toc-summary-2" class="nav-link" data-scroll-target="#summary-2"><span class="header-section-number">2.6.4</span> Summary</a></li>
  <li><a href="#note-pros-and-cons-of-output-transformations-in-logistic-regression" id="toc-note-pros-and-cons-of-output-transformations-in-logistic-regression" class="nav-link" data-scroll-target="#note-pros-and-cons-of-output-transformations-in-logistic-regression"><span class="header-section-number">2.6.5</span> Note: Pros and Cons of Output Transformations in Logistic Regression</a></li>
  <li><a href="#sigmoid-function" id="toc-sigmoid-function" class="nav-link" data-scroll-target="#sigmoid-function"><span class="header-section-number">2.6.6</span> 1. Sigmoid Function</a></li>
  <li><a href="#softmax-function" id="toc-softmax-function" class="nav-link" data-scroll-target="#softmax-function"><span class="header-section-number">2.6.7</span> 2. Softmax Function</a></li>
  <li><a href="#classic-normalization" id="toc-classic-normalization" class="nav-link" data-scroll-target="#classic-normalization"><span class="header-section-number">2.6.8</span> 3. Classic Normalization</a></li>
  <li><a href="#summary-table" id="toc-summary-table" class="nav-link" data-scroll-target="#summary-table"><span class="header-section-number">2.6.9</span> Summary Table</a></li>
  </ul></li>
  <li><a href="#problem-7" id="toc-problem-7" class="nav-link" data-scroll-target="#problem-7"><span class="header-section-number">2.7</span> Problem 7:</a>
  <ul class="collapse">
  <li><a href="#explanation-3" id="toc-explanation-3" class="nav-link" data-scroll-target="#explanation-3"><span class="header-section-number">2.7.1</span> Explanation</a></li>
  <li><a href="#python-code-illustration" id="toc-python-code-illustration" class="nav-link" data-scroll-target="#python-code-illustration"><span class="header-section-number">2.7.2</span> Python Code Illustration</a></li>
  <li><a href="#step-by-step-understanding-negative-log-likelihood-nll" id="toc-step-by-step-understanding-negative-log-likelihood-nll" class="nav-link" data-scroll-target="#step-by-step-understanding-negative-log-likelihood-nll"><span class="header-section-number">2.7.3</span> Step-by-Step: Understanding Negative Log Likelihood (NLL)</a></li>
  </ul></li>
  <li><a href="#problem-8" id="toc-problem-8" class="nav-link" data-scroll-target="#problem-8"><span class="header-section-number">2.8</span> Problem 8:</a>
  <ul class="collapse">
  <li><a href="#step-by-step-probability-odds-and-log-odds" id="toc-step-by-step-probability-odds-and-log-odds" class="nav-link" data-scroll-target="#step-by-step-probability-odds-and-log-odds"><span class="header-section-number">2.8.1</span> Step-by-Step: Probability, Odds, and Log-Odds</a></li>
  <li><a href="#what-are-the-odds-of-the-event-occurring" id="toc-what-are-the-odds-of-the-event-occurring" class="nav-link" data-scroll-target="#what-are-the-odds-of-the-event-occurring"><span class="header-section-number">2.8.2</span> 1. What are the <strong>odds</strong> of the event occurring?</a></li>
  <li><a href="#calculation" id="toc-calculation" class="nav-link" data-scroll-target="#calculation"><span class="header-section-number">2.8.3</span> Calculation:</a></li>
  <li><a href="#what-are-the-log-odds-logit-of-the-event" id="toc-what-are-the-log-odds-logit-of-the-event" class="nav-link" data-scroll-target="#what-are-the-log-odds-logit-of-the-event"><span class="header-section-number">2.8.4</span> 2. What are the <strong>log-odds</strong> (logit) of the event?</a></li>
  <li><a href="#calculation-1" id="toc-calculation-1" class="nav-link" data-scroll-target="#calculation-1"><span class="header-section-number">2.8.5</span> Calculation:</a></li>
  <li><a href="#construct-the-probability-as-a-ratio-that-equals-0.1" id="toc-construct-the-probability-as-a-ratio-that-equals-0.1" class="nav-link" data-scroll-target="#construct-the-probability-as-a-ratio-that-equals-0.1"><span class="header-section-number">2.8.6</span> 3. Construct the probability as a ratio that equals 0.1</a></li>
  <li><a href="#summary-table-1" id="toc-summary-table-1" class="nav-link" data-scroll-target="#summary-table-1"><span class="header-section-number">2.8.7</span> Summary Table</a></li>
  <li><a href="#intuition-behind-probability-odds-and-log-odds" id="toc-intuition-behind-probability-odds-and-log-odds" class="nav-link" data-scroll-target="#intuition-behind-probability-odds-and-log-odds"><span class="header-section-number">2.8.8</span> Intuition Behind Probability, Odds, and Log-Odds</a></li>
  <li><a href="#probability-p" id="toc-probability-p" class="nav-link" data-scroll-target="#probability-p"><span class="header-section-number">2.8.9</span> Probability (p)</a></li>
  <li><a href="#odds-fracp1---p" id="toc-odds-fracp1---p" class="nav-link" data-scroll-target="#odds-fracp1---p"><span class="header-section-number">2.8.10</span> Odds: <span class="math inline">\(\frac{p}{1 - p}\)</span></a></li>
  <li><a href="#log-odds-logit-logleftfracp1---pright" id="toc-log-odds-logit-logleftfracp1---pright" class="nav-link" data-scroll-target="#log-odds-logit-logleftfracp1---pright"><span class="header-section-number">2.8.11</span> Log-Odds (Logit): <span class="math inline">\(\log\left(\frac{p}{1 - p}\right)\)</span></a></li>
  <li><a href="#summary-why-use-log-odds" id="toc-summary-why-use-log-odds" class="nav-link" data-scroll-target="#summary-why-use-log-odds"><span class="header-section-number">2.8.12</span> Summary: Why use log-odds?</a></li>
  <li><a href="#interpretation-of-the-plots" id="toc-interpretation-of-the-plots" class="nav-link" data-scroll-target="#interpretation-of-the-plots"><span class="header-section-number">2.8.13</span> Interpretation of the Plots</a></li>
  <li><a href="#top-plot-probability-vs.-itself" id="toc-top-plot-probability-vs.-itself" class="nav-link" data-scroll-target="#top-plot-probability-vs.-itself"><span class="header-section-number">2.8.14</span> Top Plot: Probability vs.&nbsp;Itself</a></li>
  <li><a href="#middle-plot-probability-vs.-odds" id="toc-middle-plot-probability-vs.-odds" class="nav-link" data-scroll-target="#middle-plot-probability-vs.-odds"><span class="header-section-number">2.8.15</span> Middle Plot: Probability vs.&nbsp;Odds</a></li>
  <li><a href="#bottom-plot-probability-vs.-log-odds-logit" id="toc-bottom-plot-probability-vs.-log-odds-logit" class="nav-link" data-scroll-target="#bottom-plot-probability-vs.-log-odds-logit"><span class="header-section-number">2.8.16</span> Bottom Plot: Probability vs.&nbsp;Log-Odds (Logit)</a></li>
  <li><a href="#why-use-log-odds" id="toc-why-use-log-odds" class="nav-link" data-scroll-target="#why-use-log-odds"><span class="header-section-number">2.8.17</span> Why Use Log-Odds?</a></li>
  </ul></li>
  <li><a href="#problem-9." id="toc-problem-9." class="nav-link" data-scroll-target="#problem-9."><span class="header-section-number">2.9</span> Problem 9.</a>
  <ul class="collapse">
  <li><a href="#step-by-step-solution" id="toc-step-by-step-solution" class="nav-link" data-scroll-target="#step-by-step-solution"><span class="header-section-number">2.9.1</span> Step-by-step Solution</a></li>
  <li><a href="#final-answer" id="toc-final-answer" class="nav-link" data-scroll-target="#final-answer"><span class="header-section-number">2.9.2</span> Final Answer</a></li>
  </ul></li>
  <li><a href="#problem-10" id="toc-problem-10" class="nav-link" data-scroll-target="#problem-10"><span class="header-section-number">2.10</span> Problem 10:</a>
  <ul class="collapse">
  <li><a href="#graph-probability-vs.-odds" id="toc-graph-probability-vs.-odds" class="nav-link" data-scroll-target="#graph-probability-vs.-odds"><span class="header-section-number">2.10.1</span> Graph: Probability vs.&nbsp;Odds</a></li>
  </ul></li>
  <li><a href="#problem-11" id="toc-problem-11" class="nav-link" data-scroll-target="#problem-11"><span class="header-section-number">2.11</span> Problem 11:</a>
  <ul class="collapse">
  <li><a href="#components-of-a-generalized-linear-model-glm-in-binary-logistic-regression" id="toc-components-of-a-generalized-linear-model-glm-in-binary-logistic-regression" class="nav-link" data-scroll-target="#components-of-a-generalized-linear-model-glm-in-binary-logistic-regression"><span class="header-section-number">2.11.1</span> Components of a Generalized Linear Model (GLM) in Binary Logistic Regression</a></li>
  <li><a href="#random-component" id="toc-random-component" class="nav-link" data-scroll-target="#random-component"><span class="header-section-number">2.11.2</span> 1. <strong>Random Component</strong></a></li>
  <li><a href="#systematic-component" id="toc-systematic-component" class="nav-link" data-scroll-target="#systematic-component"><span class="header-section-number">2.11.3</span> 2. <strong>Systematic Component</strong></a></li>
  <li><a href="#link-function" id="toc-link-function" class="nav-link" data-scroll-target="#link-function"><span class="header-section-number">2.11.4</span> 3. <strong>Link Function</strong></a></li>
  <li><a href="#summary-table-2" id="toc-summary-table-2" class="nav-link" data-scroll-target="#summary-table-2"><span class="header-section-number">2.11.5</span> Summary Table</a></li>
  <li><a href="#adjusted-glm-components-for-voice-activity-detection-vad" id="toc-adjusted-glm-components-for-voice-activity-detection-vad" class="nav-link" data-scroll-target="#adjusted-glm-components-for-voice-activity-detection-vad"><span class="header-section-number">2.11.6</span> Adjusted GLM Components for Voice Activity Detection (VAD)</a></li>
  <li><a href="#random-component-1" id="toc-random-component-1" class="nav-link" data-scroll-target="#random-component-1"><span class="header-section-number">2.11.7</span> Random Component</a></li>
  <li><a href="#systematic-components" id="toc-systematic-components" class="nav-link" data-scroll-target="#systematic-components"><span class="header-section-number">2.11.8</span> Systematic Components</a></li>
  <li><a href="#link-function-1" id="toc-link-function-1" class="nav-link" data-scroll-target="#link-function-1"><span class="header-section-number">2.11.9</span> Link Function</a></li>
  <li><a href="#summary-3" id="toc-summary-3" class="nav-link" data-scroll-target="#summary-3"><span class="header-section-number">2.11.10</span> Summary</a></li>
  </ul></li>
  <li><a href="#problem-12" id="toc-problem-12" class="nav-link" data-scroll-target="#problem-12"><span class="header-section-number">2.12</span> Problem 12:</a>
  <ul class="collapse">
  <li><a href="#logistic-regression-decision-boundary-using-logit" id="toc-logistic-regression-decision-boundary-using-logit" class="nav-link" data-scroll-target="#logistic-regression-decision-boundary-using-logit"><span class="header-section-number">2.12.1</span> Logistic Regression Decision Boundary (Using Logit)</a></li>
  <li><a href="#step-1-set-the-decision-threshold" id="toc-step-1-set-the-decision-threshold" class="nav-link" data-scroll-target="#step-1-set-the-decision-threshold"><span class="header-section-number">2.12.2</span> Step 1: Set the Decision Threshold</a></li>
  <li><a href="#step-2-solve-for-the-boundary" id="toc-step-2-solve-for-the-boundary" class="nav-link" data-scroll-target="#step-2-solve-for-the-boundary"><span class="header-section-number">2.12.3</span> Step 2: Solve for the Boundary</a></li>
  <li><a href="#final-result-hyperplane-equation" id="toc-final-result-hyperplane-equation" class="nav-link" data-scroll-target="#final-result-hyperplane-equation"><span class="header-section-number">2.12.4</span> Final Result: Hyperplane Equation</a></li>
  <li><a href="#solution" id="toc-solution" class="nav-link" data-scroll-target="#solution"><span class="header-section-number">2.12.5</span> Solution</a></li>
  <li><a href="#derivation-from-logit-function" id="toc-derivation-from-logit-function" class="nav-link" data-scroll-target="#derivation-from-logit-function"><span class="header-section-number">2.12.6</span> Derivation from Logit Function</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">2.12.7</span> Conclusion</a></li>
  </ul></li>
  <li><a href="#problem-13-logit-and-sigmoid" id="toc-problem-13-logit-and-sigmoid" class="nav-link" data-scroll-target="#problem-13-logit-and-sigmoid"><span class="header-section-number">2.13</span> Problem 13: Logit and Sigmoid</a>
  <ul class="collapse">
  <li><a href="#explanation-4" id="toc-explanation-4" class="nav-link" data-scroll-target="#explanation-4"><span class="header-section-number">2.13.1</span> Explanation:</a></li>
  <li><a href="#additional-note" id="toc-additional-note" class="nav-link" data-scroll-target="#additional-note"><span class="header-section-number">2.13.2</span> Additional Note:</a></li>
  </ul></li>
  <li><a href="#derivative-of-the-natural-sigmoid-function" id="toc-derivative-of-the-natural-sigmoid-function" class="nav-link" data-scroll-target="#derivative-of-the-natural-sigmoid-function"><span class="header-section-number">2.14</span> Derivative of the Natural Sigmoid Function</a>
  <ul class="collapse">
  <li><a href="#step-1-compute-the-derivative" id="toc-step-1-compute-the-derivative" class="nav-link" data-scroll-target="#step-1-compute-the-derivative"><span class="header-section-number">2.14.1</span> Step 1: Compute the Derivative</a></li>
  <li><a href="#step-2-express-in-terms-of-sigmax" id="toc-step-2-express-in-terms-of-sigmax" class="nav-link" data-scroll-target="#step-2-express-in-terms-of-sigmax"><span class="header-section-number">2.14.2</span> Step 2: Express in Terms of <span class="math inline">\(\sigma(x)\)</span></a></li>
  <li><a href="#final-answer-1" id="toc-final-answer-1" class="nav-link" data-scroll-target="#final-answer-1"><span class="header-section-number">2.14.3</span> Final Answer:</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Logistic regression</h1>
  <div class="quarto-categories">
    <div class="quarto-category">math</div>
  </div>
  </div>

<div>
  <div class="description">
    Your post description
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://kimhungbui.github.io">Your name</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://your.workplace.website">
            Maybe your workplace?
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 12, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction-to-logistic-regression" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction to Logistic Regression</h1>
<p><em>Logistic regression</em>(Tolles and Meurer, 2016) is a model named after the logistic function, which plays a central role in the model.</p>
<p>Originally, the logistic function was created from typical statistical models of population growth. This function takes an S-shaped form and maps real values to a range in <span class="math inline">\((0, L)\)</span>. The general mathematical formula of the logistic function is:</p>
<p><span class="math display">\[
f(x) = \frac{L}{1 + e^{-k(x - x_0)}} \tag{1}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(x_0\)</span> is the value at the midpoint of the logistic curve,</li>
<li><span class="math inline">\(k\)</span> is the growth rate of the logistic function,</li>
<li><span class="math inline">\(L\)</span> is the maximum value of the logistic function.</li>
</ul>
<hr>
<p>The logistic regression model is often used in classification tasks, especially binary classification, even though the term “regression” is included in its name. The upcoming sections will explain why this naming convention was adopted.</p>
</section>
<section id="general-concepts" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> General Concepts</h1>
<section id="problem-1-increase-observation" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="problem-1-increase-observation"><span class="header-section-number">2.1</span> Problem 1: Increase observation</h2>
<p><em>True or False:</em> For a fixed number of observations in a data set, introducing more variables normally generates a model that has a better fit to the data. What may be the drawback of such a model fitting strategy?</p>
<p>Example:</p>
<div id="6e7fc00c" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create synthetic data</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.sort(<span class="dv">5</span> <span class="op">*</span> np.random.rand(n_samples, <span class="dv">1</span>), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X).ravel() <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.2</span>, size<span class="op">=</span>n_samples)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test sets</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial degrees to test</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, degree <span class="kw">in</span> <span class="bu">enumerate</span>(degrees, <span class="dv">1</span>):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate polynomial features</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    poly <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>degree)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    X_train_poly <span class="op">=</span> poly.fit_transform(X_train)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    X_test_poly <span class="op">=</span> poly.transform(X_test)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit model</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LinearRegression()</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train_poly, y_train)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    X_plot_poly <span class="op">=</span> poly.transform(X_plot)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    y_plot <span class="op">=</span> model.predict(X_plot_poly)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate errors</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    train_mse <span class="op">=</span> mean_squared_error(y_train, model.predict(X_train_poly))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    test_mse <span class="op">=</span> mean_squared_error(y_test, model.predict(X_test_poly))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, i)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X_train, y_train, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Train data'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X_test, y_test, color<span class="op">=</span><span class="st">'green'</span>, label<span class="op">=</span><span class="st">'Test data'</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, y_plot, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="ss">f'Degree </span><span class="sc">{</span>degree<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"Degree </span><span class="sc">{</span>degree<span class="sc">}</span><span class="ch">\n</span><span class="ss">Train MSE: </span><span class="sc">{</span>train_mse<span class="sc">:.2f}</span><span class="ss">, Test MSE: </span><span class="sc">{</span>test_mse<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="1430" height="375" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We see that: - Degree 1: (underfit): High bias, both train and test error are high - Degree 3 (good fit): Balanced bias-variance, good generalization. - Degree 9 (overfit): Train error very low, but test error high – model fits noise.</p>
<p><strong>True</strong> – Introducing more variables generally improves the model’s fit to the training data.<br>
However, there is a major drawback: it often leads to <strong>overfitting</strong>.</p>
<hr>
<section id="explanation" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="explanation"><span class="header-section-number">2.1.1</span> Explanation</h3>
<section id="why-the-fit-improves" class="level4">
<h4 class="anchored" data-anchor-id="why-the-fit-improves">Why the Fit Improves</h4>
<ul>
<li>In regression or classification tasks, adding more features gives the model more flexibility to match the training data.</li>
<li>This allows the model to capture finer patterns, reduce residuals, and minimize training error.</li>
</ul>
<p><strong>Example</strong>:<br>
In polynomial regression, increasing the degree (i.e., adding more variables) can make the curve pass through all data points, resulting in nearly zero training error.</p>
<hr>
</section>
</section>
<section id="drawbacks-of-this-strategy" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="drawbacks-of-this-strategy"><span class="header-section-number">2.1.2</span> Drawbacks of This Strategy</h3>
<section id="overfitting" class="level4">
<h4 class="anchored" data-anchor-id="overfitting">1. Overfitting</h4>
<ul>
<li>A model that fits the training data too well may learn noise or random fluctuations instead of the true underlying patterns.</li>
<li>This results in poor generalization to unseen or test data.</li>
</ul>
</section>
<section id="increased-variance" class="level4">
<h4 class="anchored" data-anchor-id="increased-variance">2. Increased Variance</h4>
<ul>
<li>More variables increase the model’s sensitivity to small changes in data.</li>
<li>A high-variance model may change dramatically with minor input changes.</li>
</ul>
</section>
<section id="curse-of-dimensionality" class="level4">
<h4 class="anchored" data-anchor-id="curse-of-dimensionality">3. Curse of Dimensionality</h4>
<ul>
<li>In high-dimensional spaces, data becomes sparse.</li>
<li>Concepts like distance, density, and similarity lose their meaning.</li>
<li>Many algorithms (e.g., k-NN, clustering) perform poorly in high dimensions.</li>
</ul>
</section>
<section id="interpretability" class="level4">
<h4 class="anchored" data-anchor-id="interpretability">4. Interpretability</h4>
<ul>
<li>Adding more variables makes the model harder to interpret.</li>
<li>This is a problem in domains where transparency is important (e.g., medicine, finance).</li>
</ul>
</section>
<section id="computational-cost" class="level4">
<h4 class="anchored" data-anchor-id="computational-cost">5. Computational Cost</h4>
<ul>
<li>More variables require more memory and longer training times.</li>
<li>Feature selection or dimensionality reduction may be needed to manage complexity.</li>
</ul>
<hr>
</section>
</section>
<section id="summary" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.1.3</span> Summary</h3>
<p><strong>True</strong> – Adding more variables generally improves the fit on training data,<br>
but it increases the risk of <strong>overfitting</strong>, <strong>poor generalization</strong>, and <strong>computational burden</strong>.</p>
<hr>
</section>
<section id="best-practice" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="best-practice"><span class="header-section-number">2.1.4</span> Best Practice</h3>
<p>Use techniques like <strong>cross-validation</strong> and <strong>regularization</strong> (e.g., Lasso, Ridge, dropout)<br>
to balance model complexity and generalization performance.</p>
</section>
</section>
<section id="problem-2-odds" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="problem-2-odds"><span class="header-section-number">2.2</span> Problem 2: Odds</h2>
<p>Define the term “<strong>odds of success</strong>” both qualitatively and formally. Give a numerical example that stresses the relation between probability and odds of an event occurring.</p>
<section id="definition-odds-of-success" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="definition-odds-of-success"><span class="header-section-number">2.2.1</span> Definition: Odds of Success</h3>
</section>
<section id="qualitative-definition" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="qualitative-definition"><span class="header-section-number">2.2.2</span> Qualitative Definition</h3>
<p>The <strong>odds of success</strong> express how much more likely an event is to occur than not occur. It is often used in statistics and logistic regression.</p>
<ul>
<li>If an event is <strong>very likely</strong>, the odds are high.</li>
<li>If an event is <strong>unlikely</strong>, the odds are low.</li>
<li>If the event is <strong>equally likely to happen or not</strong>, the odds are 1 (or “even odds”).</li>
</ul>
</section>
<section id="formal-definition" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="formal-definition"><span class="header-section-number">2.2.3</span> Formal Definition</h3>
<p>Let <strong>p</strong> be the probability of success (i.e., the event occurring). Then the <strong>odds of success</strong> are defined as:</p>
<p><span class="math display">\[
\text{Odds of success} = \frac{p}{1 - p}
\]</span></p>
<p>This compares the chance the event <em>does happen</em> (p) to the chance it <em>does not happen</em> (1 - p).</p>
<hr>
</section>
<section id="numerical-example" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="numerical-example"><span class="header-section-number">2.2.4</span> Numerical Example</h3>
<p>Suppose the probability of success is:</p>
<p><span class="math display">\[
p = 0.75
\]</span></p>
<p>Then the odds of success are:</p>
<p><span class="math display">\[
\text{Odds} = \frac{0.75}{1 - 0.75} = \frac{0.75}{0.25} = 3
\]</span></p>
<p><strong>Interpretation</strong>:<br>
The event is <strong>3 times more likely</strong> to occur than not occur.<br>
In other words, for every 3 successes, we expect 1 failure.</p>
<hr>
</section>
<section id="additional-comparison" class="level3" data-number="2.2.5">
<h3 data-number="2.2.5" class="anchored" data-anchor-id="additional-comparison"><span class="header-section-number">2.2.5</span> Additional Comparison</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Probability (p)</th>
<th>Odds = p / (1 - p)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.5</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>0.8</td>
<td>4.0</td>
</tr>
<tr class="odd">
<td>0.25</td>
<td>0.33</td>
</tr>
</tbody>
</table>
<p>As the probability increases toward 1, the odds increase toward infinity.</p>
<hr>
</section>
<section id="inverse-from-odds-to-probability" class="level3" data-number="2.2.6">
<h3 data-number="2.2.6" class="anchored" data-anchor-id="inverse-from-odds-to-probability"><span class="header-section-number">2.2.6</span> Inverse: From Odds to Probability</h3>
<p>If you are given the odds $ o $, you can convert back to probability:</p>
<p><span class="math display">\[
p = \frac{o}{1 + o}
\]</span></p>
<p><strong>Example</strong>:<br>
If odds = 4, then</p>
<p><span class="math display">\[
p = \frac{4}{1 + 4} = \frac{4}{5} = 0.8
\]</span></p>
</section>
</section>
<section id="problem-3-interaction" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="problem-3-interaction"><span class="header-section-number">2.3</span> Problem 3: Interaction</h2>
<ol type="1">
<li>Define what is meant by the term “interaction”, in the context of a logistic regression predictor variable.</li>
<li>What is the simplest form of an interaction? Write its formulae.</li>
<li>What statistical tests can be used to attest the significance of an interaction term?</li>
</ol>
<section id="definition-of-interaction-in-logistic-regression" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="definition-of-interaction-in-logistic-regression"><span class="header-section-number">2.3.1</span> 1. Definition of Interaction in Logistic Regression</h3>
<p>In logistic regression, an <strong>interaction</strong> occurs when the <strong>effect of one predictor variable on the outcome depends on the level of another predictor variable</strong>.</p>
<p>This means the predictors do not act independently: the combined effect of two variables is <strong>not simply additive</strong> on the log-odds scale.</p>
<p><strong>Example</strong>:<br>
If <span class="math inline">\(X_1\)</span> is age and <span class="math inline">\(X_2\)</span> is smoking status, an interaction term (<span class="math inline">\(X_1 \cdot X_2\)</span>) would capture how the effect of age on the probability of disease differs between smokers and non-smokers.</p>
<hr>
</section>
<section id="simplest-form-of-an-interaction" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="simplest-form-of-an-interaction"><span class="header-section-number">2.3.2</span> 2. Simplest Form of an Interaction</h3>
<p>The simplest interaction involves <strong>two variables</strong> in a logistic regression model. The formula (on the log-odds scale) is:</p>
<p><span class="math display">\[
\log\left( \frac{p}{1 - p} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 \cdot X_2)
\]</span></p>
<p>Where: - <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are predictors - <span class="math inline">\(X_1 \cdot X_2\)</span> is the interaction term - <span class="math inline">\(\beta_3\)</span> captures the <strong>change in effect</strong> of <span class="math inline">\(X_1\)</span> at different levels of <span class="math inline">\(X_2\)</span></p>
<p>If <span class="math inline">\(\beta_3 \ne 0\)</span>, there is a statistically significant interaction.</p>
<hr>
</section>
<section id="statistical-tests-for-interaction-terms" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="statistical-tests-for-interaction-terms"><span class="header-section-number">2.3.3</span> 3. Statistical Tests for Interaction Terms</h3>
<p>To test whether the interaction term significantly improves the model:</p>
<section id="a.-wald-test" class="level4">
<h4 class="anchored" data-anchor-id="a.-wald-test">a. <strong>Wald Test</strong></h4>
<ul>
<li>Tests if <span class="math inline">\(\beta_3 = 0\)</span></li>
<li>Based on the standard error and coefficient</li>
<li>Commonly used in software output (e.g., <code>summary()</code> in R or <code>LogitResults</code> in statsmodels)</li>
</ul>
</section>
<section id="b.-likelihood-ratio-test-lrt" class="level4">
<h4 class="anchored" data-anchor-id="b.-likelihood-ratio-test-lrt">b. <strong>Likelihood Ratio Test (LRT)</strong></h4>
<ul>
<li>Compares:
<ul>
<li><strong>Model 1</strong>: with interaction term</li>
<li><strong>Model 2</strong>: without interaction term</li>
</ul></li>
<li>Null hypothesis: interaction term does not improve the model</li>
<li>LRT is more robust than the Wald test, especially in small samples</li>
</ul>
<p><strong>Steps</strong>: 1. Fit both models (with and without interaction) 2. Compute:<br>
<span class="math display">\[
\chi^2 = -2(\log L_{\text{reduced}} - \log L_{\text{full}})
\]</span> 3. Compare with chi-square distribution (df = 1 for one interaction term)</p>
</section>
<section id="c.-anova-analysis-of-deviance" class="level4">
<h4 class="anchored" data-anchor-id="c.-anova-analysis-of-deviance">c.&nbsp;<strong>ANOVA (Analysis of Deviance)</strong></h4>
<ul>
<li>Alternative approach to compare nested models in logistic regression.</li>
<li>Often used in R with <code>anova(model1, model2, test = "Chisq")</code></li>
</ul>
</section>
</section>
<section id="note-interaction-and-information-theory" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="note-interaction-and-information-theory"><span class="header-section-number">2.3.4</span> Note: Interaction and Information Theory</h3>
<p>In the context of <strong>information theory</strong>, interaction terms in a model can be interpreted as capturing <strong>mutual information</strong> between predictor variables <strong>and</strong> their <strong>combined influence</strong> on the target.</p>
<hr>
</section>
<section id="interaction-as-additional-information" class="level3" data-number="2.3.5">
<h3 data-number="2.3.5" class="anchored" data-anchor-id="interaction-as-additional-information"><span class="header-section-number">2.3.5</span> Interaction as Additional Information</h3>
<p>Without an interaction term, a model assumes <strong>additivity</strong>: each predictor affects the outcome independently. However, if two variables <em>jointly</em> influence the outcome, then their interaction carries <strong>additional information</strong> beyond their individual effects.</p>
<p>This added value can be viewed as:</p>
<ul>
<li><strong>Extra bits of information</strong> (in the sense of entropy reduction) gained by knowing the joint effect of variables</li>
<li><strong>Mutual information</strong> between variables that is <strong>relevant to the response</strong>, not captured in their marginal contributions</li>
</ul>
<hr>
</section>
<section id="impact-on-model-performance" class="level3" data-number="2.3.6">
<h3 data-number="2.3.6" class="anchored" data-anchor-id="impact-on-model-performance"><span class="header-section-number">2.3.6</span> Impact on Model Performance</h3>
<section id="improved-predictive-power" class="level4">
<h4 class="anchored" data-anchor-id="improved-predictive-power">1. <strong>Improved Predictive Power</strong></h4>
<ul>
<li>Captures complex relationships</li>
<li>Leads to better fit and generalization, if the interaction is real and not noise</li>
</ul>
</section>
<section id="reduced-residual-uncertainty" class="level4">
<h4 class="anchored" data-anchor-id="reduced-residual-uncertainty">2. <strong>Reduced Residual Uncertainty</strong></h4>
<ul>
<li>Reduces unexplained variation in the outcome</li>
<li>Analogous to decreasing entropy in the output distribution by incorporating more structure</li>
</ul>
</section>
<section id="better-feature-representation" class="level4">
<h4 class="anchored" data-anchor-id="better-feature-representation">3. <strong>Better Feature Representation</strong></h4>
<ul>
<li>Interaction terms effectively <strong>encode feature combinations</strong> that correlate strongly with the outcome</li>
<li>Similar to feature engineering guided by <strong>information gain</strong></li>
</ul>
<hr>
</section>
</section>
<section id="summary-1" class="level3" data-number="2.3.7">
<h3 data-number="2.3.7" class="anchored" data-anchor-id="summary-1"><span class="header-section-number">2.3.7</span> Summary</h3>
<p>Adding interaction terms allows the model to <strong>capture dependency structures</strong> among variables that are meaningful to the target, thereby increasing the <strong>information</strong> the model has about the outcome. In information-theoretic terms, interactions <strong>reduce conditional entropy</strong> and increase <strong>mutual information</strong> between inputs and output.</p>
</section>
</section>
<section id="problem-4" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="problem-4"><span class="header-section-number">2.4</span> Problem 4:</h2>
<p>True or False: In machine learning terminology, unsupervised learning refers to the mapping of input covariates to a target response variable that is attempted at being predicted when the labels are known.</p>
<p><strong>False</strong></p>
<section id="explanation-1" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="explanation-1"><span class="header-section-number">2.4.1</span> Explanation</h3>
<p>In <strong>machine learning</strong>, the statement describes <strong>supervised learning</strong>, not unsupervised learning.</p>
<hr>
</section>
<section id="definitions" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="definitions"><span class="header-section-number">2.4.2</span> Definitions:</h3>
<ul>
<li><p><strong>Supervised Learning</strong>:<br>
The algorithm learns to <strong>map input features (covariates)</strong> to a known <strong>target variable (labels)</strong>.<br>
Examples: classification, regression.</p></li>
<li><p><strong>Unsupervised Learning</strong>:<br>
The algorithm is used when <strong>labels are unknown</strong>. It finds <strong>patterns or structures</strong> in the data.<br>
Examples: clustering, dimensionality reduction.</p></li>
</ul>
<hr>
</section>
<section id="why-the-statement-is-false" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="why-the-statement-is-false"><span class="header-section-number">2.4.3</span> Why the Statement is False:</h3>
<blockquote class="blockquote">
<p><em>“Unsupervised learning refers to the mapping of input covariates to a target response variable that is attempted at being predicted when the labels are known.”</em></p>
</blockquote>
<ul>
<li>It incorrectly claims <strong>unsupervised learning uses known labels</strong>, which is <strong>not true</strong>.</li>
<li>This description actually fits <strong>supervised learning</strong>.</li>
</ul>
<hr>
</section>
<section id="corrected-version" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="corrected-version"><span class="header-section-number">2.4.4</span> Corrected Version:</h3>
<blockquote class="blockquote">
<p><strong>Supervised learning</strong> refers to the mapping of input covariates to a target response variable, using known labels.</p>
</blockquote>
</section>
</section>
<section id="problem-5" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="problem-5"><span class="header-section-number">2.5</span> Problem 5:</h2>
<p>Complete the following sentence: In the case of logistic regression, the response variable is the log of the odds of being classified in […].</p>
<p><strong>Complete sentence:</strong></p>
<p>In the case of logistic regression, the response variable is the <strong>log of the odds of being classified in the reference (or “positive”) category</strong>.</p>
<hr>
<section id="explanation-2" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="explanation-2"><span class="header-section-number">2.5.1</span> Explanation</h3>
<p>Logistic regression models the probability of a binary outcome by applying the <strong>logit function</strong> to the response:</p>
<p><span class="math display">\[
\log\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k
\]</span></p>
<ul>
<li><span class="math inline">\(p\)</span> is the probability of the outcome being in the <strong>positive or reference class</strong> (e.g., “yes”, “success”, or class = 1).</li>
<li>The left-hand side is the <strong>log-odds</strong> of that outcome.</li>
<li>The model learns a linear relationship between the predictors and the <strong>log-odds</strong> of classification in the target category.</li>
</ul>
<p><strong>Solution:</strong></p>
<p>In the case of logistic regression, the response variable is the <strong>log of the odds of being classified in a group of binary or multi-class responses</strong>.<br>
This definition essentially demonstrates that <strong>odds can take the form of a vector</strong>.</p>
<hr>
</section>
<section id="clarification" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="clarification"><span class="header-section-number">2.5.2</span> Clarification:</h3>
<ul>
<li><p>For <strong>binary logistic regression</strong>, the model estimates: <span class="math display">\[
\log\left(\frac{p}{1 - p}\right)
\]</span> where <span class="math inline">\(p\)</span> is the probability of being in the positive class.</p></li>
<li><p>For <strong>multinomial (multi-class) logistic regression</strong>, the model estimates a set of log-odds: <span class="math display">\[
\log\left(\frac{p_k}{p_{reference}}\right)
\]</span> for each class <span class="math inline">\(k \ne\)</span> reference, resulting in a <strong>vector of log-odds</strong>, one for each class.</p></li>
</ul>
<p>Thus, in multiclass cases, the model output is not a single scalar log-odds but a <strong>vector of log-odds</strong>, supporting the idea that <strong>odds can be vector-valued</strong>.</p>
</section>
</section>
<section id="problem-6" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="problem-6"><span class="header-section-number">2.6</span> Problem 6:</h2>
<p>Describe how in a logistic regression model, a transformation to the response variable is applied to yield a probability distribution. Why is it considered a more informative representation of the response?</p>
<section id="logistic-regression-transformation-of-the-response-variable" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="logistic-regression-transformation-of-the-response-variable"><span class="header-section-number">2.6.1</span> Logistic Regression: Transformation of the Response Variable</h3>
<p>In logistic regression, the <strong>response variable</strong> is categorical (often binary), but the model must output continuous values to fit it using linear predictors. This is done by applying a <strong>logit transformation</strong>, and then its inverse—the <strong>logistic (sigmoid) function</strong>—to map outputs to probabilities.</p>
<hr>
</section>
<section id="step-by-step-transformation" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="step-by-step-transformation"><span class="header-section-number">2.6.2</span> Step-by-Step Transformation</h3>
<ol type="1">
<li><p><strong>Linear combination of predictors:</strong> <span class="math display">\[
z = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k
\]</span></p></li>
<li><p><strong>Logit transformation (link function):</strong> <span class="math display">\[
\text{logit}(p) = \log\left(\frac{p}{1 - p}\right) = z
\]</span></p></li>
<li><p><strong>Inverse-logit (sigmoid) function to obtain probability:</strong> <span class="math display">\[
p = \frac{1}{1 + e^{-z}}
\]</span></p></li>
</ol>
<p>This maps any real-valued input <span class="math inline">\(z \in (-\infty, \infty)\)</span> into a valid probability <span class="math inline">\(p \in (0, 1)\)</span>.</p>
<hr>
</section>
<section id="why-this-is-informative" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="why-this-is-informative"><span class="header-section-number">2.6.3</span> Why This Is Informative</h3>
<ul>
<li><p><strong>Probabilistic output</strong>: Unlike hard class labels, logistic regression provides the <strong>estimated probability</strong> of belonging to a class, which gives <strong>more nuanced information</strong>.</p></li>
<li><p><strong>Uncertainty awareness</strong>: Probabilities allow us to gauge <strong>confidence</strong> in predictions. For example, a prediction of 0.95 is more confident than 0.55.</p></li>
<li><p><strong>Threshold flexibility</strong>: You can choose decision thresholds based on the application (e.g., 0.5, 0.7) rather than being locked into fixed class predictions.</p></li>
<li><p><strong>Supports ranking and calibration</strong>: Probabilities are useful for <strong>ROC analysis</strong>, <strong>calibration</strong>, and <strong>expected loss minimization</strong>.</p></li>
</ul>
<hr>
</section>
<section id="summary-2" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="summary-2"><span class="header-section-number">2.6.4</span> Summary</h3>
<p>Logistic regression transforms the response variable through the <strong>logit link</strong> and uses its inverse to map model outputs to a <strong>valid probability distribution</strong>. This enables not only classification but also a <strong>more informative and interpretable</strong> representation of the predicted outcomes.</p>
</section>
<section id="note-pros-and-cons-of-output-transformations-in-logistic-regression" class="level3" data-number="2.6.5">
<h3 data-number="2.6.5" class="anchored" data-anchor-id="note-pros-and-cons-of-output-transformations-in-logistic-regression"><span class="header-section-number">2.6.5</span> Note: Pros and Cons of Output Transformations in Logistic Regression</h3>
<p>When transforming the response variable into a <strong>probability distribution</strong>, several methods can be used depending on the problem type. The most common are:</p>
<ul>
<li><strong>Sigmoid function</strong> — for binary classification</li>
<li><strong>Softmax function</strong> — for multi-class classification</li>
<li><strong>Classic normalization</strong> — general scaling of outputs (less used in classification)</li>
</ul>
<p>Below is a comparison of their <strong>pros and cons</strong>:</p>
<hr>
</section>
<section id="sigmoid-function" class="level3" data-number="2.6.6">
<h3 data-number="2.6.6" class="anchored" data-anchor-id="sigmoid-function"><span class="header-section-number">2.6.6</span> 1. Sigmoid Function</h3>
<p><strong>Definition:</strong> <span class="math display">\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</span></p>
<p><strong>Use Case:</strong> Binary classification (2 classes)</p>
<p><strong>Pros:</strong> - Simple and computationally efficient - Naturally maps real values to the interval (0, 1) - Interpretable as the probability of the positive class</p>
<p><strong>Cons:</strong> - Only supports binary output - Cannot capture interactions among multiple classes - Not ideal for mutually exclusive multi-class problems</p>
<hr>
</section>
<section id="softmax-function" class="level3" data-number="2.6.7">
<h3 data-number="2.6.7" class="anchored" data-anchor-id="softmax-function"><span class="header-section-number">2.6.7</span> 2. Softmax Function</h3>
<p><strong>Definition:</strong> <span class="math display">\[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\]</span></p>
<p><strong>Use Case:</strong> Multi-class classification (K &gt; 2, mutually exclusive classes)</p>
<p><strong>Pros:</strong> - Generalizes sigmoid to multi-class setting - Produces a valid probability distribution over <span class="math inline">\(K\)</span> classes - Probabilities sum to 1, suitable for cross-entropy loss</p>
<p><strong>Cons:</strong> - Sensitive to extreme values (due to exponentiation) - Less robust to outliers in inputs - Computationally more expensive than sigmoid</p>
<hr>
</section>
<section id="classic-normalization" class="level3" data-number="2.6.8">
<h3 data-number="2.6.8" class="anchored" data-anchor-id="classic-normalization"><span class="header-section-number">2.6.8</span> 3. Classic Normalization</h3>
<p><strong>Definition:</strong> <span class="math display">\[
\text{normalized}(z_i) = \frac{z_i}{\sum_{j=1}^K z_j}
\]</span></p>
<p><strong>Use Case:</strong> Sometimes used as an approximation or in non-logistic models</p>
<p><strong>Pros:</strong> - Simple and fast - Avoids exponentiation (numerically stable)</p>
<p><strong>Cons:</strong> - <strong>Not guaranteed to produce valid probabilities</strong> unless all <span class="math inline">\(z_i \ge 0\)</span> - Can yield values outside [0, 1] if inputs are not positive - Lacks probabilistic interpretation unless additional constraints are applied</p>
<hr>
</section>
<section id="summary-table" class="level3" data-number="2.6.9">
<h3 data-number="2.6.9" class="anchored" data-anchor-id="summary-table"><span class="header-section-number">2.6.9</span> Summary Table</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 17%">
<col style="width: 11%">
<col style="width: 8%">
<col style="width: 23%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Transformation</th>
<th>Best for</th>
<th>Output Range</th>
<th>Sums to 1</th>
<th>Interpretable Probabilities</th>
<th>Key Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Sigmoid</strong></td>
<td>Binary classification</td>
<td>(0, 1)</td>
<td>No</td>
<td>Yes</td>
<td>Not suitable for &gt;2 classes</td>
</tr>
<tr class="even">
<td><strong>Softmax</strong></td>
<td>Multi-class classification</td>
<td>(0, 1)</td>
<td>Yes</td>
<td>Yes</td>
<td>Sensitive to outliers</td>
</tr>
<tr class="odd">
<td><strong>Normalization</strong></td>
<td>Heuristic scaling</td>
<td>Varies</td>
<td>Possibly</td>
<td>Not always</td>
<td>May not yield valid probs</td>
</tr>
</tbody>
</table>
<div id="ae04a71a" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Raw model outputs (logits)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> np.array([<span class="fl">2.0</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span>])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Sigmoid function (binary case, apply to a single logit)</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>sigmoid_result <span class="op">=</span> sigmoid(logits[<span class="dv">0</span>])  <span class="co"># Binary case example</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Softmax function (multi-class case)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(z):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    exp_z <span class="op">=</span> np.exp(z <span class="op">-</span> np.<span class="bu">max</span>(z))  <span class="co"># stability improvement</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_z <span class="op">/</span> np.<span class="bu">sum</span>(exp_z)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>softmax_result <span class="op">=</span> softmax(logits)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Classic normalization (not ideal for probabilities unless values are positive)</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize(z):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    z_sum <span class="op">=</span> np.<span class="bu">sum</span>(z)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z <span class="op">/</span> z_sum <span class="cf">if</span> z_sum <span class="op">!=</span> <span class="dv">0</span> <span class="cf">else</span> np.zeros_like(z)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>normalize_result <span class="op">=</span> normalize(logits)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Raw logits:       "</span>, logits)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sigmoid (z=2.0):  "</span>, sigmoid_result)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Softmax:          "</span>, softmax_result)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Normalization:    "</span>, normalize_result)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot comparison</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'Class 1'</span>, <span class="st">'Class 2'</span>, <span class="st">'Class 3'</span>]</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(labels))</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">-</span> width, softmax_result, width, label<span class="op">=</span><span class="st">'Softmax'</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>ax.bar(x, normalize_result, width, label<span class="op">=</span><span class="st">'Normalization'</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">+</span> width, [sigmoid_result, <span class="dv">0</span>, <span class="dv">0</span>], width, label<span class="op">=</span><span class="st">'Sigmoid (binary)'</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output Value'</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Output Transformations'</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(labels)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Raw logits:        [2.  1.  0.1]
Sigmoid (z=2.0):   0.8807970779778823
Softmax:           [0.65900114 0.24243297 0.09856589]
Normalization:     [0.64516129 0.32258065 0.03225806]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-3-output-2.png" width="662" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Summary: - Sigmoid applies to one logit for binary classification.</p>
<ul>
<li><p>Softmax distributes probabilities across multiple classes.</p></li>
<li><p>Normalization divides values by their sum but doesn’t always yield valid probabilities.</p></li>
</ul>
</section>
</section>
<section id="problem-7" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="problem-7"><span class="header-section-number">2.7</span> Problem 7:</h2>
<p>Complete the following sentence: Minimizing the negative log likelihood also means maximizing the […] of selecting the […] class.</p>
<p><strong>Complete sentence:</strong></p>
<p>Minimizing the negative log likelihood also means <strong>maximizing the likelihood of selecting the correct class</strong>.</p>
<section id="explanation-3" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="explanation-3"><span class="header-section-number">2.7.1</span> Explanation</h3>
<p>Minimizing the <strong>negative log likelihood (NLL)</strong> is equivalent to <strong>maximizing the likelihood</strong> of the model predicting the <strong>correct class</strong>.</p>
<section id="why" class="level4">
<h4 class="anchored" data-anchor-id="why">Why?</h4>
<p>Given: - A model that outputs predicted probabilities <span class="math inline">\(p(y_i \mid x_i)\)</span> for each observation - True class labels <span class="math inline">\(y_i\)</span></p>
<p>Then the <strong>likelihood</strong> for the correct predictions is: <span class="math display">\[
L = \prod_{i=1}^{n} p(y_i \mid x_i)
\]</span></p>
<p>Taking the <strong>log-likelihood</strong>: <span class="math display">\[
\log L = \sum_{i=1}^{n} \log p(y_i \mid x_i)
\]</span></p>
<p>The <strong>negative log-likelihood (NLL)</strong> is: <span class="math display">\[
\text{NLL} = -\log L = -\sum_{i=1}^{n} \log p(y_i \mid x_i)
\]</span></p>
<p>So <strong>minimizing NLL</strong> is mathematically the same as <strong>maximizing the log-likelihood</strong>, which increases the probability assigned to the correct class.</p>
<hr>
</section>
</section>
<section id="python-code-illustration" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="python-code-illustration"><span class="header-section-number">2.7.2</span> Python Code Illustration</h3>
<p>Below is an example comparing NLL for different predicted probabilities of the correct class:</p>
<div id="bc92337f" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated predicted probabilities for the correct class</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>p_correct <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">1.0</span>, <span class="dv">100</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>nll <span class="op">=</span> <span class="op">-</span>np.log(p_correct)  <span class="co"># Negative log-likelihood</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>plt.plot(p_correct, nll, label<span class="op">=</span><span class="st">'NLL = -log(p)'</span>, color<span class="op">=</span><span class="st">'darkblue'</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Negative Log-Likelihood vs. Probability of Correct Class'</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Probability for Correct Class'</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Negative Log-Likelihood'</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-1.png" width="662" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>When the model assigns a high probability to the correct class, the NLL is low.</p>
<p>As the probability approaches 0, the NLL becomes very large.</p>
<p>Thus, minimizing NLL encourages the model to be more confident and accurate in predicting the correct class.</p>
</section>
<section id="step-by-step-understanding-negative-log-likelihood-nll" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="step-by-step-understanding-negative-log-likelihood-nll"><span class="header-section-number">2.7.3</span> Step-by-Step: Understanding Negative Log Likelihood (NLL)</h3>
<p>This walkthrough will help you see <strong>how and why minimizing NLL means maximizing the probability of the correct class</strong>, both conceptually and numerically.</p>
<hr>
<section id="step-1-define-the-task" class="level4">
<h4 class="anchored" data-anchor-id="step-1-define-the-task">Step 1: Define the task</h4>
<p>We have a binary classification model, and it predicts a probability for the <strong>correct class</strong>.</p>
<hr>
</section>
<section id="step-2-simulate-model-predictions" class="level4">
<h4 class="anchored" data-anchor-id="step-2-simulate-model-predictions">Step 2: Simulate model predictions</h4>
<p>We simulate predicted probabilities for the <strong>true class (label = 1)</strong>.</p>
</section>
<section id="code" class="level4">
<h4 class="anchored" data-anchor-id="code">Code:</h4>
<div id="c5943799" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>predicted_probs <span class="op">=</span> [<span class="fl">0.9</span>, <span class="fl">0.7</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>]  <span class="co"># Predicted probability for the correct class</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>predicted_probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>[0.9, 0.7, 0.5, 0.3, 0.1]</code></pre>
</div>
</div>
<div id="165e7542" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted Probability → Negative Log-Likelihood"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> predicted_probs:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    nll <span class="op">=</span> <span class="op">-</span>np.log(p)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>p<span class="sc">:.1f}</span><span class="ss"> → </span><span class="sc">{</span>nll<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted Probability → Negative Log-Likelihood
0.9 → 0.1054
0.7 → 0.3567
0.5 → 0.6931
0.3 → 1.2040
0.1 → 2.3026</code></pre>
</div>
</div>
<div id="94a1516d" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>p_vals <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">1.0</span>, <span class="dv">100</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>nll_vals <span class="op">=</span> <span class="op">-</span>np.log(p_vals)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>plt.plot(p_vals, nll_vals, label<span class="op">=</span><span class="st">"NLL = -log(p)"</span>, color<span class="op">=</span><span class="st">"blue"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted Probability for Correct Class"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Negative Log-Likelihood"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"NLL vs. Predicted Probability"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-7-output-1.png" width="662" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>When the model is confident and right (e.g.&nbsp;<span class="math inline">\(p = 0.9\)</span>), the NLL is low.</p>
<p>When it’s unsure or wrong (e.g.&nbsp;<span class="math inline">\(p = 0.1\)</span>), the NLL is high.</p>
<p>Therefore, minimizing NLL encourages the model to assign high probability to the correct class.</p>
</section>
</section>
</section>
<section id="problem-8" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="problem-8"><span class="header-section-number">2.8</span> Problem 8:</h2>
<p>Assume the probability of an event occurring is p = 0.1. 1. What are the odds of the event occurring?. 2. What are the log-odds of the event occurring?. 3. Construct the probability of the event as a ratio that equals 0.1.</p>
<section id="step-by-step-probability-odds-and-log-odds" class="level3" data-number="2.8.1">
<h3 data-number="2.8.1" class="anchored" data-anchor-id="step-by-step-probability-odds-and-log-odds"><span class="header-section-number">2.8.1</span> Step-by-Step: Probability, Odds, and Log-Odds</h3>
<p>Assume the probability of an event occurring is:</p>
<p><span class="math display">\[
p = 0.1
\]</span></p>
<hr>
</section>
<section id="what-are-the-odds-of-the-event-occurring" class="level3" data-number="2.8.2">
<h3 data-number="2.8.2" class="anchored" data-anchor-id="what-are-the-odds-of-the-event-occurring"><span class="header-section-number">2.8.2</span> 1. What are the <strong>odds</strong> of the event occurring?</h3>
<p><strong>Definition:</strong> &gt; Odds are the ratio of the probability of the event <strong>occurring</strong> to the probability of it <strong>not occurring</strong>.</p>
<p><span class="math display">\[
\text{odds} = \frac{p}{1 - p}
\]</span></p>
</section>
<section id="calculation" class="level3" data-number="2.8.3">
<h3 data-number="2.8.3" class="anchored" data-anchor-id="calculation"><span class="header-section-number">2.8.3</span> Calculation:</h3>
<p><span class="math display">\[
\text{odds} = \frac{0.1}{1 - 0.1} = \frac{0.1}{0.9} \approx 0.111
\]</span></p>
<hr>
</section>
<section id="what-are-the-log-odds-logit-of-the-event" class="level3" data-number="2.8.4">
<h3 data-number="2.8.4" class="anchored" data-anchor-id="what-are-the-log-odds-logit-of-the-event"><span class="header-section-number">2.8.4</span> 2. What are the <strong>log-odds</strong> (logit) of the event?</h3>
<p><strong>Definition:</strong> &gt; Log-odds are the logarithm of the odds (also known as the <strong>logit function</strong>):</p>
<p><span class="math display">\[
\text{log-odds} = \log\left(\frac{p}{1 - p}\right)
\]</span></p>
</section>
<section id="calculation-1" class="level3" data-number="2.8.5">
<h3 data-number="2.8.5" class="anchored" data-anchor-id="calculation-1"><span class="header-section-number">2.8.5</span> Calculation:</h3>
<p><span class="math display">\[
\log\left(\frac{0.1}{0.9}\right) = \log(0.111...) \approx -2.197
\]</span></p>
<hr>
</section>
<section id="construct-the-probability-as-a-ratio-that-equals-0.1" class="level3" data-number="2.8.6">
<h3 data-number="2.8.6" class="anchored" data-anchor-id="construct-the-probability-as-a-ratio-that-equals-0.1"><span class="header-section-number">2.8.6</span> 3. Construct the probability as a ratio that equals 0.1</h3>
<p>We want to express:</p>
<p><span class="math display">\[
\frac{\text{favorable outcomes}}{\text{total outcomes}} = 0.1
\]</span></p>
<p>One example:</p>
<p><span class="math display">\[
\frac{1}{10} = 0.1
\]</span></p>
<p>So, this means <strong>1 favorable case out of 10 total cases</strong>, or 9 unfavorable cases.</p>
<hr>
</section>
<section id="summary-table-1" class="level3" data-number="2.8.7">
<h3 data-number="2.8.7" class="anchored" data-anchor-id="summary-table-1"><span class="header-section-number">2.8.7</span> Summary Table</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Value</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Probability</td>
<td>0.1</td>
<td>given</td>
</tr>
<tr class="even">
<td>Odds</td>
<td>0.111</td>
<td><span class="math inline">\(\frac{0.1}{0.9}\)</span></td>
</tr>
<tr class="odd">
<td>Log-Odds</td>
<td>-2.197</td>
<td><span class="math inline">\(\log\left(\frac{0.1}{0.9}\right)\)</span></td>
</tr>
<tr class="even">
<td>Ratio Form</td>
<td>1:9</td>
<td><span class="math inline">\(1/10 = 0.1\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="intuition-behind-probability-odds-and-log-odds" class="level3" data-number="2.8.8">
<h3 data-number="2.8.8" class="anchored" data-anchor-id="intuition-behind-probability-odds-and-log-odds"><span class="header-section-number">2.8.8</span> Intuition Behind Probability, Odds, and Log-Odds</h3>
<p>Understanding <strong>why</strong> we use these representations helps clarify their role in models like logistic regression.</p>
<hr>
</section>
<section id="probability-p" class="level3" data-number="2.8.9">
<h3 data-number="2.8.9" class="anchored" data-anchor-id="probability-p"><span class="header-section-number">2.8.9</span> Probability (p)</h3>
<ul>
<li><strong>Intuitive measure</strong> of likelihood: ranges between 0 and 1.</li>
<li>Easy to interpret: “There is a 10% chance this will happen.”</li>
</ul>
<p>But: <strong>Not ideal for modeling</strong>, because probabilities are bounded, and small changes near 0 or 1 can be disproportionate.</p>
<hr>
</section>
<section id="odds-fracp1---p" class="level3" data-number="2.8.10">
<h3 data-number="2.8.10" class="anchored" data-anchor-id="odds-fracp1---p"><span class="header-section-number">2.8.10</span> Odds: <span class="math inline">\(\frac{p}{1 - p}\)</span></h3>
<ul>
<li>Represent <strong>relative chances</strong>: how likely something is vs.&nbsp;not.</li>
<li>Example: odds = 2 means “twice as likely to happen than not.”</li>
</ul>
<p>Odds are <strong>unbounded</strong> (0 to ∞), unlike probabilities. This makes them easier to model with <strong>linear functions</strong>.</p>
<hr>
</section>
<section id="log-odds-logit-logleftfracp1---pright" class="level3" data-number="2.8.11">
<h3 data-number="2.8.11" class="anchored" data-anchor-id="log-odds-logit-logleftfracp1---pright"><span class="header-section-number">2.8.11</span> Log-Odds (Logit): <span class="math inline">\(\log\left(\frac{p}{1 - p}\right)\)</span></h3>
<ul>
<li><strong>Transforms probabilities</strong> to the entire real line: <span class="math inline">\((-\infty, +\infty)\)</span>.</li>
<li>Linear in model parameters — makes <strong>logistic regression</strong> a linear model in log-odds space.</li>
<li>Symmetric: log-odds of 0 means <span class="math inline">\(p = 0.5\)</span>.</li>
</ul>
<blockquote class="blockquote">
<p>This transformation enables optimization with gradient-based methods and maintains interpretability via the inverse sigmoid function.</p>
</blockquote>
<hr>
</section>
<section id="summary-why-use-log-odds" class="level3" data-number="2.8.12">
<h3 data-number="2.8.12" class="anchored" data-anchor-id="summary-why-use-log-odds"><span class="header-section-number">2.8.12</span> Summary: Why use log-odds?</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Representation</th>
<th>Range</th>
<th>Good For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Probability</td>
<td>[0, 1]</td>
<td>Intuition, interpretability</td>
</tr>
<tr class="even">
<td>Odds</td>
<td>[0, ∞)</td>
<td>Relative comparison</td>
</tr>
<tr class="odd">
<td>Log-Odds</td>
<td>(−∞, ∞)</td>
<td>Linear modeling, optimization</td>
</tr>
</tbody>
</table>
<p><strong>Log-odds give models a mathematically stable and interpretable way to reason about binary outcomes</strong> — especially for logistic regression.</p>
<div id="0a77e35e" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Probabilities from 0.01 to 0.99 (avoid 0 and 1 to prevent log(0))</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">500</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute odds and log-odds</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>odds <span class="op">=</span> p <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> p)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>log_odds <span class="op">=</span> np.log(odds)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">10</span>), sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(p, p, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_ylabel(<span class="st">"Probability (p)"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_title(<span class="st">"Probability vs. Itself (Identity)"</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].grid(<span class="va">True</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Odds</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(p, odds, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_ylabel(<span class="st">"Odds (p / (1 - p))"</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_title(<span class="st">"Probability vs. Odds"</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].grid(<span class="va">True</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Log-Odds</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].plot(p, log_odds, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].set_ylabel(<span class="st">"Log-Odds (log(p / (1 - p)))"</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].set_xlabel(<span class="st">"Probability (p)"</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].set_title(<span class="st">"Probability vs. Log-Odds"</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].grid(<span class="va">True</span>)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-8-output-1.png" width="758" height="950" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="interpretation-of-the-plots" class="level3" data-number="2.8.13">
<h3 data-number="2.8.13" class="anchored" data-anchor-id="interpretation-of-the-plots"><span class="header-section-number">2.8.13</span> Interpretation of the Plots</h3>
</section>
<section id="top-plot-probability-vs.-itself" class="level3" data-number="2.8.14">
<h3 data-number="2.8.14" class="anchored" data-anchor-id="top-plot-probability-vs.-itself"><span class="header-section-number">2.8.14</span> Top Plot: Probability vs.&nbsp;Itself</h3>
<ul>
<li>This is the <strong>identity function</strong>, where the output equals the input.</li>
<li>Useful to visualize the <strong>bounded linearity</strong> of probability values.</li>
<li>Range is limited to [0, 1], which restricts direct use in linear models.</li>
</ul>
<hr>
</section>
<section id="middle-plot-probability-vs.-odds" class="level3" data-number="2.8.15">
<h3 data-number="2.8.15" class="anchored" data-anchor-id="middle-plot-probability-vs.-odds"><span class="header-section-number">2.8.15</span> Middle Plot: Probability vs.&nbsp;Odds</h3>
<ul>
<li>Odds are computed as:<br>
<span class="math display">\[
\text{odds} = \frac{p}{1 - p}
\]</span></li>
<li>As <span class="math inline">\(p \to 1\)</span>, the odds grow <strong>rapidly</strong> (approaching ∞).</li>
<li>As <span class="math inline">\(p \to 0\)</span>, the odds approach 0.</li>
<li><strong>Nonlinear and asymmetric</strong>, making it difficult to model directly.</li>
</ul>
<hr>
</section>
<section id="bottom-plot-probability-vs.-log-odds-logit" class="level3" data-number="2.8.16">
<h3 data-number="2.8.16" class="anchored" data-anchor-id="bottom-plot-probability-vs.-log-odds-logit"><span class="header-section-number">2.8.16</span> Bottom Plot: Probability vs.&nbsp;Log-Odds (Logit)</h3>
<ul>
<li>Log-odds are computed as:<br>
<span class="math display">\[
\text{log-odds} = \log\left(\frac{p}{1 - p}\right)
\]</span></li>
<li>The transformation is:
<ul>
<li><strong>Smooth</strong></li>
<li><strong>Symmetric</strong> around <span class="math inline">\(p = 0.5\)</span></li>
<li><strong>Linear near <span class="math inline">\(p = 0.5\)</span></strong></li>
<li>Maps <span class="math inline">\(p \in (0, 1)\)</span> to <span class="math inline">\((-\infty, +\infty)\)</span></li>
</ul></li>
</ul>
<hr>
</section>
<section id="why-use-log-odds" class="level3" data-number="2.8.17">
<h3 data-number="2.8.17" class="anchored" data-anchor-id="why-use-log-odds"><span class="header-section-number">2.8.17</span> Why Use Log-Odds?</h3>
<p>The log-odds transformation allows:</p>
<ul>
<li>Applying <strong>linear models</strong> to binary classification.</li>
<li>Smooth optimization using gradient descent.</li>
<li>Easy interpretability: a one-unit increase in input causes a fixed increase in log-odds.</li>
</ul>
<p>Thus, <strong>log-odds</strong> are the foundation of <strong>logistic regression</strong>, enabling a linear combination of inputs to model a probability through the <strong>sigmoid inverse</strong>.</p>
</section>
</section>
<section id="problem-9." class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="problem-9."><span class="header-section-number">2.9</span> Problem 9.</h2>
<p><strong>True or False:</strong> If the odds of success in a binary response is 4, the corresponding probability of success is 0.8.</p>
<hr>
<section id="step-by-step-solution" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="step-by-step-solution"><span class="header-section-number">2.9.1</span> Step-by-step Solution</h3>
<p>We are given: <span class="math display">\[
\text{odds} = 4
\]</span></p>
<p>Recall the relationship between <strong>odds</strong> and <strong>probability</strong>: <span class="math display">\[
\text{odds} = \frac{p}{1 - p}
\]</span></p>
<p>Solve for <span class="math inline">\(p\)</span>: <span class="math display">\[
\frac{p}{1 - p} = 4
\Rightarrow p = 4(1 - p)
\Rightarrow p = 4 - 4p
\Rightarrow 5p = 4
\Rightarrow p = \frac{4}{5} = 0.8
\]</span></p>
<hr>
</section>
<section id="final-answer" class="level3" data-number="2.9.2">
<h3 data-number="2.9.2" class="anchored" data-anchor-id="final-answer"><span class="header-section-number">2.9.2</span> Final Answer</h3>
<p><strong>True</strong> – If the odds are 4, the probability of success is <strong>0.8</strong>.</p>
</section>
</section>
<section id="problem-10" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="problem-10"><span class="header-section-number">2.10</span> Problem 10:</h2>
<p>Draw a graph of odds to probabilities, mapping the entire range of probabilities to their respective odds.</p>
<div id="54099d2a" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a range of probabilities from 0.01 to 0.99</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">500</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>odds <span class="op">=</span> p <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> p)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>plt.plot(p, odds, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Probability (p)"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Odds (p / (1 - p))"</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mapping: Probability to Odds"</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">20</span>)  <span class="co"># limit to see behavior better near p=1</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="758" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="graph-probability-vs.-odds" class="level3" data-number="2.10.1">
<h3 data-number="2.10.1" class="anchored" data-anchor-id="graph-probability-vs.-odds"><span class="header-section-number">2.10.1</span> Graph: Probability vs.&nbsp;Odds</h3>
<p>This plot shows how probability values map to odds:</p>
<ul>
<li><p>Formula:<br>
<span class="math display">\[
\text{odds} = \frac{p}{1 - p}
\]</span></p></li>
<li><p>As the probability approaches 1, the odds grow rapidly toward infinity.</p></li>
<li><p>As the probability approaches 0, the odds approach 0.</p></li>
<li><p>The function is <strong>nonlinear and increasing</strong>, with a sharp curve as $ p $.</p></li>
</ul>
<p>This graph helps visualize why odds are unbounded and why it’s useful to convert them to log-odds in modeling.</p>
</section>
</section>
<section id="problem-11" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="problem-11"><span class="header-section-number">2.11</span> Problem 11:</h2>
<p>The logistic regression model is a subset of a broader range of machine learning models known as generalized linear models (GLMs), which also include analysis of variance (ANOVA), vanilla linear regression, etc. There are three components to a GLM; identify these three components for binary logistic regression.</p>
<section id="components-of-a-generalized-linear-model-glm-in-binary-logistic-regression" class="level3" data-number="2.11.1">
<h3 data-number="2.11.1" class="anchored" data-anchor-id="components-of-a-generalized-linear-model-glm-in-binary-logistic-regression"><span class="header-section-number">2.11.1</span> Components of a Generalized Linear Model (GLM) in Binary Logistic Regression</h3>
<p>A <strong>Generalized Linear Model (GLM)</strong> has three main components. For <strong>binary logistic regression</strong>, they are:</p>
<hr>
</section>
<section id="random-component" class="level3" data-number="2.11.2">
<h3 data-number="2.11.2" class="anchored" data-anchor-id="random-component"><span class="header-section-number">2.11.2</span> 1. <strong>Random Component</strong></h3>
<p>Specifies the <strong>distribution</strong> of the response variable.</p>
<ul>
<li>In binary logistic regression, the response ( Y {0, 1} ) is assumed to follow a <strong>Bernoulli distribution</strong>: <span class="math display">\[
Y \sim \text{Bernoulli}(p)
\]</span></li>
</ul>
<hr>
</section>
<section id="systematic-component" class="level3" data-number="2.11.3">
<h3 data-number="2.11.3" class="anchored" data-anchor-id="systematic-component"><span class="header-section-number">2.11.3</span> 2. <strong>Systematic Component</strong></h3>
<p>Represents the <strong>linear predictor</strong>, which is a linear combination of input features:</p>
<ul>
<li><p>Let ( x = (x_1, x_2, , x_n) ), then: <span class="math display">\[
\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n
\]</span></p></li>
<li><p>This is often written compactly as: <span class="math display">\[
\eta = \mathbf{x}^\top \boldsymbol{\beta}
\]</span></p></li>
</ul>
<hr>
</section>
<section id="link-function" class="level3" data-number="2.11.4">
<h3 data-number="2.11.4" class="anchored" data-anchor-id="link-function"><span class="header-section-number">2.11.4</span> 3. <strong>Link Function</strong></h3>
<p>Connects the expected value of the response to the linear predictor.</p>
<ul>
<li><p>In logistic regression, the link function is the <strong>logit</strong> function: <span class="math display">\[
\text{logit}(p) = \log\left( \frac{p}{1 - p} \right) = \eta
\]</span></p></li>
<li><p>The inverse of the logit gives the <strong>sigmoid function</strong> to recover probabilities: <span class="math display">\[
p = \frac{1}{1 + e^{-\eta}}
\]</span></p></li>
</ul>
<hr>
</section>
<section id="summary-table-2" class="level3" data-number="2.11.5">
<h3 data-number="2.11.5" class="anchored" data-anchor-id="summary-table-2"><span class="header-section-number">2.11.5</span> Summary Table</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 71%">
</colgroup>
<thead>
<tr class="header">
<th>GLM Component</th>
<th>Logistic Regression Specification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Component</td>
<td>( Y (p) )</td>
</tr>
<tr class="even">
<td>Systematic Component</td>
<td>( = ^ )</td>
</tr>
<tr class="odd">
<td>Link Function</td>
<td>( (p) = ( ) )</td>
</tr>
</tbody>
</table>
</section>
<section id="adjusted-glm-components-for-voice-activity-detection-vad" class="level3" data-number="2.11.6">
<h3 data-number="2.11.6" class="anchored" data-anchor-id="adjusted-glm-components-for-voice-activity-detection-vad"><span class="header-section-number">2.11.6</span> Adjusted GLM Components for Voice Activity Detection (VAD)</h3>
<p>Assume the binary outcome: - <span class="math inline">\(Y = 1\)</span>: voice activity detected - <span class="math inline">\(Y = 0\)</span>: no voice activity detected</p>
<p>We define the GLM components for a logistic regression model as follows:</p>
<hr>
</section>
<section id="random-component-1" class="level3" data-number="2.11.7">
<h3 data-number="2.11.7" class="anchored" data-anchor-id="random-component-1"><span class="header-section-number">2.11.7</span> Random Component</h3>
<p>The response variable <span class="math inline">\(Y\)</span> is binary:</p>
<p><span class="math display">\[
Y \sim \text{Bernoulli}(p)
\]</span></p>
<p>where <span class="math inline">\(p = \mathbb{P}(Y = 1 \mid \text{features})\)</span> represents the probability that voice activity is present in a given time frame.</p>
<hr>
</section>
<section id="systematic-components" class="level3" data-number="2.11.8">
<h3 data-number="2.11.8" class="anchored" data-anchor-id="systematic-components"><span class="header-section-number">2.11.8</span> Systematic Components</h3>
<p>We propose <strong>two alternative linear predictors</strong> using different input features:</p>
<section id="systematic-component-a" class="level4">
<h4 class="anchored" data-anchor-id="systematic-component-a">Systematic Component A:</h4>
<p>Use energy and zero-crossing rate: <span class="math display">\[
\eta = \beta_0 + \beta_1 \cdot \text{Energy} + \beta_2 \cdot \text{ZCR}
\]</span></p>
<ul>
<li><strong>Energy</strong>: overall signal power in the frame<br>
</li>
<li><strong>ZCR</strong> (Zero Crossing Rate): frequency of sign changes in waveform</li>
</ul>
</section>
<section id="systematic-component-b" class="level4">
<h4 class="anchored" data-anchor-id="systematic-component-b">Systematic Component B:</h4>
<p>Use MFCC coefficients (common in speech processing): <span class="math display">\[
\eta = \beta_0 + \beta_1 \cdot \text{MFCC}_1 + \beta_2 \cdot \text{MFCC}_2 + \cdots + \beta_{13} \cdot \text{MFCC}_{13}
\]</span></p>
<ul>
<li><strong>MFCCs</strong>: Mel-Frequency Cepstral Coefficients — compact representation of spectral shape</li>
</ul>
<hr>
</section>
</section>
<section id="link-function-1" class="level3" data-number="2.11.9">
<h3 data-number="2.11.9" class="anchored" data-anchor-id="link-function-1"><span class="header-section-number">2.11.9</span> Link Function</h3>
<p>Use the <strong>logit</strong> link function to relate the probability to the linear predictor: <span class="math display">\[
\text{logit}(p) = \log\left( \frac{p}{1 - p} \right) = \eta
\]</span></p>
<p>or equivalently: <span class="math display">\[
p = \frac{1}{1 + e^{-\eta}}
\]</span></p>
<hr>
</section>
<section id="summary-3" class="level3" data-number="2.11.10">
<h3 data-number="2.11.10" class="anchored" data-anchor-id="summary-3"><span class="header-section-number">2.11.10</span> Summary</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 76%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Component</td>
<td><span class="math inline">\(Y \sim \text{Bernoulli}(p)\)</span></td>
</tr>
<tr class="even">
<td>Systematic A</td>
<td><span class="math inline">\(\eta = \beta_0 + \beta_1 \cdot \text{Energy} + \beta_2 \cdot \text{ZCR}\)</span></td>
</tr>
<tr class="odd">
<td>Systematic B</td>
<td><span class="math inline">\(eta = \beta_0 + \sum_{i=1}^{13} \beta_i \cdot \text{MFCC}_i\)</span></td>
</tr>
<tr class="even">
<td>Link Function</td>
<td><span class="math inline">\(\text{logit}(p) = \log\left( \frac{p}{1 - p} \right)\)</span></td>
</tr>
</tbody>
</table>
<p>This setup applies logistic regression to real-world audio-based classification.</p>
<div id="db38d52f" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate data for two systematic components</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated features for Systematic A: Energy &amp; ZCR</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>energy <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.5</span>, scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span>n_samples)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>zcr <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.3</span>, scale<span class="op">=</span><span class="fl">0.05</span>, size<span class="op">=</span>n_samples)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>X_A <span class="op">=</span> np.column_stack((energy, zcr))</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated features for Systematic B: 13 MFCCs</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>mfcc <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>(n_samples, <span class="dv">13</span>))</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>X_B <span class="op">=</span> mfcc</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated binary labels based on true linear model</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_labels(X, true_coef):</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> X <span class="op">@</span> true_coef[<span class="dv">1</span>:] <span class="op">+</span> true_coef[<span class="dv">0</span>]</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>logits))</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (np.random.rand(<span class="bu">len</span>(probs)) <span class="op">&lt;</span> probs).astype(<span class="bu">int</span>), probs</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="co"># True coefficients for Systematic A and B</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>true_coef_A <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">8</span>])  <span class="co"># Intercept, Energy, ZCR</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>true_coef_B <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">0.5</span>] <span class="op">+</span> [<span class="fl">0.3</span>]<span class="op">*</span><span class="dv">13</span>)  <span class="co"># Intercept + MFCCs</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate labels</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>y_A, p_A <span class="op">=</span> simulate_labels(X_A, true_coef_A)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>y_B, p_B <span class="op">=</span> simulate_labels(X_B, true_coef_B)</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit logistic regression models</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>scaler_A <span class="op">=</span> StandardScaler().fit(X_A)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>X_A_std <span class="op">=</span> scaler_A.transform(X_A)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>model_A <span class="op">=</span> LogisticRegression().fit(X_A_std, y_A)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>scaler_B <span class="op">=</span> StandardScaler().fit(X_B)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>X_B_std <span class="op">=</span> scaler_B.transform(X_B)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>model_B <span class="op">=</span> LogisticRegression().fit(X_B_std, y_B)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>p_pred_A <span class="op">=</span> model_A.predict_proba(X_A_std)[:, <span class="dv">1</span>]</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>p_pred_B <span class="op">=</span> model_B.predict_proba(X_B_std)[:, <span class="dv">1</span>]</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot probability distributions</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>plt.hist(p_pred_A[y_A <span class="op">==</span> <span class="dv">0</span>], bins<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'No Voice'</span>)</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>plt.hist(p_pred_A[y_A <span class="op">==</span> <span class="dv">1</span>], bins<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Voice'</span>)</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Predicted Probabilities (Energy &amp; ZCR)"</span>)</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Probability of Voice Activity"</span>)</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Count"</span>)</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>plt.hist(p_pred_B[y_B <span class="op">==</span> <span class="dv">0</span>], bins<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'No Voice'</span>)</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>plt.hist(p_pred_B[y_B <span class="op">==</span> <span class="dv">1</span>], bins<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Voice'</span>)</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Predicted Probabilities (MFCC Features)"</span>)</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Probability of Voice Activity"</span>)</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Count"</span>)</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="1142" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>What this illustrates: Logistic regression maps the linear combination of audio features (systematic component) to a probability of voice activity.</p>
<p>The predicted probability distributions show how well the features separate the two classes (voice vs.&nbsp;no voice).</p>
<p>This supports the GLM formulation where:</p>
<p>Inputs are combined linearly,</p>
<p>The output is transformed via the logit (sigmoid) link,</p>
<p>And the response is modeled as a Bernoulli random variable.</p>
</section>
</section>
<section id="problem-12" class="level2" data-number="2.12">
<h2 data-number="2.12" class="anchored" data-anchor-id="problem-12"><span class="header-section-number">2.12</span> Problem 12:</h2>
<p>Let us consider the <strong>logit transformation</strong>, i.e., <strong>log-odds</strong>. Assume a scenario in which the logit forms the linear decision boundary:</p>
<p><span class="math display">\[
\log \left( \frac{\Pr(Y = 1 \mid \mathbf{X})}{\Pr(Y = 0 \mid \mathbf{X})} \right) = \theta_0 + \boldsymbol{\theta}^T \mathbf{X} \tag{2.1}
\]</span></p>
<p>where: - <span class="math inline">\(\mathbf{X}\)</span> is a vector of systematic components (input features), - <span class="math inline">\(\boldsymbol{\theta}\)</span> is a vector of predictor coefficients, - <span class="math inline">\(\theta_0\)</span> is the intercept.</p>
<p><strong>Task:</strong><br>
Write the mathematical expression for the <strong>hyperplane</strong> that describes the <strong>decision boundary</strong> for this logistic regression model.</p>
<section id="logistic-regression-decision-boundary-using-logit" class="level3" data-number="2.12.1">
<h3 data-number="2.12.1" class="anchored" data-anchor-id="logistic-regression-decision-boundary-using-logit"><span class="header-section-number">2.12.1</span> Logistic Regression Decision Boundary (Using Logit)</h3>
<p>Given the logit model:</p>
<p><span class="math display">\[
\log \left( \frac{\Pr(Y = 1 \mid \mathbf{X})}{\Pr(Y = 0 \mid \mathbf{X})} \right) = \theta_0 + \boldsymbol{\theta}^T \mathbf{X}
\]</span></p>
<p>This expression defines the <strong>log-odds</strong> as a linear function of input features <span class="math inline">\(\mathbf{X}\)</span>.</p>
<hr>
</section>
<section id="step-1-set-the-decision-threshold" class="level3" data-number="2.12.2">
<h3 data-number="2.12.2" class="anchored" data-anchor-id="step-1-set-the-decision-threshold"><span class="header-section-number">2.12.2</span> Step 1: Set the Decision Threshold</h3>
<p>In binary classification, the decision boundary occurs when both classes are equally likely:</p>
<p><span class="math display">\[
\Pr(Y = 1 \mid \mathbf{X}) = \Pr(Y = 0 \mid \mathbf{X}) = 0.5
\]</span></p>
<p>Thus, the odds ratio becomes:</p>
<p><span class="math display">\[
\frac{\Pr(Y = 1 \mid \mathbf{X})}{\Pr(Y = 0 \mid \mathbf{X})} = 1
\]</span></p>
<p>Taking the logarithm:</p>
<p><span class="math display">\[
\log \left( \frac{\Pr(Y = 1 \mid \mathbf{X})}{\Pr(Y = 0 \mid \mathbf{X})} \right) = 0
\]</span></p>
<hr>
</section>
<section id="step-2-solve-for-the-boundary" class="level3" data-number="2.12.3">
<h3 data-number="2.12.3" class="anchored" data-anchor-id="step-2-solve-for-the-boundary"><span class="header-section-number">2.12.3</span> Step 2: Solve for the Boundary</h3>
<p>Set the log-odds to zero in the original equation:</p>
<p><span class="math display">\[
\theta_0 + \boldsymbol{\theta}^T \mathbf{X} = 0
\]</span></p>
<hr>
</section>
<section id="final-result-hyperplane-equation" class="level3" data-number="2.12.4">
<h3 data-number="2.12.4" class="anchored" data-anchor-id="final-result-hyperplane-equation"><span class="header-section-number">2.12.4</span> Final Result: Hyperplane Equation</h3>
<p>This is the equation of the <strong>decision boundary</strong> — a hyperplane that separates the feature space:</p>
<p><span class="math display">\[
\boxed{\theta_0 + \boldsymbol{\theta}^T \mathbf{X} = 0}
\]</span></p>
<ul>
<li>If <span class="math inline">\(\theta_0 + \boldsymbol{\theta}^T \mathbf{X} &gt; 0\)</span>, then <span class="math inline">\(\Pr(Y=1) &gt; 0.5\)</span></li>
<li>If <span class="math inline">\(\theta_0 + \boldsymbol{\theta}^T \mathbf{X} &lt; 0\)</span>, then <span class="math inline">\(\Pr(Y=1) &lt; 0.5\)</span></li>
</ul>
<p>This linear boundary is fundamental in logistic regression for classification tasks.</p>
</section>
<section id="solution" class="level3" data-number="2.12.5">
<h3 data-number="2.12.5" class="anchored" data-anchor-id="solution"><span class="header-section-number">2.12.5</span> Solution</h3>
<p>The <strong>hyperplane</strong> that defines the <strong>decision boundary</strong> in a logistic regression model is:</p>
<p><span class="math display">\[
\theta_0 + \boldsymbol{\theta}^T \mathbf{X} = 0 \tag{2.15}
\]</span></p>
<hr>
</section>
<section id="derivation-from-logit-function" class="level3" data-number="2.12.6">
<h3 data-number="2.12.6" class="anchored" data-anchor-id="derivation-from-logit-function"><span class="header-section-number">2.12.6</span> Derivation from Logit Function</h3>
<p>We start from the <strong>logit model</strong>:</p>
<p><span class="math display">\[
\log \left( \frac{\Pr(Y = 1 \mid \mathbf{X})}{\Pr(Y = 0 \mid \mathbf{X})} \right) = \theta_0 + \boldsymbol{\theta}^T \mathbf{X}
\]</span></p>
<p>This expression defines the <strong>log-odds</strong> of the response variable <span class="math inline">\(Y\)</span> being 1 as a linear function of the input vector <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>At the <strong>decision boundary</strong>, we are equally likely to classify the outcome as either class (i.e., <span class="math inline">\(\Pr(Y=1) = \Pr(Y=0) = 0.5\)</span>). This implies:</p>
<p><span class="math display">\[
\frac{\Pr(Y = 1 \mid \mathbf{X})}{\Pr(Y = 0 \mid \mathbf{X})} = 1
\]</span></p>
<p>Taking the logarithm:</p>
<p><span class="math display">\[
\log \left( \frac{\Pr(Y = 1 \mid \mathbf{X})}{\Pr(Y = 0 \mid \mathbf{X})} \right) = 0
\]</span></p>
<p>Now set the left-hand side of the model equal to 0:</p>
<p><span class="math display">\[
\theta_0 + \boldsymbol{\theta}^T \mathbf{X} = 0
\]</span></p>
<hr>
</section>
<section id="conclusion" class="level3" data-number="2.12.7">
<h3 data-number="2.12.7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">2.12.7</span> Conclusion</h3>
<p>The equation</p>
<p><span class="math display">\[
\boxed{\theta_0 + \boldsymbol{\theta}^T \mathbf{X} = 0}
\]</span></p>
<p>is the <strong>mathematical expression of the hyperplane</strong> that separates the classes. It forms the <strong>decision boundary</strong> in logistic regression, where the model predicts:</p>
<ul>
<li>Class 1 if <span class="math inline">\(\theta_0 + \boldsymbol{\theta}^T \mathbf{X} &gt; 0\)</span></li>
<li>Class 0 if <span class="math inline">\(\theta_0 + \boldsymbol{\theta}^T \mathbf{X} &lt; 0\)</span></li>
</ul>
</section>
</section>
<section id="problem-13-logit-and-sigmoid" class="level2" data-number="2.13">
<h2 data-number="2.13" class="anchored" data-anchor-id="problem-13-logit-and-sigmoid"><span class="header-section-number">2.13</span> Problem 13: Logit and Sigmoid</h2>
<p>True or False:</p>
<p><strong>Statement:</strong><br>
<em>The logit function and the natural logistic (sigmoid) function are inverses of each other.</em></p>
<p><strong>Answer:</strong><br>
<strong>True</strong></p>
<hr>
<section id="explanation-4" class="level3" data-number="2.13.1">
<h3 data-number="2.13.1" class="anchored" data-anchor-id="explanation-4"><span class="header-section-number">2.13.1</span> Explanation:</h3>
<ul>
<li><p>The <strong>sigmoid function</strong> (also known as the <strong>logistic function</strong>) is defined as:</p>
<p><span class="math display">\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</span></p></li>
<li><p>The <strong>logit function</strong> is the <strong>inverse</strong> of the sigmoid and is defined as:</p>
<p><span class="math display">\[
\text{logit}(p) = \log \left( \frac{p}{1 - p} \right)
\]</span></p></li>
<li><p>These two functions are <strong>mathematical inverses</strong>:</p>
<ul>
<li>Applying the logit to the output of a sigmoid returns the original input.</li>
<li>Applying the sigmoid to the output of a logit returns the original probability.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="additional-note" class="level3" data-number="2.13.2">
<h3 data-number="2.13.2" class="anchored" data-anchor-id="additional-note"><span class="header-section-number">2.13.2</span> Additional Note:</h3>
<p>The <strong>sigmoid function</strong> is widely used: - In <strong>binary classification</strong> to map a linear model’s output to a probability in [0, 1]. - As an <strong>activation function</strong> in artificial neural networks (although less common now compared to ReLU).</p>
<p>Thus, the statement is <strong>True</strong>.</p>
</section>
</section>
<section id="derivative-of-the-natural-sigmoid-function" class="level2" data-number="2.14">
<h2 data-number="2.14" class="anchored" data-anchor-id="derivative-of-the-natural-sigmoid-function"><span class="header-section-number">2.14</span> Derivative of the Natural Sigmoid Function</h2>
<p>Let the <strong>sigmoid function</strong> be defined as:</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>This maps real values <span class="math inline">\(x \in \mathbb{R}\)</span> to a range in <span class="math inline">\((0, 1)\)</span>.</p>
<hr>
<section id="step-1-compute-the-derivative" class="level3" data-number="2.14.1">
<h3 data-number="2.14.1" class="anchored" data-anchor-id="step-1-compute-the-derivative"><span class="header-section-number">2.14.1</span> Step 1: Compute the Derivative</h3>
<p>We differentiate <span class="math inline">\(\sigma(x)\)</span> with respect to <span class="math inline">\(x\)</span>:</p>
<p>Let:</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}} = f(x)
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\frac{d}{dx} \sigma(x) = \frac{d}{dx} \left( \frac{1}{1 + e^{-x}} \right)
\]</span></p>
<p>Apply the quotient rule or chain rule:</p>
<p>Let <span class="math inline">\(u(x) = 1 + e^{-x}\)</span>, then:</p>
<p><span class="math display">\[
\frac{d}{dx} \left( \frac{1}{u(x)} \right) = -\frac{1}{u(x)^2} \cdot \frac{d}{dx} u(x)
\]</span></p>
<p>We have:</p>
<p><span class="math display">\[
\frac{d}{dx} u(x) = \frac{d}{dx} (1 + e^{-x}) = -e^{-x}
\]</span></p>
<p>So:</p>
<p><span class="math display">\[
\frac{d}{dx} \sigma(x) = -\frac{1}{(1 + e^{-x})^2} \cdot (-e^{-x}) = \frac{e^{-x}}{(1 + e^{-x})^2}
\]</span></p>
<hr>
</section>
<section id="step-2-express-in-terms-of-sigmax" class="level3" data-number="2.14.2">
<h3 data-number="2.14.2" class="anchored" data-anchor-id="step-2-express-in-terms-of-sigmax"><span class="header-section-number">2.14.2</span> Step 2: Express in Terms of <span class="math inline">\(\sigma(x)\)</span></h3>
<p>Since:</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}}, \quad \text{then} \quad 1 - \sigma(x) = \frac{e^{-x}}{1 + e^{-x}}
\]</span></p>
<p>So the derivative becomes:</p>
<p><span class="math display">\[
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
\]</span></p>
<hr>
</section>
<section id="final-answer-1" class="level3" data-number="2.14.3">
<h3 data-number="2.14.3" class="anchored" data-anchor-id="final-answer-1"><span class="header-section-number">2.14.3</span> Final Answer:</h3>
<p><span class="math display">\[
\boxed{\frac{d}{dx} \sigma(x) = \sigma(x) \cdot (1 - \sigma(x))}
\]</span></p>
<p>This elegant result is widely used in training neural networks via backpropagation.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kimhungbui\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quarto-dev/quarto-docs" data-repo-id="" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>